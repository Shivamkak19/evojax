
=== ./evojax/version.py ===
__version__ = "0.2.17"

=== ./evojax/util.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import logging
import numpy as np
from typing import Union
from typing import Tuple
from typing import Callable

import jax.numpy as jnp
from jax import tree_util
from flax.core import FrozenDict


def get_params_format_fn(init_params: FrozenDict) -> Tuple[int, Callable]:
    """Return a function that formats the parameters into a correct format."""

    flat, tree = tree_util.tree_flatten(init_params)
    params_sizes = np.cumsum([np.prod(p.shape) for p in flat])

    def params_format_fn(params: jnp.ndarray) -> FrozenDict:
        params = tree_util.tree_map(
            lambda x, y: x.reshape(y.shape),
            jnp.split(params, params_sizes, axis=-1)[:-1],
            flat)
        return tree_util.tree_unflatten(tree, params)

    return params_sizes[-1], params_format_fn


def create_logger(name: str,
                  log_dir: str = None,
                  debug: bool = False) -> logging.Logger:
    """Create a logger.

    Args:
        name - Name of the logger.
        log_dir - The logger will also log to an external file in the specified
                  directory if specified.
        debug - If we should log in DEBUG mode.

    Returns:
        logging.RootLogger.
    """

    if log_dir and not os.path.exists(log_dir):
        os.makedirs(log_dir)
    log_format = '%(name)s: %(asctime)s [%(levelname)s] %(message)s'
    logging.basicConfig(
        level=logging.DEBUG if debug else logging.INFO, format=log_format)
    logger = logging.getLogger(name)
    if log_dir:
        log_file = os.path.join(log_dir, '{}.txt'.format(name))
        file_hdl = logging.FileHandler(log_file)
        formatter = logging.Formatter(fmt=log_format)
        file_hdl.setFormatter(formatter)
        logger.addHandler(file_hdl)
    # Set level explicitly, otherwise the logger does not output.
    logger.setLevel(logging.DEBUG if debug else logging.INFO)
    return logger


def load_model(model_dir: str) -> Tuple[np.ndarray, np.ndarray]:
    """Load policy parameters from the specified directory.

    Args:
        model_dir - Directory to load the model from.
    Returns:
        A pair of parameters, the shapes of which are
        (param_size,) and (1 + 2 * obs_params_size,).
    """

    model_file = os.path.join(model_dir, 'model.npz')
    if not os.path.exists(model_file):
        raise ValueError('Model file {} does not exist.')
    with np.load(model_file) as data:
        params = data['params']
        obs_params = data['obs_params']
    return params, obs_params


def save_model(model_dir: str,
               model_name: str,
               params: Union[np.ndarray, jnp.ndarray],
               obs_params: Union[np.ndarray, jnp.ndarray] = None,
               best: bool = False) -> None:
    """Save policy parameters to the specified directory.

    Args:
        model_dir - Directory to save the model.
        model_name - Filename.
        params - The parameters to save.
        obs_params - The observation parameters to save
        best - Whether to save a copy as best.npz.
    """

    model_file = os.path.join(model_dir, '{}.npz'.format(model_name))
    np.savez(model_file,
             params=np.array(params),
             obs_params=np.array(obs_params))
    if best:
        model_file = os.path.join(model_dir, 'best.npz')
        np.savez(model_file,
                 params=np.array(params),
                 obs_params=np.array(obs_params))


def save_lattices(log_dir: str,
                  file_name: str,
                  fitness_lattice: jnp.ndarray,
                  params_lattice: jnp.ndarray,
                  occupancy_lattice: jnp.ndarray) -> None:
    """Save QD method's lattices."""
    file_name = os.path.join(log_dir, '{}.npz'.format(file_name))
    np.savez(file_name,
             params_lattice=np.array(params_lattice),
             fitness_lattice=np.array(fitness_lattice),
             occupancy_lattice=np.array(occupancy_lattice))


def get_tensorboard_log_fn(
        log_dir: str) -> Callable[[int, jnp.ndarray, str], None]:
    """
    Returns a custom logging function for the `evojax` `Trainer`.
    The function logs rewards after every train/test iteration with Tensorboard.
    It tries to use `tensorflow` or `pytorch` as tensorboard provider.

    Args:
        log_dir - directory to save store the tensorboard logs
    """
    try:
        from torch.utils.tensorboard import SummaryWriter

        def log_with_pytorch(i: int, scores: jnp.ndarray, stage: str):
            with SummaryWriter(log_dir=log_dir) as writer:
                writer.add_scalar(
                    f"{stage}/score_min", scores.min().item(), global_step=i)
                writer.add_scalar(
                    f"{stage}/score_max", scores.max().item(), global_step=i)
                writer.add_scalar(
                    f"{stage}/score_mean", scores.mean().item(), global_step=i)
                writer.add_scalar(
                    f"{stage}/score_std", scores.std().item(), global_step=i)
                writer.add_histogram(
                    f"{stage}/score_distribution", np.array(scores),
                    global_step=i)

        return log_with_pytorch

    except ImportError:
        pass

    try:
        import tensorflow as tf

        def log_with_tf(i: int, scores: jnp.ndarray, stage: str):
            with tf.summary.SummaryWriter(log_dir=log_dir).as_default():
                tf.summary.scalar(
                    f"{stage}/score_min", scores.min().item(), step=i)
                tf.summary.scalar(
                    f"{stage}/score_max", scores.max().item(), step=i)
                tf.summary.scalar(
                    f"{stage}/score_mean", scores.mean().item(), step=i)
                tf.summary.scalar(
                    f"{stage}/score_std", scores.std().item(), step=i)
                tf.summary.histogram(
                    f"{stage}/score_distribution", np.array(scores), step=i)

        return log_with_tf

    except ImportError:
        pass

    raise ImportError(
        "Please install the tensorboard AND (tensorflow OR pytorch) "
        "packages to log the rewards to tensorboard")

=== ./evojax/algo/cma_wrapper.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This is a wrapper of the CMA-ES algorithm.

This is for users who want to use CMA-ES before we release a pure JAX version.

CMA-ES paper: https://arxiv.org/abs/1604.00772
"""

import sys

import logging
import numpy as np
from typing import Union

import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


class CMA(NEAlgorithm):
    """A wrapper of CMA-ES."""

    def __init__(self,
                 param_size: int,
                 pop_size: int,
                 init_stdev: float = 0.1,
                 seed: int = 0,
                 logger: logging.Logger = None):
        if logger is None:
            self.logger = create_logger(name='CMA')
        else:
            self.logger = logger
        self.pop_size = pop_size

        try:
            import cma
        except ModuleNotFoundError:
            print("You need to install cma for its CMA-ES:")
            print("  pip install cma")
            sys.exit(1)

        self.cma = cma.CMAEvolutionStrategy(
            x0=np.zeros(param_size),
            sigma0=init_stdev,
            inopts={
                'popsize': pop_size,
                'seed': seed if seed > 0 else 42,
                'randn': np.random.randn,
            },
        )
        self.params = None
        self._best_params = None

        self.jnp_array = jax.jit(jnp.array)
        self.jnp_stack = jax.jit(jnp.stack)

    def ask(self) -> jnp.ndarray:
        self.params = self.cma.ask()
        return self.jnp_stack(self.params)

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        self.cma.tell(self.params, -np.array(fitness))
        self._best_params = np.array(self.cma.result.xfavorite)

    @property
    def best_params(self) -> jnp.ndarray:
        return self.jnp_array(self._best_params)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self._best_params = np.array(params)
        self.cma.x0 = self._best_params.copy()

=== ./evojax/algo/open_es.py ===
import sys

import logging
from typing import Union
import numpy as np
import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


class OpenES(NEAlgorithm):
    """A wrapper around evosax's OpenAI Evolution Strategies.
    Implementation:
    https://github.com/RobertTLange/evosax/blob/main/evosax/strategies/open_es.py
    Reference: Salimans et al. (2017) - https://arxiv.org/pdf/1703.03864.pdf

    NOTE: More details on the optimizer configuration can be found here
    https://github.com/RobertTLange/evosax/blob/main/evosax/utils/optimizer.py
    """

    def __init__(
        self,
        param_size: int,
        pop_size: int,
        optimizer: str = "adam",
        optimizer_config: dict = {
            "lrate_init": 0.01,  # Initial learning rate
            "lrate_decay": 0.999,  # Multiplicative decay factor
            "lrate_limit": 0.001,  # Smallest possible lrate
            "beta_1": 0.99,  # beta_1 Adam
            "beta_2": 0.999,  # beta_2 Adam
            "eps": 1e-8,  # eps constant Adam denominator
        },
        init_stdev: float = 0.01,
        decay_stdev: float = 0.999,
        limit_stdev: float = 0.001,
        w_decay: float = 0.0,
        seed: int = 0,
        logger: logging.Logger = None,
    ):
        """Initialization function.

        Args:
            param_size - Parameter size.
            pop_size - Population size.
            elite_ratio - Population elite fraction used for gradient estimate.
            optimizer - Optimizer name ("sgd", "adam", "rmsprop", "clipup").
            optimizer_config - Configuration of optimizer hyperparameters.
            init_stdev - Initial scale of Gaussian perturbation.
            decay_stdev - Multiplicative scale decay between tell iterations.
            limit_stdev - Smallest scale (clipping limit).
            w_decay - L2 weight regularization coefficient.
            seed - Random seed for parameters sampling.
            logger - Logger.
        """

        # Delayed importing of evosax

        if sys.version_info.minor < 7:
            print("evosax, which is needed byOpenES, requires python>=3.7")
            print("  please consider upgrading your Python version.")
            sys.exit(1)

        try:
            import evosax
        except ModuleNotFoundError:
            print("You need to install evosax for its OpenES implementation:")
            print("  pip install evosax")
            sys.exit(1)

        if logger is None:
            self.logger = create_logger(name="OpenES")
        else:
            self.logger = logger

        self.param_size = param_size
        self.pop_size = abs(pop_size)
        self.rand_key = jax.random.PRNGKey(seed=seed)

        # Instantiate evosax's Open ES strategy
        self.es = evosax.OpenES(
            popsize=pop_size,
            num_dims=param_size,
            opt_name=optimizer,
        )

        # Set hyperparameters according to provided inputs
        # Set hyperparameters according to provided inputs
        self.es_params = self.es.default_params.replace(
            sigma_init=init_stdev,
            sigma_decay=decay_stdev,
            sigma_limit=limit_stdev,
            init_min=0.0,
            init_max=0.0,
        )

        # Update optimizer-specific parameters of Adam
        self.es_params = self.es_params.replace(
            opt_params=self.es_params.opt_params.replace(**optimizer_config)
        )

        # Initialize the evolution strategy state
        self.rand_key, init_key = jax.random.split(self.rand_key)
        self.es_state = self.es.initialize(init_key, self.es_params)

        # By default evojax assumes maximization of fitness score!
        # Evosax, on the other hand, minimizes!
        self.fit_shaper = evosax.FitnessShaper(
            centered_rank=True, w_decay=w_decay, maximize=True
        )

    def ask(self) -> jnp.ndarray:
        self.rand_key, ask_key = jax.random.split(self.rand_key)
        self.params, self.es_state = self.es.ask(
            ask_key, self.es_state, self.es_params
        )
        return self.params

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        # Reshape fitness to conform with evosax minimization
        fit_re = self.fit_shaper.apply(self.params, fitness)
        self.es_state = self.es.tell(
            self.params, fit_re, self.es_state, self.es_params
        )

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self.es_state.mean, copy=True)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self.es_state = self.es_state.replace(
            best_member=jnp.array(params, copy=True),
            mean=jnp.array(params, copy=True),
        )

=== ./evojax/algo/cma_evosax.py ===
import sys

import logging
from typing import Union
import numpy as np
import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


class CMA_ES(NEAlgorithm):
    """A wrapper around evosax's CMA-ES.
    Implementation: https://github.com/RobertTLange/evosax/blob/main/evosax/strategies/cma_es.py
    Reference: Hansen & Ostermeier (2008) - http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf
    """

    def __init__(
        self,
        param_size: int,
        pop_size: int,
        elite_ratio: float = 0.5,
        init_stdev: float = 0.1,
        w_decay: float = 0.0,
        seed: int = 0,
        logger: logging.Logger = None,
    ):
        """Initialization function.

        Args:
            param_size - Parameter size.
            pop_size - Population size.
            elite_ratio - Population elite fraction used for gradient estimate.
            init_stdev - Initial scale of istropic part of covariance.
            w_decay - L2 weight regularization coefficient.
            seed - Random seed for parameters sampling.
            logger - Logger.
        """

        # Delayed importing of evosax

        if sys.version_info.minor < 7:
            print("evosax, which is needed by CMA-ES, requires python>=3.7")
            print("  please consider upgrading your Python version.")
            sys.exit(1)

        try:
            import evosax
        except ModuleNotFoundError:
            print("You need to install evosax for its CMA-ES:")
            print("  pip install evosax")
            sys.exit(1)

        # Set up object variables.

        if logger is None:
            self.logger = create_logger(name="CMA_ES")
        else:
            self.logger = logger

        self.param_size = param_size
        self.pop_size = abs(pop_size)
        self.elite_ratio = elite_ratio
        self.rand_key = jax.random.PRNGKey(seed=seed)

        # Instantiate evosax's ARS strategy
        self.es = evosax.CMA_ES(
            popsize=pop_size,
            num_dims=param_size,
            elite_ratio=elite_ratio,
        )

        # Set hyperparameters according to provided inputs
        self.es_params = self.es.default_params.replace(sigma_init=init_stdev)

        # Initialize the evolution strategy state
        self.rand_key, init_key = jax.random.split(self.rand_key)
        self.es_state = self.es.initialize(init_key, self.es_params)

        # By default evojax assumes maximization of fitness score!
        # Evosax, on the other hand, minimizes!
        self.fit_shaper = evosax.FitnessShaper(w_decay=w_decay, maximize=True)

    def ask(self) -> jnp.ndarray:
        self.rand_key, ask_key = jax.random.split(self.rand_key)
        self.params, self.es_state = self.es.ask(
            ask_key, self.es_state, self.es_params
        )
        return self.params

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        # Reshape fitness to conform with evosax minimization
        fit_re = self.fit_shaper.apply(self.params, fitness)
        self.es_state = self.es.tell(
            self.params, fit_re, self.es_state, self.es_params
        )

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self.es_state.mean, copy=True)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self.es_state = self.es_state.replace(
            best_member=jnp.array(params, copy=True),
            mean=jnp.array(params, copy=True),
        )

=== ./evojax/algo/neat_wrapper.py ===
from typing import Union
import jax
import jax.numpy as jnp
import numpy as np
from evojax.algo.base import NEAlgorithm
from tensorneat.src.tensorneat.algorithm.neat import NEAT as TensorNEAT
from tensorneat.src.tensorneat.common import State
from tensorneat.src.tensorneat.genome import DefaultGenome, BiasNode
from tensorneat.src.tensorneat.common import ACT, AGG


class NEATWrapper(NEAlgorithm):
    """Wrapper to make TensorNEAT compatible with EvoJAX."""

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        hidden_size: int = 20,
        pop_size: int = 128,
        species_size: int = 10,
        max_nodes: int = 50,
        max_conns: int = 200,
        seed: int = 0,
    ):
        super().__init__()
        self.pop_size = pop_size
        self._best_params = None
        self.best_fitness = float("-inf")

        # Create genome
        self.genome = DefaultGenome(
            num_inputs=input_dim,
            num_outputs=output_dim,
            max_nodes=max_nodes,
            max_conns=max_conns,
            node_gene=BiasNode(
                activation_options=ACT.tanh,
                aggregation_options=AGG.sum,
            ),
            output_transform=ACT.tanh,
        )

        # Initialize NEAT algorithm
        self.neat = TensorNEAT(
            genome=self.genome,
            pop_size=pop_size,
            species_size=species_size,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            species_elitism=2,
            max_stagnation=15,
            spawn_number_change_rate=0.5,
        )

        # Initialize state with randkey
        initial_key = jax.random.PRNGKey(seed)
        self.state = State(randkey=initial_key)
        self.state = self.neat.setup(self.state)

        # Store current population
        self.current_pop_nodes = None
        self.current_pop_conns = None

    def ask(self) -> jnp.ndarray:
        """Get current population parameters."""
        self.current_pop_nodes, self.current_pop_conns = self.neat.ask(self.state)
        return jnp.arange(self.pop_size)

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        """Update algorithm with fitness results."""
        if isinstance(fitness, np.ndarray):
            fitness = jnp.array(fitness)

        self.state = self.neat.tell(self.state, fitness)

        # Update best params
        best_idx = jnp.argmax(fitness)
        if self._best_params is None or fitness[best_idx] > self.best_fitness:
            self.best_fitness = fitness[best_idx]
            self._best_params = (
                self.current_pop_nodes[best_idx],
                self.current_pop_conns[best_idx],
                self.state,
            )

    @property
    def best_params(self) -> jnp.ndarray:
        """Get best performing individual's parameters."""
        if self._best_params is None:
            raise ValueError("No best parameters available yet")
        return self._best_params

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        """Set best parameters."""
        raise NotImplementedError("Setting best_params directly not supported in NEAT")

    def get_params(self, idx: int) -> tuple:
        """Get parameters for specific index."""
        return (
            self.current_pop_nodes[idx],
            self.current_pop_conns[idx],
            self.state,
        )

=== ./evojax/algo/pgpe.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of the PGPE algorithm in JAX.

Ref: https://github.com/nnaisense/pgpelib/blob/release/pgpelib/pgpe.py
"""

import numpy as np
import logging
from typing import Optional
from typing import Union
from typing import Tuple
from functools import partial

import jax
import jax.numpy as jnp
from jax import random

try:
    from jax.example_libraries import optimizers
except ModuleNotFoundError:
    from jax.experimental import optimizers

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


@partial(jax.jit, static_argnums=(1,))
def process_scores(
    x: Union[np.ndarray, jnp.ndarray], use_ranking: bool
) -> jnp.ndarray:
    """Convert fitness scores to rank if necessary."""

    x = jnp.array(x)
    if use_ranking:
        ranks = jnp.zeros(x.size, dtype=int)
        ranks = ranks.at[x.argsort()].set(jnp.arange(x.size)).reshape(x.shape)
        return ranks / ranks.max() - 0.5
    else:
        return x


@jax.jit
def compute_reinforce_update(
    fitness_scores: jnp.ndarray, scaled_noises: jnp.ndarray, stdev: jnp.ndarray
) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """Compute the updates for the center and the standard deviation."""

    fitness_scores = fitness_scores.reshape((-1, 2))
    baseline = jnp.mean(fitness_scores)
    all_scores = (fitness_scores[:, 0] - fitness_scores[:, 1]).squeeze()
    all_avg_scores = fitness_scores.sum(axis=-1) / 2
    stdev_sq = stdev ** 2.0
    total_mu = scaled_noises * jnp.expand_dims(all_scores, axis=1) * 0.5
    total_sigma = (
        (jnp.expand_dims(all_avg_scores, axis=1) - baseline)
        * (scaled_noises ** 2 - jnp.expand_dims(stdev_sq, axis=0))
        / stdev
    )
    return total_mu.mean(axis=0), total_sigma.mean(axis=0)


@jax.jit
def update_stdev(
    stdev: jnp.ndarray, lr: float, grad: jnp.ndarray, max_change: float
) -> jnp.ndarray:
    """Update (and clip) the standard deviation."""

    allowed_delta = jnp.abs(stdev) * max_change
    min_allowed = stdev - allowed_delta
    max_allowed = stdev + allowed_delta
    return jnp.clip(stdev + lr * grad, min_allowed, max_allowed)


@partial(jax.jit, static_argnums=(3, 4))
def ask_func(
    key: jnp.ndarray,
    stdev: jnp.ndarray,
    center: jnp.ndarray,
    num_directions: int,
    solution_size: int,
) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:
    """A function that samples a population of parameters from Gaussian."""

    next_key, key = random.split(key)
    scaled_noises = random.normal(key, [num_directions, solution_size]) * stdev
    solutions = jnp.hstack(
        [center + scaled_noises, center - scaled_noises]
    ).reshape(-1, solution_size)
    return next_key, scaled_noises, solutions


class PGPE(NEAlgorithm):
    """Policy Gradient with Parameter-based Exploration (PGPE) algorithm.

    Ref: https://people.idsia.ch/~juergen/icann2008sehnke.pdf
    """

    def __init__(
        self,
        pop_size: int,
        param_size: int,
        init_params: Optional[Union[jnp.ndarray, np.ndarray]] = None,
        optimizer: Optional[str] = None,
        optimizer_config: Optional[dict] = None,
        center_learning_rate: float = 0.15,
        stdev_learning_rate: float = 0.1,
        init_stdev: Union[float, jnp.ndarray, np.ndarray] = 0.1,
        stdev_max_change: float = 0.2,
        solution_ranking: bool = True,
        seed: int = 0,
        logger: logging.Logger = None,
    ):
        """Initialization function.

        Args:
            pop_size - Population size.
            param_size - Parameter size.
            init_params - Initial parameters, all zeros if not given.
            optimizer - Possible values are {None, 'adam', 'clipup'}.
            optimizer_config - Configurations specific to the optimizer.
                               For None: No configuration is required.
                               For Adam: {'epsilon', 'beta1', 'beta2'}.
                               For ClipUp: {'momentum', 'max_speed'}.
            center_learning_rate - Learning rate for the Gaussian mean.
            stdev_learning_rate - Learning rate for the Gaussian stdev.
            init_stdev - Initial stdev for the Gaussian distribution.
            stdev_max_change - Maximum allowed change for stdev in abs values.
            solution_ranking - Should we treat the fitness as rankings or not.
            seed - Random seed for parameters sampling.
        """

        if logger is None:
            self._logger = create_logger(name="PGPE")
        else:
            self._logger = logger

        self.pop_size = abs(pop_size)
        if self.pop_size % 2 == 1:
            self.pop_size += 1
            self._logger.info(
                "Population size should be an even number, set to {}".format(
                    self.pop_size
                )
            )
        self._num_directions = self.pop_size // 2

        if init_params is None:
            self._center = np.zeros(abs(param_size))
        else:
            self._center = init_params
        self._center = jnp.array(self._center)
        if isinstance(init_stdev, float):
            self._stdev = np.ones(abs(param_size)) * abs(init_stdev)
        self._stdev = jnp.array(self._stdev)

        self._center_lr = abs(center_learning_rate)
        self._stdev_lr = abs(stdev_learning_rate)
        self._stdev_max_change = abs(stdev_max_change)
        self._solution_ranking = solution_ranking

        if optimizer_config is None:
            optimizer_config = {}
        decay_coef = optimizer_config.get("center_lr_decay_coef", 1.0)
        self._lr_decay_steps = optimizer_config.get(
            "center_lr_decay_steps", 1000
        )

        if optimizer == "adam":
            opt_init, opt_update, get_params = optimizers.adam(
                step_size=lambda x: self._center_lr * jnp.power(decay_coef, x),
                b1=optimizer_config.get("beta1", 0.9),
                b2=optimizer_config.get("beta2", 0.999),
                eps=optimizer_config.get("epsilon", 1e-8),
            )
        elif optimizer == "clipup":
            opt_init, opt_update, get_params = clipup(
                step_size=lambda x: self._center_lr * jnp.power(decay_coef, x),
                momentum=optimizer_config.get("momentum", 0.9),
                max_speed=optimizer_config.get("max_speed", 0.15),
                fix_gradient_size=optimizer_config.get(
                    "fix_gradient_size", True
                ),
            )
        else:
            opt_init, opt_update, get_params = optimizers.sgd(
                step_size=lambda x: self._center_lr * jnp.power(decay_coef, x),
            )
        self._t = 0
        self._opt_state = jax.jit(opt_init)(self._center)
        self._opt_update = jax.jit(opt_update)
        self._get_params = jax.jit(get_params)

        self._key = random.PRNGKey(seed=seed)
        self._solutions = None
        self._scaled_noises = None

    def ask(self) -> jnp.ndarray:
        self._key, self._scaled_noises, self._solutions = ask_func(
            self._key,
            self._stdev,
            self._center,
            self._num_directions,
            self._center.size,
        )
        return self._solutions

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        fitness_scores = process_scores(fitness, self._solution_ranking)
        grad_center, grad_stdev = compute_reinforce_update(
            fitness_scores=fitness_scores,
            scaled_noises=self._scaled_noises,
            stdev=self._stdev,
        )
        self._opt_state = self._opt_update(
            self._t // self._lr_decay_steps, -grad_center, self._opt_state
        )
        self._t += 1
        self._center = self._get_params(self._opt_state)
        self._stdev = update_stdev(
            stdev=self._stdev,
            lr=self._stdev_lr,
            max_change=self._stdev_max_change,
            grad=grad_stdev,
        )

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self._center, copy=True)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self._center = jnp.array(params, copy=True)


@optimizers.optimizer
def clipup(
    step_size: float,
    momentum: float = 0.9,
    max_speed: float = 0.15,
    fix_gradient_size: bool = True,
):
    """Construct optimizer triple for ClipUp."""

    step_size = optimizers.make_schedule(step_size)

    def init(x0):
        v0 = jnp.zeros_like(x0)
        return x0, v0

    def update(i, g, state):
        x, v = state
        g = jax.lax.cond(
            fix_gradient_size,
            lambda p: p / jnp.sqrt(jnp.sum(p * p)),
            lambda p: p,
            g,
        )
        step = g * step_size(i)
        v = momentum * v + step
        # Clip.
        length = jnp.sqrt(jnp.sum(v * v))
        v = jax.lax.cond(
            length > max_speed, lambda p: p * max_speed / length, lambda p: p, v
        )
        return x - v, v

    def get_params(state):
        x, _ = state
        return x

    return init, update, get_params

=== ./evojax/algo/ars_native.py ===
import jax
import jax.numpy as jnp
import numpy as np
from evojax.algo.base import NEAlgorithm
from typing import Union
try:
    from jax.example_libraries import optimizers
except ModuleNotFoundError:
    from jax.experimental import optimizers
import logging
from evojax.util import create_logger


def get_optim(opt, optimizer_config, init_params):
	''' convenience function for creating optimizer functions from configs '''
	lrate_init = optimizer_config.get('lrate_init', 1e-3) 
	lrate_limit = optimizer_config.get('lrate_limit', 1e-6)
	decay_coef = optimizer_config.get('decay_coef', 1)
	step_size=lambda x: jnp.maximum(lrate_init * jnp.power(decay_coef, x), lrate_limit)

	if opt == 'sgd':
		opt_init, opt_update, get_params = optimizers.sgd(
			step_size=step_size,
		)
	elif opt == 'adam':
		opt_init, opt_update, get_params = optimizers.adam(
			step_size=step_size,
			b1=optimizer_config.get("beta1", 0.9),
			b2=optimizer_config.get("beta2", 0.999),
			eps=optimizer_config.get("epsilon", 1e-8),
		)
	else:
		raise Exception("optimizer not supported. Choose between sgd and adam")
	opt_state = jax.jit(opt_init)(init_params)
	opt_update = jax.jit(opt_update)
	get_params = jax.jit(get_params)

	return opt_state, opt_update, get_params



class ARS_native(NEAlgorithm):
	''' Augmented Random Search 
	    Mania et al. (2018) https://arxiv.org/pdf/1803.07055.pdf'''
	
	def __init__(
		self, 
		param_size, 
		pop_size, 
		elite_ratio, 
		decay_stdev=0.999, 
		limit_stdev=0.01, 
		init_stdev=0.03, 
		seed=0, 
		version=1,
		optimizer='sgd',
		optimizer_config={},
		logger: logging.Logger = None
	):
		if logger is None:
			self.logger = create_logger(name="ARS_native")
		else:
			self.logger = logger

		assert pop_size % 2 == 0   # total perturbations considering positive and negative directions 
		self.pop_size = pop_size
		assert 0 < elite_ratio <= 1
		self.elite_ratio = elite_ratio
		self.elite_pop_size = int(self.pop_size / 2 * self.elite_ratio)
		self.directions = pop_size // 2
		self.params_population = None
		self.param_size = param_size

		self.noise = None
		self.stdev = init_stdev 
		self.rand_key, _key = jax.random.split(key=jax.random.PRNGKey(seed=seed), num=2)
		init_min, init_max = 0, 0
		self.mean_params = jax.random.uniform(_key, (param_size,), minval=init_min, maxval=init_max) 

		self.version = version
		self.decay_stdev = decay_stdev
		self.limit_stdev = limit_stdev
		self.opt_state, self.opt_update, self.opt_get_params = get_optim(optimizer, optimizer_config, self.mean_params)
		self._t = 0

		self.lr_decay_steps = optimizer_config.get('lr_decay_steps', 1)


	def ask(self):
		self.rand_key, _key = jax.random.split(key=self.rand_key, num=2)
		self.noise = jax.random.normal(key=_key, shape=(self.directions, self.param_size))
		perturbation = jnp.concatenate([self.noise, -self.noise], axis=0)

		# evaluate each perturbation in both directions 
		self.params_population = self.mean_params + self.stdev * perturbation
		return self.params_population

	def tell(self, fitness):
		pos = fitness[:self.directions]
		neg = fitness[self.directions:]

		selected_dir_ids = jnp.minimum(pos, neg).argsort()[:self.elite_pop_size]
		selected_fitness_stdev = jnp.std(
			jnp.concatenate(
				[pos[selected_dir_ids], neg[selected_dir_ids]]
			)
		) + 1e-05
		fitness_diff = pos[selected_dir_ids]-neg[selected_dir_ids]
		selected_noise = self.noise[selected_dir_ids]
		update = 1 / (self.elite_pop_size*selected_fitness_stdev) * jnp.dot(selected_noise.T, fitness_diff) 

		self.opt_state = self.opt_update(self._t // self.lr_decay_steps, -update, self.opt_state)
		self._t += 1
		self.mean_params = self.opt_get_params(self.opt_state)

		self.stdev = jnp.maximum(self.stdev*self.decay_stdev, self.limit_stdev)

	@property
	def best_params(self) -> jnp.ndarray:	
		return self.mean_params
	
	@best_params.setter
	def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
		self.mean_params = params 

=== ./evojax/algo/ars.py ===
import sys

import logging
from typing import Union, Optional
import numpy as np
import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


class ARS(NEAlgorithm):
    """A wrapper around evosax's Augmented Random Search.
    Implementation: https://github.com/RobertTLange/evosax/blob/main/evosax/strategies/ars.py
    Reference: Mania et al. (2018) - https://arxiv.org/pdf/1803.07055.pdf

    NOTE: More details on the optimizer configuration can be found here
    https://github.com/RobertTLange/evosax/blob/main/evosax/utils/optimizer.py
    """

    def __init__(
        self,
        param_size: int,
        pop_size: int,
        elite_ratio: float = 0.2,
        optimizer: str = "clipup",
        optimizer_config: dict = {
            "lrate_init": 0.15,  # Initial learning rate
            "lrate_decay": 0.999,  # Multiplicative decay factor
            "lrate_limit": 0.05,  # Smallest possible lrate
            "max_speed": 0.3,  # Max. clipping velocity
            "momentum": 0.9,  # Momentum coefficient
        },
        init_stdev: float = 0.01,
        decay_stdev: float = 0.999,
        limit_stdev: float = 0.001,
        w_decay: float = 0.0,
        seed: int = 0,
        logger: Optional[logging.Logger] = None,
    ):
        """Initialization function.

        Args:
            param_size - Parameter size.
            pop_size - Population size.
            elite_ratio - Population elite fraction used for gradient estimate.
            optimizer - Optimizer name ("sgd", "adam", "rmsprop", "clipup").
            optimizer_config - Configuration of optimizer hyperparameters.
            init_stdev - Initial scale of Gaussian perturbation.
            decay_stdev - Multiplicative scale decay between tell iterations.
            limit_stdev - Smallest scale (clipping limit).
            w_decay - L2 weight regularization coefficient.
            seed - Random seed for parameters sampling.
            logger - Logger.
        """

        # Delayed importing of evosax

        if sys.version_info.minor < 7:
            print(
                "evosax, which is needed by Augmented Random Search, requires"
                " python>=3.7"
            )
            print("  please consider upgrading your Python version.")
            sys.exit(1)

        try:
            import evosax
        except ModuleNotFoundError:
            print("You need to install evosax for its Augmented Random Search:")
            print("  pip install evosax")
            sys.exit(1)

        # Set up object variables.

        if logger is None:
            self.logger = create_logger(name="ARS")
        else:
            self.logger = logger

        self.param_size = param_size
        self.pop_size = abs(pop_size)
        self.elite_ratio = elite_ratio
        self.rand_key = jax.random.PRNGKey(seed=seed)

        # Instantiate evosax's ARS strategy
        self.es = evosax.ARS(
            popsize=pop_size,
            num_dims=param_size,
            elite_ratio=elite_ratio,
            opt_name=optimizer,
        )

        # Set hyperparameters according to provided inputs
        self.es_params = self.es.default_params.replace(
            sigma_init=init_stdev,
            sigma_decay=decay_stdev,
            sigma_limit=limit_stdev,
            init_min=0.0,
            init_max=0.0,
        )

        # Update optimizer-specific parameters of Adam
        self.es_params = self.es_params.replace(
            opt_params=self.es_params.opt_params.replace(**optimizer_config)
        )

        # Initialize the evolution strategy state
        self.rand_key, init_key = jax.random.split(self.rand_key)
        self.es_state = self.es.initialize(init_key, self.es_params)

        # By default evojax assumes maximization of fitness score!
        # Evosax, on the other hand, minimizes!
        self.fit_shaper = evosax.FitnessShaper(w_decay=w_decay, maximize=True)

    def ask(self) -> jnp.ndarray:
        self.rand_key, ask_key = jax.random.split(self.rand_key)
        self.params, self.es_state = self.es.ask(
            ask_key, self.es_state, self.es_params
        )
        return self.params

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        # Reshape fitness to conform with evosax minimization
        fit_re = self.fit_shaper.apply(self.params, fitness)
        self.es_state = self.es.tell(
            self.params, fit_re, self.es_state, self.es_params
        )

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self.es_state.mean, copy=True)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self.es_state = self.es_state.replace(
            best_member=jnp.array(params, copy=True),
            mean=jnp.array(params, copy=True),
        )

=== ./evojax/algo/cma_jax.py ===
"""
**Experimental** CMA-ES optimizer using JAX backend.

Code in this file is an adaption from <https://github.com/CyberAgentAILab/cmaes/blob/main/cmaes/_cma.py>
which is a faithful implementation of CMA-ES (Covariance matrix adaptation evolution strategy)
described in <https://arxiv.org/abs/1604.00772>.

Overall, the adaption replaces the NumPy backend with JAX. To facilitate efficient computation in JAX, it:
- Uses stateful computation for JAX that is JIT-able.
- Enables paralleling sampling.

Some edge-case are not concerned for now:
- No pickling functionality (`__getstate__()` and `__setstate__()`).
- No bound / repeated sampling for bounded parameters. This makes `ask` easier.
- No `funhist` / early stop. This makes `tell` and stateful computation easier.

This is still an experimental implementation and has not been thoroughly-tested yet.
"""

from typing import NamedTuple
from functools import partial
import logging
import math
from typing import Optional

import numpy as np

import jax
from jax import numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


_EPS = 1e-8


# We set thresholds for mean and sigma respectively.
_MEAN_MAX_X64 = 1e302
_SIGMA_MAX_X64 = 1e302
_MEAN_MAX_X32 = 1e32
_SIGMA_MAX_X32 = 1e32


class _HyperParameters(NamedTuple):
    n_dim: int
    pop_size: int
    mu: int


class _Coefficients(NamedTuple):
    mu_eff: jnp.ndarray
    cc: jnp.ndarray
    c1: jnp.ndarray
    cmu: jnp.ndarray
    c_sigma: jnp.ndarray
    d_sigma: jnp.ndarray
    cm: jnp.ndarray
    chi_n: jnp.ndarray
    weights: jnp.ndarray
    sigma_max: jnp.ndarray


class _State(NamedTuple):
    p_sigma: jnp.ndarray
    pc: jnp.ndarray
    mean: jnp.ndarray
    C: jnp.ndarray
    sigma: jnp.ndarray
    D: jnp.ndarray
    B: jnp.ndarray
    g: int
    key: jnp.ndarray


class CMA_ES_JAX(NEAlgorithm):
    """CMA-ES stochastic optimizer class with ask-and-tell interface, using JAX backend.
    Args:
        param_size:
            A parameter size.
        pop_size:
            A population size (optional).
            If not specified, a proper value will be inferred from param_size.
        mean:
            Initial mean vector of multi-variate gaussian distributions (optional).
            If specified, it must have a dimension of (param_size, ).
            If not specified, a default value of all-zero array will be used.
        init_stdev:
            Initial standard deviation of covariance matrix (optional).
            If not specified, a default value of 0.1 will be used.
        seed:
            A seed number (optional).
        cov:
            A covariance matrix (optional).
        logger:
            A logging.Logger instance (optional).
            If not specified, a new one will be created.
        enable_numeric_check:
            A bool indicating whether to enable numeric check (optional).
            If True, a numeric check for mean and standard deviation is enabled.
            The default value is False.
    """

    def __init__(
        self,
        param_size: int,
        pop_size: Optional[int] = None,
        mean: Optional[jnp.ndarray] = None,
        init_stdev: Optional[float] = 0.1,
        seed: Optional[int] = 0,
        cov: Optional[jnp.ndarray] = None,
        logger: logging.Logger = None,
        enable_numeric_check: Optional[bool] = False,
    ):
        if mean is None:
            mean = jnp.zeros(param_size)
        else:
            mean = ensure_jnp(mean)
            assert mean.shape == (param_size,), \
                f"Both params_size and mean are specified." \
                f"In this case,  mean (whose shape is {mean.shape}) must have a dimension of (param_size, )" \
                f" (i.e. {(param_size, )}), which is not true."
        mean = ensure_jnp(mean)

        if enable_numeric_check:
            mean_max = ensure_jnp(_MEAN_MAX_X64 if jax.config.jax_enable_x64 else _MEAN_MAX_X32)
            sigma_max = ensure_jnp(_SIGMA_MAX_X64 if jax.config.jax_enable_x64 else _SIGMA_MAX_X32)
        else:
            mean_max = ensure_jnp(jnp.inf)
            sigma_max = ensure_jnp(jnp.inf)

        if enable_numeric_check:
            assert jnp.all(
                jnp.abs(mean) <= mean_max
            ), f"Abs of all elements of mean vector must be less than {mean_max}"

        n_dim = len(mean)
        assert n_dim > 1, "The dimension of mean must be larger than 1"

        assert init_stdev > 0, "init_stdev must be non-zero positive value"
        sigma = init_stdev
        sigma = ensure_jnp(sigma)

        population_size = pop_size
        if population_size is None:
            population_size = 4 + math.floor(3 * math.log(n_dim))  # (eq. 48)
        assert population_size > 0, "pop_size must be non-zero positive value."

        mu = population_size // 2

        # (eq.49)
        weights_prime = jnp.array(
            [
                math.log((population_size + 1) / 2) - math.log(i + 1)
                for i in range(population_size)
            ]
        )
        mu_eff = (jnp.sum(weights_prime[:mu]) **
                  2) / jnp.sum(weights_prime[:mu] ** 2)
        mu_eff_minus = (
            jnp.sum(weights_prime[mu:]) ** 2) / jnp.sum(weights_prime[mu:] ** 2)

        # learning rate for the rank-one update
        alpha_cov = 2.0
        c1 = alpha_cov / ((n_dim + 1.3) ** 2 + mu_eff)
        assert isinstance(c1, jnp.ndarray)
        # learning rate for the rank-μ update
        cmu = min(
            1 - c1 - 1e-8,  # 1e-8 is for large pop_size.
            alpha_cov
            * (mu_eff - 2 + 1 / mu_eff)
            / ((n_dim + 2) ** 2 + alpha_cov * mu_eff / 2),
        )
        assert c1 + cmu <= 1, "invalid learning rate for the rank-one and/or rank-μ update"

        min_alpha = min(
            1 + c1 / cmu,  # eq.50
            1 + (2 * mu_eff_minus) / (mu_eff + 2),  # eq.51
            (1 - c1 - cmu) / (n_dim * cmu),  # eq.52
        )

        # (eq.53)
        positive_sum = jnp.sum(weights_prime[weights_prime > 0])
        negative_sum = jnp.sum(jnp.abs(weights_prime[weights_prime < 0]))
        weights = jnp.where(
            weights_prime >= 0,
            1 / positive_sum * weights_prime,
            min_alpha / negative_sum * weights_prime,
        )
        cm = 1  # (eq. 54)

        # learning rate for the cumulation for the step-size control (eq.55)
        c_sigma = (mu_eff + 2) / (n_dim + mu_eff + 5)
        d_sigma = 1 + 2 * \
            max(0, math.sqrt((mu_eff - 1) / (n_dim + 1)) - 1) + c_sigma
        assert (
            c_sigma < 1
        ), "invalid learning rate for cumulation for the step-size control"

        # learning rate for cumulation for the rank-one update (eq.56)
        cc = (4 + mu_eff / n_dim) / (n_dim + 4 + 2 * mu_eff / n_dim)
        assert cc <= 1, "invalid learning rate for cumulation for the rank-one update"

        self.hyper_parameters = _HyperParameters(
            n_dim=n_dim,
            pop_size=population_size,
            mu=mu,
        )

        self.coefficients = _Coefficients(
            mu_eff=mu_eff,
            cc=cc,
            c1=c1,
            cmu=cmu,
            c_sigma=c_sigma,
            d_sigma=d_sigma,
            cm=cm,
            # E||N(0, I)|| (p.28)
            chi_n=math.sqrt(n_dim) * (
                1.0 - (1.0 / (4.0 * n_dim)) +
                1.0 / (21.0 * (n_dim ** 2))
            ),
            weights=weights,
            sigma_max=sigma_max,
        )

        # evolution path (state)
        if cov is None:
            cov = jnp.eye(n_dim)
        else:
            assert cov.shape == (
                n_dim, n_dim), "Invalid shape of covariance matrix"

        self.state = _State(
            p_sigma=jnp.zeros(n_dim),
            pc=jnp.zeros(n_dim),
            mean=mean,
            C=cov,
            sigma=sigma,
            D=None,
            B=None,
            g=0,
            key=jax.random.PRNGKey(seed),
        )

        # Below are helper vars.
        self._latest_solutions = None
        if logger is None:
            # Change this name accordingly
            self.logger = create_logger(name="CMA")
        else:
            self.logger = logger

    @property
    def dim(self) -> int:
        """A number of dimensions"""
        return self.hyper_parameters.n_dim

    @property
    def population_size(self) -> int:
        """A population size"""
        return self.hyper_parameters.pop_size

    @property
    def pop_size(self) -> int:
        """A population size, as expected by EvoJAX trainer."""
        return self.hyper_parameters.pop_size

    @property
    def generation(self) -> int:
        """Generation number which is monotonically incremented
        when multi-variate gaussian distribution is updated."""
        return self.state.g

    def _eigen_decomposition(self) -> jnp.ndarray:
        if self.state.B is not None and self.state.D is not None:
            return self.state.B, self.state.D
        B, D, C = _eigen_decomposition_core(self.state.C)
        self.state = self.state._replace(B=B, D=D, C=C)
        return B, D

    def _ask(self, n_samples) -> jnp.ndarray:
        """Real implementaiton of ask, which samples multiple parameters in parallel."""
        n_dim = self.hyper_parameters.n_dim
        mean, sigma = self.state.mean, self.state.sigma
        B, D = self._eigen_decomposition()

        key, subkey = jax.random.split(self.state.key)
        self.state = self.state._replace(key=key)
        subkey = jax.random.split(subkey, n_samples)
        x = _batch_sample_solution(subkey, B, D, n_dim, mean, sigma)

        return x

    def ask(self, n_samples: int = None) -> jnp.ndarray:
        """A wrapper of _ask, which handles optional n_samples and saves latest samples."""

        if n_samples is None:
            n_samples = self.pop_size

        x = self._ask(n_samples)
        self._latest_solutions = x
        return x

    def tell(self, fitness: jnp.ndarray, solutions: Optional[jnp.ndarray] = None) -> None:
        """Tell evaluation values as fitness."""

        if solutions is None:
            assert self._latest_solutions is not None, \
                "`soltuions` is not given, expecting using latest samples but this was not done."
            assert self._latest_solutions.shape[0] == self.hyper_parameters.pop_size, \
                f"Latest samples (shape={self._latest_solutions.shape}) not having pop_size-length ({self._popsize})."
            solutions = self._latest_solutions
        else:
            assert solutions.shape[0] == self.hyper_parameters.pop_size, \
                "Given solutions must have pop_size-length, which is not ture."

        # We want maximization, while the following logics is for minimimzation.
        # Handle this calse by simply revert fitness
        fitness = - fitness

        # real computation

        # - must do it as _tell_core below expects B, C, D to be computed.
        B, D = self._eigen_decomposition()
        self.state = self.state._replace(B=B, D=D)

        next_state = _tell_core(hps=self.hyper_parameters, coeff=self.coefficients,
                                state=self.state, fitness=fitness, solutions=solutions)

        self.state = next_state

    def save_state(self) -> dict:
        fn = lambda x: np.array(x) if isinstance(x, jnp.ndarray) else x

        state = {
            key: fn(value)
            for key, value in self.state._asdict().items()
        }
        return state

    def load_state(self, saved_state: dict):
        fn = lambda x: jnp.array(x) if isinstance(x, np.ndarray) else x

        self.state = _State(**{
            key: fn(value)
            for key, value in saved_state.items()
        })

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self.state.mean, copy=True)

    def get_best_params_ref(self) -> jnp.ndarray:
        return self.state.mean


def ensure_jnp(x):
    '''Return a copy of x that is ensured to be jnp.ndarray.'''
    return jnp.array(x)


@partial(jax.jit, static_argnums=0)
def _tell_core(
    hps: _HyperParameters,
    coeff: _Coefficients,
    state: _State,
    fitness: jnp.ndarray,
    solutions: jnp.ndarray,
) -> _State:
    next_state = state

    g = state.g + 1
    next_state = next_state._replace(g=g)

    ranking = jnp.argsort(fitness, axis=0)

    sorted_solutions = solutions[ranking]

    B, D = state.B, state.D

    # Sample new population of search_points, for k=1, ..., pop_size
    B, D = state.B, state.D  # already computed.
    next_state = next_state._replace(B=None, D=None)

    x_k = jnp.array(sorted_solutions)  # ~ N(m, σ^2 C)
    y_k = (x_k - state.mean) / state.sigma  # ~ N(0, C)

    # Selection and recombination
    # y_w = jnp.sum(y_k[: hps.mu].T * coeff.weights[: hps.mu], axis=1)  # eq.41
    # use lax.dynamic_slice_in_dim here:
    y_w = jnp.sum(
        jax.lax.dynamic_slice_in_dim(y_k, 0, hps.mu, axis=0).T *
        jax.lax.dynamic_slice_in_dim(coeff.weights,  0, hps.mu, axis=0),
        axis=1,
    )
    mean = state.mean + coeff.cm * state.sigma * y_w
    next_state = next_state._replace(mean=mean)

    # Step-size control
    C_2 = B.dot(jnp.diag(1 / D)).dot(B.T)  # C^(-1/2) = B D^(-1) B^T
    p_sigma = (1 - coeff.c_sigma) * state.p_sigma + jnp.sqrt(
        coeff.c_sigma * (2 - coeff.c_sigma) * coeff.mu_eff
    ) * C_2.dot(y_w)
    next_state = next_state._replace(p_sigma=p_sigma)

    norm_p_sigma = jnp.linalg.norm(state.p_sigma)
    sigma = state.sigma * jnp.exp(
        (coeff.c_sigma / coeff.d_sigma) * (norm_p_sigma / coeff.chi_n - 1)
    )
    sigma = jnp.min(jnp.array([sigma, coeff.sigma_max]))
    next_state = next_state._replace(sigma=sigma)

    # Covariance matrix adaption
    h_sigma_cond_left = norm_p_sigma / jnp.sqrt(
        1 - (1 - coeff.c_sigma) ** (2 * (state.g + 1))
    )
    h_sigma_cond_right = (1.4 + 2 / (hps.n_dim + 1)) * coeff.chi_n
    # h_sigma = 1.0 if h_sigma_cond_left < h_sigma_cond_right else 0.0  # (p.28)
    h_sigma = jax.lax.cond(
        pred=(h_sigma_cond_left < h_sigma_cond_right),
        true_fun=(lambda: 1.0),
        false_fun=(lambda: 0.0),
    )

    # (eq.45)
    pc = (1 - coeff.cc) * state.pc + h_sigma * \
        jnp.sqrt(coeff.cc * (2 - coeff.cc) * coeff.mu_eff) * y_w
    next_state = next_state._replace(pc=pc)

    # (eq.46)
    w_io = coeff.weights * jnp.where(
        coeff.weights >= 0,
        1,
        hps.n_dim / (jnp.linalg.norm(C_2.dot(y_k.T), axis=0) ** 2 + _EPS),
    )

    delta_h_sigma = (1 - h_sigma) * coeff.cc * (2 - coeff.cc)  # (p.28)
    # assert delta_h_sigma <= 1

    # (eq.47)
    rank_one = jnp.outer(state.pc, state.pc)

    # This way of computing rank_mu can lead to OOM:
    # rank_mu = jnp.sum(
    #    jnp.array([w * jnp.outer(y, y) for w, y in zip(w_io, y_k)]), axis=0
    # )
    # Try another way of computing rank_mu
    rank_mu = jnp.zeros_like(state.C)
    for w, y in zip(w_io, y_k):
        rank_mu = rank_mu + w * jnp.outer(y, y)

    C = (
        (
            1
            + coeff.c1 * delta_h_sigma
            - coeff.c1
            - coeff.cmu * jnp.sum(coeff.weights)
        )
        * state.C
        + coeff.c1 * rank_one
        + coeff.cmu * rank_mu
    )
    next_state = next_state._replace(C=C)

    return next_state


def _sample_solution(key, B, D, n_dim, mean, sigma) -> jnp.ndarray:
    z = jax.random.normal(key, shape=(n_dim,))   # ~ N(0, I)
    y = B.dot(jnp.diag(D)).dot(z)  # ~ N(0, C)
    x = mean + sigma * y  # ~ N(m, σ^2 C)
    return x


_batch_sample_solution = jax.jit(
    jax.vmap(
        _sample_solution,
        in_axes=(0, None, None, None, None, None),
        out_axes=0
    ),
    static_argnums=3,
)


@jax.jit
def _eigen_decomposition_core(_C):
    _C = (_C + _C.T) / 2
    D2, B = jnp.linalg.eigh(_C)
    D = jnp.sqrt(jnp.where(D2 < 0, _EPS, D2))
    _C = jnp.dot(jnp.dot(B, jnp.diag(D ** 2)), B.T)
    _B, _D = B, D
    return _B, _D, _C

=== ./evojax/algo/simple_ga.py ===
import logging
import numpy as np
from typing import Union
from typing import Tuple

import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


class SimpleGA(NEAlgorithm):
    """A simple genetic algorithm implementing truncation selection."""

    def __init__(self,
                 param_size: int,
                 pop_size: int,
                 truncation_divisor: int = 2,
                 sigma: float = 0.01,
                 seed: int = 0,
                 logger: logging.Logger = None):
        """Initialization function.

        Args:
            param_size - Parameter size.
            pop_size - Population size.
            truncation_divisor - Number by which the population is truncated
                                 every iteration.
            sigma - Variance of normal distribution for parameter perturbation
            seed - Random seed for parameters sampling.
            logger - Logger
        """

        if logger is None:
            self.logger = create_logger(name='SimpleGA')
        else:
            self.logger = logger

        self.param_size = param_size

        self.pop_size = abs(pop_size)
        self.truncation_divisor = abs(truncation_divisor)

        if self.pop_size % 2 == 1:
            self.pop_size += 1
            self.logger.info(
                'Population size should be an even number, set to {}'.format(
                    self.pop_size))

        if self.pop_size % self.truncation_divisor != 0:
            self.truncation_divisor = 2
            self.logger.info(
                'Population size must be a multiple of truncation divisor, \
                 set to {}'.format(self.truncation_divisor))

        self.truncation = self.pop_size // self.truncation_divisor
        self.sigma = sigma

        self.params = jnp.zeros((pop_size, param_size))
        self._best_params = None

        self.rand_key = jax.random.PRNGKey(seed=seed)

        self.jnp_array = jax.jit(jnp.array)

        def ask_fn(key: jnp.ndarray,
                   params: Union[np.ndarray,
                                 jnp.ndarray]) -> Tuple[jnp.ndarray,
                                                        Union[np.ndarray,
                                                              jnp.ndarray]]:

            next_key, sample_key = jax.random.split(key=key, num=2)

            perturbations = jax.random.normal(key=sample_key,
                                              shape=(self.pop_size,
                                                     self.param_size))

            return next_key, params + perturbations * self.sigma

        self.ask_fn = jax.jit(ask_fn)

        def tell_fn(fitness: Union[np.ndarray,
                                   jnp.ndarray],
                    params: Union[np.ndarray,
                                  jnp.ndarray]) -> Union[np.ndarray,
                                                         jnp.ndarray]:

            params = params[fitness.argsort(axis=0)]
            params = params[-self.truncation:].repeat(self.truncation_divisor,
                                                      axis=0)
            return params

        self.tell_fn = jax.jit(tell_fn)

    def ask(self) -> jnp.ndarray:
        self.rand_key, self.params = self.ask_fn(self.rand_key, self.params)
        return self.params

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        self.params = self.tell_fn(fitness, self.params)
        self._best_params = self.params[-1]

    @property
    def best_params(self) -> jnp.ndarray:
        return self.jnp_array(self._best_params)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self.params = jnp.repeat(params[None, :], self.pop_size, axis=0)
        self._best_params = jnp.array(params, copy=True)

=== ./evojax/algo/__init__.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .base import NEAlgorithm
from .base import QualityDiversityMethod
from .cma_wrapper import CMA
from .pgpe import PGPE
from .ars import ARS
from .simple_ga import SimpleGA
from .open_es import OpenES
from .cma_evosax import CMA_ES
from .sep_cma_es import Sep_CMA_ES
from .cma_jax import CMA_ES_JAX
from .map_elites import MAPElites
from .iamalgam import iAMaLGaM
from .fcrfmc import FCRFMC
from .crfmnes import CRFMNES
from .ars_native import ARS_native
from .fpgpec import FPGPEC

Strategies = {
    "CMA": CMA,
    "PGPE": PGPE,
    "SimpleGA": SimpleGA,
    "ARS": ARS,
    "OpenES": OpenES,
    "CMA_ES": CMA_ES,
    "Sep_CMA_ES": Sep_CMA_ES,
    "CMA_ES_JAX": CMA_ES_JAX,
    "MAPElites": MAPElites,
    "iAMaLGaM": iAMaLGaM,
    "FCRFMC": FCRFMC,
    "CRFMNES": CRFMNES,
    "ARS_native": ARS_native,
    "FPGPEC": FPGPEC,
}

__all__ = [
    "NEAlgorithm",
    "QualityDiversityMethod",
    "CMA",
    "PGPE",
    "ARS",
    "SimpleGA",
    "OpenES",
    "CMA_ES",
    "Sep_CMA_ES",
    "CMA_ES_JAX",
    "MAPElites",
    "iAMaLGaM",
    "FCRFMC",
    "CRFMNES",
    "Strategies",
    "ARS_native",
    "FPGPEC",
]

=== ./evojax/algo/README.md ===
The following table summarizes the available neuroevolution algorithms and their performance.  
We hope this helps EvoJAX users choose the appropriate ones for their experiments.

| Algorithms                                                                                                                                             | Description                                                                                                                                                                                                                                  | Contributors                                    | Performance                                                          |
|--------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|----------------------------------------------------------------------|
| [ARS](https://arxiv.org/pdf/1803.07055.pdf)                                                                                                            | This is a wrapper of the ARS implementation in [evosax](https://github.com/RobertTLange/evosax). ([source](https://github.com/google/evojax/blob/main/evojax/algo/ars.py))                                                                   | [RobertTLange](https://github.com/RobertTLange)  | [PR](https://github.com/google/evojax/pull/9#issue-1143656302)       |
| [ARS_native](https://arxiv.org/abs/1803.07055)                                                                                                            | Native jax implementation of Augmented Random Search. ([source](https://github.com/google/evojax/blob/main/evojax/algo/ars_native.py))                                                                   | [EdoardoPona](https://github.com/EdoardoPona)  | [PR](https://github.com/google/evojax/pull/47)       |
| [CMA-ES](https://arxiv.org/abs/1604.00772)                                                                                                             | This is a wrapper of the [original](https://github.com/CMA-ES/pycma) implementation. Notice that this runs on CPUs. ([source](https://github.com/google/evojax/blob/main/evojax/algo/cma_wrapper.py))                                        | EvoJAX team | TODO                                                                | 
| [CMA-ES](https://arxiv.org/abs/1604.00772)                                                                                                             | This is a wrapper of the CMA-ES implementation in [evosax](https://github.com/RobertTLange/evosax). ([source](https://github.com/google/evojax/blob/main/evojax/algo/cma_evosax.py))                                        | [RobertTLange](https://github.com/RobertTLange) | [PR](https://github.com/google/evojax/pull/21)                                                                | 
| [CMA-ES](https://arxiv.org/abs/1604.00772)                                                                                                             | This is a wrapper of the Sep-CMA-ES implementation in [evosax](https://github.com/RobertTLange/evosax). It runs fast but the covariance matrix in this implementation is diagonal. ([source](https://github.com/google/evojax/blob/main/evojax/algo/sep_cma_es.py))                     | [RobertTLange](https://github.com/RobertTLange) | [PR](https://github.com/google/evojax/pull/20)                                                                | 
| [CMA-ES](https://arxiv.org/abs/1604.00772)                                                                                                             | This is a CMA-ES optimizer using JAX backend, adpated from [this](https://github.com/CyberAgentAILab/cmaes/blob/main/cmaes/_cma.py) faithful implementation of the original CMA-ES algorithm. ([source](https://github.com/google/evojax/blob/main/evojax/algo/cma_jax.py))                     | EvoJAX Team | [PR](https://github.com/google/evojax/pull/32)                                                                | 
| [CR-FM-NES](https://arxiv.org/abs/2201.11422)                                                                                                             | This is a CR-FM-NES optimizer using JAX backend, adapted from [this](https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/crfmnes.py) implementation of the original CR-FM-NES algorithm. ([performance](https://github.com/dietmarwo/fast-cma-es/blob/master/tutorials/EvoJax.adoc)) ([source](https://github.com/google/evojax/blob/main/evojax/algo/crfmnes.py))                    | [dietmarwo](https://github.com/dietmarwo) | [PR](https://github.com/google/evojax/pull/46)
| [CR-FM-NES](https://arxiv.org/abs/2201.11422)                                                                                                             | This is a wrapper of the CR-FM-NES C++/Eigen implementation from [fcmaes](https://github.com/dietmarwo/fast-cma-es), using [this](https://github.com/dietmarwo/fast-cma-es/blob/master/_fcmaescpp/crfmnes.cpp) implementation of the original CR-FM-NES algorithm. ([performance](https://github.com/dietmarwo/fast-cma-es/blob/master/tutorials/EvoJax.adoc)) ([source](https://github.com/google/evojax/blob/main/evojax/algo/fcrfmc.py))                     | [dietmarwo](https://github.com/dietmarwo) | [PR](https://github.com/google/evojax/pull/44)
| [OpenES](https://arxiv.org/pdf/1703.03864.pdf)                                                                                                    | This is a wrapper of the OpenES implementation in [evosax](https://github.com/RobertTLange/evosax). ([source](https://github.com/google/evojax/blob/main/evojax/algo/open_es.py)) | [RobertTLange](https://github.com/RobertTLange)                                     | [Table](https://github.com/google/evojax/tree/main/scripts/benchmarks#openes)             |                                                    |
| [PGPE](https://people.idsia.ch/~juergen/nn2010.pdf)                                                                                                    | This implementation is well tested, all examples are based on this algorithm. We provide two optimizers: Adam and [ClipUp](https://github.com/nnaisense/pgpelib). ([source](https://github.com/google/evojax/blob/main/evojax/algo/pgpe.py)) | EvoJAX team                                     | [Table](https://github.com/google/evojax/tree/main/scripts/benchmarks#pgpe)             |                                                    |
| [PGPE](https://people.idsia.ch/~juergen/nn2010.pdf)                                                                                                    | This is a wrapper of the PGPE C++/Eigen implementation from [fcmaes](https://github.com/dietmarwo/fast-cma-es), using [this](https://github.com/dietmarwo/fast-cma-es/blob/master/_fcmaescpp/pgpe.cpp) implementation of the original PGPE algorithm. We provide one optimizer: Adam. ([performance](https://github.com/dietmarwo/fast-cma-es/blob/master/tutorials/EvoJax.adoc)) ([source](https://github.com/google/evojax/blob/main/evojax/algo/fpgpec.py))                     | [dietmarwo](https://github.com/dietmarwo) | [PR](https://github.com/google/evojax/pull/51)
| [SimpleGA](http://cognet.mit.edu/book/simple-genetic-algorithm#:~:text=The%20Simple%20Genetic%20Algorithm%20(SGA,objects%20related%20to%20the%20SGA.)) | This implementation has truncation selection but no crossover. ([source](https://github.com/google/evojax/blob/main/evojax/algo/simple_ga.py))                                                                                                                                                                  | [MaximilienLC](https://github.com/MaximilienLC) | [PR](https://github.com/google/evojax/pull/5#issuecomment-1043879609) |
| [MAP-Elites](https://arxiv.org/abs/1504.04909) | This implementation uses [iso-line variantion](https://arxiv.org/abs/1804.03906) as the emitter. ([source](https://github.com/google/evojax/blob/main/evojax/algo/map_elites.py))                                                                                                                                                                  | EvoJAX team | [PR](https://github.com/google/evojax/pull/33) |

=== ./evojax/algo/fcrfmc.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This is a wrapper of the CR-FM-NES algorithm.
    See https://github.com/dietmarwo/fast-cma-es/blob/master/_fcmaescpp/crfmnes.cpp .
    Eigen based implementation of Fast Moving Natural Evolution Strategy 
    for High-Dimensional Problems (CR-FM-NES), see https://arxiv.org/abs/2201.11422 .
    Derived from https://github.com/nomuramasahir0/crfmnes .
"""

import sys

import logging
import numpy as np
import math
from typing import Union
from typing import Optional

import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger

class FCRFMC(NEAlgorithm):
    """A wrapper of CR-FM-NES Eigen version."""

    def __init__(self,
                 param_size: int,
                 pop_size: int,
                 init_params: Optional[Union[jnp.ndarray, np.ndarray]] = None,
                 init_stdev: float = 0.1,
                 seed: int = 0,
                 logger: logging.Logger = None):
        if logger is None:
            self.logger = create_logger(name='FCRFMC')
        else:
            self.logger = logger
        self.pop_size = pop_size
        
        if init_params is None:
            center = np.zeros(abs(param_size))
        else:
            center = init_params
        
        try:
            from numpy.random import MT19937, Generator
            from fcmaes import crfmnescpp

        except ModuleNotFoundError:
            print("You need to install fcmaes:")
            print("  pip install fcmaes --upgrade")
            sys.exit(1)
                 
        self.fcrfm = crfmnescpp.CRFMNES_C(param_size, None, center,
                init_stdev, pop_size, Generator(MT19937(seed)))    

        self.params = None
        self._best_params = None
        self.maxy = -math.inf
        
        self.jnp_array = jax.jit(jnp.array)
        self.jnp_stack = jax.jit(jnp.stack)
        
    def ask(self) -> jnp.ndarray:
        self.params = self.fcrfm.ask()
        return self.jnp_stack(self.params)

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        y = np.array(fitness)
        self.fcrfm.tell(-y)

        maxy = np.max(y)
        if self.maxy < maxy:
            self.maxy = maxy
            self._best_params = self.params[np.argmax(y)]
            
    @property
    def best_params(self) -> jnp.ndarray:
        return self.jnp_array(self._best_params)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self._best_params = np.array(params)

=== ./evojax/algo/fpgpec.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This is a wrapper of the PGPE algorithm.
    See https://github.com/dietmarwo/fast-cma-es/blob/master/_fcmaescpp/pgpe.cpp .
    Eigen based implementation of PGPE see http://mediatum.ub.tum.de/doc/1099128/631352.pdf .
    Derived from https://github.com/google/evojax/blob/main/evojax/algo/pgpe.py .
"""

import sys

import logging
import numpy as np
import math
from typing import Union
from typing import Optional

import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger

from time import time

class FPGPEC(NEAlgorithm):
    """A wrapper of PGPE Eigen version."""

    def __init__(self,
            pop_size: int,
            param_size: int,
            init_params: Optional[Union[jnp.ndarray, np.ndarray]] = None,
            optimizer: Optional[str] = None,
            optimizer_config: Optional[dict] = None,
            center_learning_rate: float = 0.15,
            stdev_learning_rate: float = 0.1,
            init_stdev: Union[float, jnp.ndarray, np.ndarray] = 0.1,
            stdev_max_change: float = 0.2,
            solution_ranking: bool = False,
            seed: int = 0,
            logger: logging.Logger = None,
        ):
        if logger is None:
            self.logger = create_logger(name='FPGPEC')
        else:
            self.logger = logger
        self.pop_size = pop_size

        try:
            from numpy.random import MT19937, Generator
            from fcmaes import pgpecpp

        except ModuleNotFoundError:
            print("You need to install fcmaes:")
            print("  pip install fcmaes --upgrade")
            sys.exit(1)
        
        if not optimizer is None and optimizer != "adam":
            print("FPGPEC currently only supports adam optimizer")
            sys.exit(1)            
        
        if init_params is None:
            center = np.zeros(abs(param_size))
        else:
            center = init_params
        
        if isinstance(init_stdev, float):
            init_stdev = np.full(abs(param_size), abs(init_stdev))
        init_stdev = jnp.array(init_stdev)
        
        if optimizer_config is None:
            optimizer_config = {}
        decay_coef = optimizer_config.get("center_lr_decay_coef", 1.0)
        lr_decay_steps = optimizer_config.get(
            "center_lr_decay_steps", 1000
        )
        bounds = None # Bounds([-3]*param_size,[3]*param_size)         
        self.fcrfm = pgpecpp.PGPE_C(param_size, bounds, center,
                init_stdev, pop_size, Generator(MT19937(seed)), 
                lr_decay_steps = lr_decay_steps,
                use_ranking = solution_ranking, 
                center_learning_rate = center_learning_rate,
                stdev_learning_rate = stdev_learning_rate, 
                stdev_max_change = stdev_max_change, 
                b1 = optimizer_config.get("beta1", 0.9),
                b2 = optimizer_config.get("beta2", 0.999), 
                eps = optimizer_config.get("epsilon", 1e-8), 
                decay_coef = decay_coef,                
                )    

        self.params = None
        self._best_params = None
        self.maxy = -math.inf
        
        self.jnp_array = jax.jit(jnp.array)
        self.jnp_stack = jax.jit(jnp.stack)
        
        self.t0 = time()
        self.evals = 0

    def ask(self) -> jnp.ndarray:
        self.params = self.fcrfm.ask()
        return self.jnp_stack(self.params)

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        y = np.array(fitness)
        self.fcrfm.tell(-y)

        maxy = np.max(y)
        self.evals += self.pop_size
        if self.maxy < maxy:
            self._best_params = self.params[np.argmax(y)]   
            self.maxy = maxy
            
    @property
    def best_params(self) -> jnp.ndarray:
        return self.jnp_array(self._best_params)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        print("best_params called", params[0], self._best_params[0])
        self._best_params = np.array(params)

=== ./evojax/algo/crfmnes.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""JAX port of Fast Moving Natural Evolution Strategy 
    for High-Dimensional Problems (CR-FM-NES), see https://arxiv.org/abs/2201.11422 .
    Derived from https://github.com/nomuramasahir0/crfmnes"""   

import math
import numpy as np
from typing import Union
from typing import Optional
import logging
import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger

class CRFMNES(NEAlgorithm):
    """A wrapper of CR-FM-NES jax port."""

    def __init__(self,
                 param_size: int,
                 pop_size: int,
                 init_params: Optional[Union[jnp.ndarray, np.ndarray]] = None,                 
                 init_stdev: float = 0.1,
                 seed: int = 0,
                 logger: logging.Logger = None):
        if logger is None:
            self.logger = create_logger(name='FCRFM')
        else:
            self.logger = logger
        self.pop_size = pop_size

        if init_params is None:
            center = np.zeros(abs(param_size))
        else:
            center = init_params
            
        self.crfm = CRFM(param_size, pop_size, center, init_stdev, jax.random.PRNGKey(seed))    

        self.params = None
        self._best_params = None

        self.jnp_array = jax.jit(jnp.array)
        self.jnp_stack = jax.jit(jnp.stack)

    def ask(self) -> jnp.ndarray:
        self.params = self.crfm.ask()
        return self.jnp_stack(self.params)

    def tell(self, fitness: jnp.ndarray) -> None:
        self.crfm.tell(-np.array(fitness))
        self._best_params = self.crfm.x_best

    @property
    def best_params(self) -> jnp.ndarray:
        return self.jnp_array(self._best_params)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self._best_params = jnp.array(params)
        self.crfm.set_m(self._best_params.copy())

class CRFM():
    def __init__(self, num_dims: 
                 int, popsize: int, 
                 mean: Optional[Union[jnp.ndarray, np.ndarray]], 
                 input_sigma: float, 
                 rng: jax.random.PRNGKey):
        """Fast Moving Natural Evolution Strategy 
        for High-Dimensional Problems (CR-FM-NES), see https://arxiv.org/abs/2201.11422 .
        Derived from https://github.com/nomuramasahir0/crfmnes"""        
        if popsize % 2 == 1: # requires even popsize
            popsize += 1
        self.lamb = popsize
        self.dim = num_dims
        self.sigma = input_sigma 
        self.rng = rng       
        self.m = jnp.array([mean]).T
        self.v = jax.random.normal(rng, (self.dim, 1)) / jnp.sqrt(self.dim)       
        self.D = jnp.ones([self.dim, 1])

        self.w_rank_hat = (jnp.log(self.lamb / 2 + 1) - jnp.log(jnp.arange(1, self.lamb + 1))).reshape(self.lamb, 1)
        self.w_rank_hat = self.w_rank_hat.at[jnp.where(self.w_rank_hat < 0)].set(0)
        self.w_rank = self.w_rank_hat / sum(self.w_rank_hat) - (1. / self.lamb)        
        self.mueff = float(1 / jnp.dot((self.w_rank + (1 / self.lamb)).T, (self.w_rank + (1 / self.lamb)))[0][0])

        self.cs = (self.mueff + 2.) / (self.dim + self.mueff + 5.)
        self.cc = (4. + self.mueff / self.dim) / (self.dim + 4. + 2. * self.mueff / self.dim)
        self.c1_cma = 2. / (math.pow(self.dim + 1.3, 2) + self.mueff)
        # initialization
        self.chiN = math.sqrt(self.dim) * (1. - 1. / (4. * self.dim) + 1. / (21. * self.dim * self.dim))
        self.pc = jnp.zeros((self.dim, 1))
        self.ps = jnp.zeros((self.dim, 1))
        # distance weight parameter
        self.h_inv = get_h_inv(self.dim)
        self.alpha_dist = lambda lambF: self.h_inv * min(1., math.sqrt(self.lamb / self.dim)) * math.sqrt(
            lambF / self.lamb)
        self.w_dist_hat = lambda z, lambF: exp(self.alpha_dist(lambF) * jnp.linalg.norm(z))
        # learning rate
        self.eta_m = 1.0
        self.eta_move_sigma = 1.
        self.eta_stag_sigma = lambda lambF: math.tanh((0.024 * lambF + 0.7 * self.dim + 20.) / (self.dim + 12.))
        self.eta_conv_sigma = lambda lambF: 2. * math.tanh((0.025 * lambF + 0.75 * self.dim + 10.) / (self.dim + 4.))
        self.c1 = lambda lambF: self.c1_cma * (self.dim - 5) / 6 * (lambF / self.lamb)
        self.eta_B = lambda lambF: jnp.tanh((min(0.02 * lambF, 3 * jnp.log(self.dim)) + 5) / (0.23 * self.dim + 25))

        self.g = 0
        self.no_of_evals = 0
        self.iteration = 0
        self.stop = 0

        self.idxp = jnp.arange(self.lamb / 2, dtype=int)
        self.idxm = jnp.arange(self.lamb / 2, self.lamb, dtype=int)
        self.z = jnp.zeros([self.dim, self.lamb])

        self.f_best = float('inf')
        self.x_best = jnp.empty(self.dim)

    def set_m(self, params: jnp.ndarray):
        self.m = jnp.array(params).reshape((self.dim, 1))

    def ask(self) -> jnp.ndarray:
        key, self.rng = jax.random.split(self.rng)
        zhalf = jax.random.normal(key, (self.dim, int(self.lamb / 2)))
        self.z = self.z.at[:, self.idxp].set(zhalf)
        self.z = self.z.at[:, self.idxm].set(-zhalf)
        self.normv = jnp.linalg.norm(self.v)
        self.normv2 = self.normv ** 2
        self.vbar = self.v / self.normv
        self.y = self.z + ((jnp.sqrt(1 + self.normv2) - 1) * jnp.dot(self.vbar, jnp.dot(self.vbar.T, self.z)))
        self.x = self.m + (self.sigma * self.y) * self.D
        return self.x.T

    def tell(self, evals_no_sort: np.ndarray) -> None:
        sorted_indices = sort_indices_by(evals_no_sort, self.z)
        best_eval_id = sorted_indices[0]       
        f_best = evals_no_sort[best_eval_id]
        x_best = self.x[:, best_eval_id]
        self.z = self.z[:, sorted_indices]
        y = self.y[:, sorted_indices]
        x = self.x[:, sorted_indices]
        self.no_of_evals += self.lamb
        self.g += 1
        if f_best < self.f_best:
            self.f_best = f_best
            self.x_best = x_best   
                    
        # This operation assumes that if the solution is infeasible, infinity comes in as input.
        lambF = jnp.sum(evals_no_sort < jnp.finfo(float).max)
        # evolution path p_sigma
        self.ps = (1 - self.cs) * self.ps + jnp.sqrt(self.cs * (2. - self.cs) * self.mueff) * jnp.dot(self.z, self.w_rank)
        ps_norm = jnp.linalg.norm(self.ps)
        # distance weight
        f1 =  self.h_inv * min(1., math.sqrt(self.lamb / self.dim)) * math.sqrt(lambF / self.lamb)        
        w_tmp = self.w_rank_hat * jnp.exp(jnp.linalg.norm(self.z, axis = 0) * f1).reshape((self.lamb,1))
        weights_dist = w_tmp / sum(w_tmp) - 1. / self.lamb
        # switching weights and learning rate
        weights = weights_dist if ps_norm >= self.chiN else self.w_rank
        eta_sigma = self.eta_move_sigma if ps_norm >= self.chiN else self.eta_stag_sigma(
            lambF) if ps_norm >= 0.1 * self.chiN else self.eta_conv_sigma(lambF)
        # update pc, m
        wxm = jnp.dot(x - self.m, weights)
        self.pc = (1. - self.cc) * self.pc + jnp.sqrt(self.cc * (2. - self.cc) * self.mueff) * wxm / self.sigma
        self.m += self.eta_m * wxm
        normv4 = self.normv2 ** 2
        exY = jnp.append(y, self.pc / self.D, axis=1)  # dim x lamb+1
        yy = exY * exY  # dim x lamb+1
        ip_yvbar = jnp.dot(self.vbar.T, exY)
        yvbar = exY * self.vbar  # dim x lamb+1. exYのそれぞれの列にvbarがかかる
        gammav = 1. + self.normv2
        vbarbar = self.vbar * self.vbar
        alphavd = min(
            [1, math.sqrt(normv4 + (2 * gammav - math.sqrt(gammav)) / jnp.max(vbarbar)) / (2 + self.normv2)])  # scalar       
        t = exY * ip_yvbar - self.vbar * (ip_yvbar ** 2 + gammav) / 2  # dim x lamb+1
        b = -(1 - alphavd ** 2) * normv4 / gammav + 2 * alphavd ** 2
        H = jnp.ones([self.dim, 1]) * 2 - (b + 2 * alphavd ** 2) * vbarbar  # dim x 1
        invH = H ** (-1)
        s_step1 = yy - self.normv2 / gammav * (yvbar * ip_yvbar) - jnp.ones([self.dim, self.lamb + 1])  # dim x lamb+1
        ip_vbart = jnp.dot(self.vbar.T, t)  # 1 x lamb+1
        s_step2 = s_step1 - alphavd / gammav * ((2 + self.normv2) * (t * self.vbar) - self.normv2 * jnp.dot(vbarbar, ip_vbart))  # dim x lamb+1
        invHvbarbar = invH * vbarbar
        ip_s_step2invHvbarbar = jnp.dot(invHvbarbar.T, s_step2)  # 1 x lamb+1       
        div = 1 + b * jnp.dot(vbarbar.T, invHvbarbar)
        if jnp.amin(abs(div)) == 0:
            self.logger.info('error: div is zero')
            return      
        s = (s_step2 * invH) - b / div * jnp.dot(invHvbarbar, ip_s_step2invHvbarbar)  # dim x lamb+1
        ip_svbarbar = jnp.dot(vbarbar.T, s)  # 1 x lamb+1
        t = t - alphavd * ((2 + self.normv2) * (s * self.vbar) - jnp.dot(self.vbar, ip_svbarbar))  # dim x lamb+1
        # update v, D
        exw = jnp.append(self.eta_B(lambF) * weights, jnp.full((1, 1), self.c1(lambF)), axis=0)  # lamb+1 x 1
        self.v = self.v + jnp.dot(t, exw) / self.normv
        self.D = self.D + jnp.dot(s, exw) * self.D
        # calculate detA
        if jnp.amin(self.D) < 0:
            self.logger.info('error: invalid D')
            return
        nthrootdetA = exp(jnp.sum(jnp.log(self.D)) / self.dim + jnp.log(1 + jnp.dot(self.v.T, self.v)[0][0]) / (2 * self.dim))
        self.D = self.D / nthrootdetA
        # update sigma
        G_s = jnp.sum(  jnp.dot( (self.z * self.z - jnp.ones([self.dim, self.lamb])), weights  )) / self.dim
        self.sigma = self.sigma * exp(eta_sigma / 2 * G_s)    

def get_h_inv(dim: int) -> float:
    f = lambda a, b: ((1. + a * a) * exp(a * a / 2.) / 0.24) - 10. - dim
    f_prime = lambda a: (1. / 0.24) * a * exp(a * a / 2.) * (3. + a * a)
    h_inv = 1.0
    while abs(f(h_inv, dim)) > 1e-10:
        h_inv = h_inv - 0.5 * (f(h_inv, dim) / f_prime(h_inv))
    return h_inv

def exp(a: float) -> float:
    return math.exp(min(100, a)) # avoid overflow

def sort_indices_by(evals: np.ndarray, z: jnp.ndarray) -> jnp.ndarray:
    lam = len(evals)
    sorted_indices = np.argsort(evals)
    sorted_evals = evals[sorted_indices]
    no_of_feasible_solutions = np.where(sorted_evals != jnp.inf)[0].size
    if no_of_feasible_solutions != lam:
        infeasible_z = z[:, np.where(evals == jnp.inf)[0]]
        distances = np.sum(infeasible_z ** 2, axis=0)
        infeasible_indices = sorted_indices[no_of_feasible_solutions:]
        indices_sorted_by_distance = np.argsort(distances)
        sorted_indices = sorted_indices.at[no_of_feasible_solutions:].set(infeasible_indices[indices_sorted_by_distance])
    return sorted_indices

=== ./evojax/algo/sep_cma_es.py ===
import sys

import logging
from typing import Union, Optional
import numpy as np
import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


class Sep_CMA_ES(NEAlgorithm):
    """A wrapper around evosax's Sep-CMA-ES.
    Implementation: https://github.com/RobertTLange/evosax/blob/main/evosax/strategies/sep_cma_es.py
    Reference: Ros & Hansen (2008) - https://hal.inria.fr/inria-00287367/document
    """

    def __init__(
        self,
        param_size: int,
        pop_size: int,
        elite_ratio: float = 0.5,
        init_stdev: float = 0.1,
        w_decay: float = 0.0,
        seed: int = 0,
        logger: Optional[logging.Logger] = None,
    ):
        """Initialization function.

        Args:
            param_size - Parameter size.
            pop_size - Population size.
            elite_ratio - Population elite fraction used for gradient estimate.
            init_stdev - Initial scale of istropic part of covariance.
            w_decay - L2 weight regularization coefficient.
            seed - Random seed for parameters sampling.
            logger - Logger.
        """

        # Delayed importing of evosax

        if sys.version_info.minor < 7:
            print("evosax, which is needed by Sep-CMA-ES, requires python>=3.7")
            print("  please consider upgrading your Python version.")
            sys.exit(1)

        try:
            import evosax
        except ModuleNotFoundError:
            print("You need to install evosax for its Sep-CMA-ES:")
            print("  pip install evosax")
            sys.exit(1)

        # Set up object variables.

        if logger is None:
            self.logger = create_logger(name="Sep_CMA_ES")
        else:
            self.logger = logger

        self.param_size = param_size
        self.pop_size = abs(pop_size)
        self.elite_ratio = elite_ratio
        self.rand_key = jax.random.PRNGKey(seed=seed)

        # Instantiate evosax's ARS strategy
        self.es = evosax.Sep_CMA_ES(
            popsize=pop_size,
            num_dims=param_size,
            elite_ratio=elite_ratio,
        )

        # Set hyperparameters according to provided inputs
        self.es_params = self.es.default_params.replace(sigma_init=init_stdev)

        # Initialize the evolution strategy state
        self.rand_key, init_key = jax.random.split(self.rand_key)
        self.es_state = self.es.initialize(init_key, self.es_params)

        # By default evojax assumes maximization of fitness score!
        # Evosax, on the other hand, minimizes!
        self.fit_shaper = evosax.FitnessShaper(w_decay=w_decay, maximize=True)

    def ask(self) -> jnp.ndarray:
        self.rand_key, ask_key = jax.random.split(self.rand_key)
        self.params, self.es_state = self.es.ask(
            ask_key, self.es_state, self.es_params
        )
        return self.params

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        # Reshape fitness to conform with evosax minimization
        fit_re = self.fit_shaper.apply(self.params, fitness)
        self.es_state = self.es.tell(
            self.params, fit_re, self.es_state, self.es_params
        )

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self.es_state.mean, copy=True)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self.es_state = self.es_state.replace(
            best_member=jnp.array(params, copy=True),
            mean=jnp.array(params, copy=True),
        )

=== ./evojax/algo/iamalgam.py ===
import sys

import logging
from typing import Union, Optional
import numpy as np
import jax
import jax.numpy as jnp

from evojax.algo.base import NEAlgorithm
from evojax.util import create_logger


class iAMaLGaM(NEAlgorithm):
    """A wrapper around evosax's iAMaLGaM.
    Implementation: https://github.com/RobertTLange/evosax/blob/main/evosax/strategies/indep_iamalgam.py
    Reference: Bosman et al. (2013) - https://tinyurl.com/y9fcccx2
    """

    def __init__(
        self,
        param_size: int,
        pop_size: int,
        elite_ratio: float = 0.35,
        full_covariance: bool = False,
        eta_sigma: Optional[float] = None,
        eta_shift: Optional[float] = None,
        init_stdev: float = 0.01,
        decay_stdev: float = 0.999,
        limit_stdev: float = 0.001,
        w_decay: float = 0.0,
        seed: int = 0,
        logger: Optional[logging.Logger] = None,
    ):
        """Initialization function.

        Args:
            param_size - Parameter size.
            pop_size - Population size.
            elite_ratio - Population elite fraction used for mean update.
            full_covariance - Whether to estimate full covariance or only diag.
            eta_sigma - Lrate for covariance (use default if not provided).
            eta_shift - Lrate for mean shift (use default if not provided).
            init_stdev - Initial scale of Gaussian perturbation.
            decay_stdev - Multiplicative scale decay between tell iterations.
            limit_stdev - Smallest scale (clipping limit).
            w_decay - L2 weight regularization coefficient.
            seed - Random seed for parameters sampling.
            logger - Logger.
        """

        # Delayed importing of evosax

        if sys.version_info.minor < 7:
            print("evosax, which is needed by iAMaLGaM, requires python>=3.7")
            print("  please consider upgrading your Python version.")
            sys.exit(1)

        try:
            import evosax
        except ModuleNotFoundError:
            print("You need to install evosax for its iAMaLGaM:")
            print("  pip install evosax")
            sys.exit(1)

        # Set up object variables.

        if logger is None:
            self.logger = create_logger(name="iAMaLGaM")
        else:
            self.logger = logger

        self.param_size = param_size
        self.pop_size = abs(pop_size)
        self.rand_key = jax.random.PRNGKey(seed=seed)

        # Instantiate evosax's iAMaLGaM - choice between full cov & diagonal
        if full_covariance:
            self.es = evosax.Full_iAMaLGaM(
                popsize=pop_size, num_dims=param_size, elite_ratio=elite_ratio
            )
        else:
            self.es = evosax.Indep_iAMaLGaM(
                popsize=pop_size, num_dims=param_size, elite_ratio=elite_ratio
            )

        # Set hyperparameters according to provided inputs
        self.es_params = self.es.default_params.replace(
            sigma_init=init_stdev,
            sigma_decay=decay_stdev,
            sigma_limit=limit_stdev,
            init_min=0.0,
            init_max=0.0,
        )

        # Only replace learning rates for mean shift and sigma if provided!
        if eta_shift is not None:
            self.es_params = self.es_params.replace(eta_shift=eta_shift)
        if eta_sigma is not None:
            self.es_params = self.es_params.replace(eta_sigma=eta_sigma)

        # Initialize the evolution strategy state
        self.rand_key, init_key = jax.random.split(self.rand_key)
        self.es_state = self.es.initialize(init_key, self.es_params)

        # By default evojax assumes maximization of fitness score!
        # Evosax, on the other hand, minimizes!
        self.fit_shaper = evosax.FitnessShaper(w_decay=w_decay, maximize=True)

    def ask(self) -> jnp.ndarray:
        self.rand_key, ask_key = jax.random.split(self.rand_key)
        self.params, self.es_state = self.es.ask(
            ask_key, self.es_state, self.es_params
        )
        return self.params

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        # Reshape fitness to conform with evosax minimization
        fit_re = self.fit_shaper.apply(self.params, fitness)
        self.es_state = self.es.tell(
            self.params, fit_re, self.es_state, self.es_params
        )

    @property
    def best_params(self) -> jnp.ndarray:
        return jnp.array(self.es_state.mean, copy=True)

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        self.es_state = self.es_state.replace(
            best_member=jnp.array(params, copy=True),
            mean=jnp.array(params, copy=True),
        )

=== ./evojax/algo/map_elites.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of the MAP-ELITES algorithm in JAX.

Ref: https://arxiv.org/pdf/1504.04909.pdf

In this implementation, we use a modified version iso+line emitter.
"""

import logging
import numpy as np
from typing import Optional
from typing import Union

import jax
import jax.numpy as jnp

from evojax.algo.base import QualityDiversityMethod
from evojax.task.base import TaskState
from evojax.task.base import BDExtractor
from evojax.util import create_logger


class MAPElites(QualityDiversityMethod):
    """The MAP-ELITES algorithm."""

    def __init__(self,
                 pop_size: int,
                 param_size: int,
                 bd_extractor: BDExtractor,
                 init_params: Optional[Union[jnp.ndarray, np.ndarray]] = None,
                 iso_sigma: float = 0.05,
                 line_sigma: float = 0.5,
                 seed: int = 0,
                 logger: logging.Logger = None):
        """Initialization function.

        Args:
            pop_size - Population size.
            param_size - Parameter size.
            bd_extractors - A list of behavior descriptor extractors.
            init_params - Initial parameters, all zeros if not given.
            iso_sigma - Standard deviation for Gaussian sampling for mutation.
            line_sigma - Parameter for cross over.
            seed - Random seed for parameters sampling.
            logger - Logging utility.
        """

        if logger is None:
            self._logger = create_logger(name='MAP_ELITES')
        else:
            self._logger = logger

        self.pop_size = abs(pop_size)
        self.param_size = param_size
        self.bd_names = [x[0] for x in bd_extractor.bd_spec]
        self.bd_n_bins = [x[1] for x in bd_extractor.bd_spec]
        self.params_lattice = jnp.zeros((np.prod(self.bd_n_bins), param_size))
        self.fitness_lattice = -float('Inf') * jnp.ones(np.prod(self.bd_n_bins))
        self.occupancy_lattice = jnp.zeros(
            np.prod(self.bd_n_bins), dtype=jnp.int32)
        self.population = None
        self.bin_idx = jnp.zeros(self.pop_size, dtype=jnp.int32)
        self.key = jax.random.PRNGKey(seed)

        init_param = (
            jnp.array(init_params)
            if init_params is not None else jnp.zeros(param_size))

        def sample_parents(key, occupancy, params):
            new_key, sample_key, mutate_key = jax.random.split(key, 3)
            parents = jax.lax.cond(
                occupancy.sum() > 0,
                lambda: jnp.take(params, axis=0, indices=jax.random.choice(
                    key=sample_key, a=jnp.arange(occupancy.size), replace=True,
                    p=occupancy / occupancy.sum(), shape=(2 * self.pop_size,))),
                lambda: init_param[None, :] + iso_sigma * jax.random.normal(
                    sample_key, shape=(2 * self.pop_size, self.param_size)),
            )
            return new_key, mutate_key, parents.reshape((2, self.pop_size, -1))
        self._sample_parents = jax.jit(sample_parents)

        def generate_population(parents, key):
            k1, k2 = jax.random.split(key, 2)
            return (parents[0] + iso_sigma *
                    jax.random.normal(key=k1, shape=parents[0].shape) +
                    # Uniform sampling instead of Gaussian.
                    jax.random.uniform(
                        key=k2, minval=0, maxval=line_sigma, shape=()) *
                    (parents[1] - parents[0]))
        self._gen_pop = jax.jit(generate_population)

        def get_bin_idx(task_state):
            bd_idx = [
                task_state.__dict__[name].astype(int) for name in self.bd_names]
            return jnp.ravel_multi_index(bd_idx, self.bd_n_bins, mode='clip')
        self._get_bin_idx = jax.jit(jax.vmap(get_bin_idx))

        def update_fitness_and_param(
                target_bin, bin_idx,
                fitness, fitness_lattice, param, param_lattice):
            best_ix = jnp.where(
                bin_idx == target_bin, fitness, fitness_lattice.min()).argmax()
            best_fitness = fitness[best_ix]
            new_fitness_lattice = jnp.where(
                best_fitness > fitness_lattice[target_bin],
                best_fitness, fitness_lattice[target_bin])
            new_param_lattice = jnp.where(
                best_fitness > fitness_lattice[target_bin],
                param[best_ix], param_lattice[target_bin])
            return new_fitness_lattice, new_param_lattice
        self._update_lattices = jax.jit(jax.vmap(
            update_fitness_and_param,
            in_axes=(0, None, None, None, None, None)))

    def ask(self) -> jnp.ndarray:
        self.key, mutate_key, parents = self._sample_parents(
            key=self.key,
            occupancy=self.occupancy_lattice,
            params=self.params_lattice)
        self.population = self._gen_pop(parents, mutate_key)
        return self.population

    def observe_bd(self, task_state: TaskState) -> None:
        self.bin_idx = self._get_bin_idx(task_state)

    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        unique_bins = jnp.unique(self.bin_idx)
        fitness_lattice, params_lattice = self._update_lattices(
            unique_bins, self.bin_idx,
            fitness, self.fitness_lattice,
            self.population, self.params_lattice)
        self.occupancy_lattice = self.occupancy_lattice.at[unique_bins].set(1)
        self.fitness_lattice = self.fitness_lattice.at[unique_bins].set(
            fitness_lattice)
        self.params_lattice = self.params_lattice.at[unique_bins].set(
            params_lattice)

    @property
    def best_params(self) -> jnp.ndarray:
        ix = jnp.argmax(self.fitness_lattice, axis=0)
        return self.params_lattice[ix]

=== ./evojax/algo/base.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from abc import ABC
from abc import abstractmethod
from typing import Any
from typing import Dict
from typing import Union
import numpy as np
import jax.numpy as jnp


class NEAlgorithm(ABC):
    """Interface of all Neuro-evolution algorithms in EvoJAX."""

    pop_size: int

    @abstractmethod
    def ask(self) -> jnp.ndarray:
        """Ask the algorithm for a population of parameters.

        Returns
            A Jax array of shape (population_size, param_size).
        """
        raise NotImplementedError()

    @abstractmethod
    def tell(self, fitness: Union[np.ndarray, jnp.ndarray]) -> None:
        """Report the fitness of the population to the algorithm.

        Args:
            fitness - The fitness scores array.
        """
        raise NotImplementedError()

    def save_state(self) -> Any:
        """Optionally, save the state of the algorithm.

        Returns
            Saved state.
        """
        return None

    def load_state(self, saved_state: Any) -> None:
        """Optionally, load the saved state of the algorithm.

        Args:
            saved_states - The result of self.save_states().
        """
        pass

    @property
    def best_params(self) -> jnp.ndarray:
        raise NotImplementedError()

    @best_params.setter
    def best_params(self, params: Union[np.ndarray, jnp.ndarray]) -> None:
        raise NotImplementedError()


class QualityDiversityMethod(NEAlgorithm):
    """Quality diversity method."""

    params_lattice: jnp.ndarray
    fitness_lattice: jnp.ndarray
    occupancy_lattice: jnp.ndarray

    @abstractmethod
    def observe_bd(self, bd: Dict[str, jnp.ndarray]) -> None:
        raise NotImplementedError()

=== ./evojax/__init__.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .obs_norm import ObsNormalizer
from .sim_mgr import SimManager
from .trainer import Trainer


__all__ = ['ObsNormalizer', 'SimManager', 'Trainer']

=== ./evojax/sim_mgr.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
from functools import partial
from typing import Tuple
from typing import Union
import time

import jax
import jax.numpy as jnp
from jax import random
from jax.tree_util import tree_map

from evojax.obs_norm import ObsNormalizer
from evojax.task.base import TaskState
from evojax.task.base import VectorizedTask
from evojax.policy.base import PolicyState
from evojax.policy.base import PolicyNetwork
from evojax.util import create_logger


@partial(jax.jit, static_argnums=(1, 2, 3, 4, 5))
def get_task_reset_keys(key: jnp.ndarray,
                        test: bool,
                        pop_size: int,
                        n_tests: int,
                        n_repeats: int,
                        ma_training: bool) -> Tuple[jnp.ndarray, jnp.ndarray]:
    key, subkey = random.split(key=key)
    if ma_training:
        reset_keys = random.split(subkey, n_repeats)
    else:
        if test:
            reset_keys = random.split(subkey, n_tests * n_repeats)
        else:
            reset_keys = random.split(subkey, n_repeats)
            reset_keys = jnp.tile(reset_keys, (pop_size, 1))
    return key, reset_keys


@jax.jit
def split_params_for_pmap(param: jnp.ndarray) -> jnp.ndarray:
    return jnp.stack(jnp.split(param, jax.local_device_count()))


@jax.jit
def split_states_for_pmap(
        state: Union[TaskState, PolicyState]) -> Union[TaskState, PolicyState]:
    return tree_map(split_params_for_pmap, state)


@jax.jit
def reshape_data_from_pmap(data: jnp.ndarray) -> jnp.ndarray:
    # data.shape = (#device, steps, #jobs/device, *)
    data = data.transpose([1, 0] + [i for i in range(2, data.ndim)])
    return jnp.reshape(data, (data.shape[0], data.shape[1] * data.shape[2], -1))


@jax.jit
def merge_state_from_pmap(state: TaskState) -> TaskState:
    return jax.tree_map(
        lambda x: x.reshape((x.shape[0] * x.shape[1], *x.shape[2:])), state)


@partial(jax.jit, static_argnums=(1, 2))
def duplicate_params(params: jnp.ndarray,
                     repeats: int,
                     ma_training: bool) -> jnp.ndarray:
    if ma_training:
        return jnp.tile(params, (repeats, ) + (1,) * (params.ndim - 1))
    else:
        return jnp.repeat(params, repeats=repeats, axis=0)


@jax.jit
def update_score_and_mask(score, reward, mask, done):
    new_score = score + reward * mask
    new_mask = mask * (1 - done.ravel())
    return new_score, new_mask


@partial(jax.jit, static_argnums=(1,))
def report_score(scores, n_repeats):
    return jnp.mean(scores.ravel().reshape((-1, n_repeats)), axis=-1)


@jax.jit
def all_done(masks):
    return masks.sum() == 0


class SimManager(object):
    """Simulation manager."""

    def __init__(self,
                 n_repeats: int,
                 test_n_repeats: int,
                 pop_size: int,
                 n_evaluations: int,
                 policy_net: PolicyNetwork,
                 train_vec_task: VectorizedTask,
                 valid_vec_task: VectorizedTask,
                 seed: int = 0,
                 obs_normalizer: ObsNormalizer = None,
                 use_for_loop: bool = False,
                 logger: logging.Logger = None):
        """Initialization function.

        Args:
            n_repeats - Number of repeated parameter evaluations.
            pop_size - Population size.
            n_evaluations - Number of evaluations of the best parameter.
            policy_net - Policy network.
            train_vec_task - Vectorized tasks for training.
            valid_vec_task - Vectorized tasks for validation.
            seed - Random seed.
            obs_normalizer - Observation normalization helper.
            use_for_loop - Use for loop for rollout instead of jax.lax.scan.
            logger - Logger.
        """

        if logger is None:
            self._logger = create_logger(name='SimManager')
        else:
            self._logger = logger

        self._use_for_loop = use_for_loop
        self._logger.info('use_for_loop={}'.format(self._use_for_loop))
        self._key = random.PRNGKey(seed=seed)
        self._n_repeats = n_repeats
        self._test_n_repeats = test_n_repeats
        self._pop_size = pop_size
        self._n_evaluations = max(n_evaluations, jax.local_device_count())
        self._ma_training = train_vec_task.multi_agent_training

        self.obs_normalizer = obs_normalizer
        if self.obs_normalizer is None:
            self.obs_normalizer = ObsNormalizer(
                obs_shape=train_vec_task.obs_shape,
                dummy=True,
            )
        self.obs_params = self.obs_normalizer.get_init_params()

        self._num_device = jax.local_device_count()
        if self._pop_size % self._num_device != 0:
            raise ValueError(
                'pop_size must be multiples of GPU/TPUs: '
                'pop_size={}, #devices={}'.format(
                    self._pop_size, self._num_device))
        if self._n_evaluations % self._num_device != 0:
            raise ValueError(
                'n_evaluations must be multiples of GPU/TPUs: '
                'n_evaluations={}, #devices={}'.format(
                    self._n_evaluations, self._num_device))

        def step_once(carry, input_data, task):
            (task_state, policy_state, params, obs_params,
             accumulated_reward, valid_mask) = carry
            if task.multi_agent_training:
                num_tasks, num_agents = task_state.obs.shape[:2]
                task_state = task_state.replace(
                    obs=task_state.obs.reshape((-1, *task_state.obs.shape[2:])))
            org_obs = task_state.obs
            normed_obs = self.obs_normalizer.normalize_obs(org_obs, obs_params)
            task_state = task_state.replace(obs=normed_obs)
            actions, policy_state = policy_net.get_actions(
                task_state, params, policy_state)
            if task.multi_agent_training:
                task_state = task_state.replace(
                    obs=task_state.obs.reshape(
                        (num_tasks, num_agents, *task_state.obs.shape[1:])))
                actions = actions.reshape(
                    (num_tasks, num_agents, *actions.shape[1:]))
            task_state, reward, done = task.step(task_state, actions)
            if task.multi_agent_training:
                reward = reward.ravel()
                done = jnp.repeat(done, num_agents, axis=0)
            accumulated_reward = accumulated_reward + reward * valid_mask
            valid_mask = valid_mask * (1 - done.ravel())
            return ((task_state, policy_state, params, obs_params,
                     accumulated_reward, valid_mask),
                    (org_obs, valid_mask))

        def rollout(task_states, policy_states, params, obs_params,
                    step_once_fn, max_steps):
            accumulated_rewards = jnp.zeros(params.shape[0])
            valid_masks = jnp.ones(params.shape[0])
            ((task_states, policy_states, params, obs_params,
              accumulated_rewards, valid_masks),
             (obs_set, obs_mask)) = jax.lax.scan(
                step_once_fn,
                (task_states, policy_states, params, obs_params,
                 accumulated_rewards, valid_masks), (), max_steps)
            return accumulated_rewards, obs_set, obs_mask, task_states

        self._policy_reset_fn = jax.jit(policy_net.reset)
        self._policy_act_fn = jax.jit(policy_net.get_actions)

        if (
                hasattr(train_vec_task, 'bd_extractor') and
                train_vec_task.bd_extractor is not None
        ):
            self._bd_summarize_fn = jax.jit(
                train_vec_task.bd_extractor.summarize)
        else:
            self._bd_summarize_fn = lambda x: x

        # Set up training functions.
        self._train_reset_fn = train_vec_task.reset
        self._train_step_fn = train_vec_task.step
        self._train_max_steps = train_vec_task.max_steps
        self._train_rollout_fn = partial(
            rollout,
            step_once_fn=partial(step_once, task=train_vec_task),
            max_steps=train_vec_task.max_steps)
        if self._num_device > 1:
            self._train_rollout_fn = jax.jit(jax.pmap(
                self._train_rollout_fn, in_axes=(0, 0, 0, None)))

        # Set up validation functions.
        self._valid_reset_fn = valid_vec_task.reset
        self._valid_step_fn = valid_vec_task.step
        self._valid_max_steps = valid_vec_task.max_steps
        self._valid_rollout_fn = partial(
            rollout,
            step_once_fn=partial(step_once, task=valid_vec_task),
            max_steps=valid_vec_task.max_steps)
        if self._num_device > 1:
            self._valid_rollout_fn = jax.jit(jax.pmap(
                self._valid_rollout_fn, in_axes=(0, 0, 0, None)))

    def eval_params(self,
                    params: jnp.ndarray,
                    test: bool) -> Tuple[jnp.ndarray, TaskState]:
        """Evaluate population parameters or test the best parameter.

        Args:
            params - Parameters to be evaluated.
            test - Whether we are testing the best parameter
        Returns:
            An array of fitness scores.
        """
        if self._use_for_loop:
            return self._for_loop_eval(params, test)
        else:
            return self._scan_loop_eval(params, test)

    def _for_loop_eval(self,
                       params: jnp.ndarray,
                       test: bool) -> Tuple[jnp.ndarray, TaskState]:
        """Rollout using for loop (no multi-device or ma_training yet)."""
        policy_reset_func = self._policy_reset_fn
        policy_act_func = self._policy_act_fn
        if test:
            n_repeats = self._test_n_repeats
            task_reset_func = self._valid_reset_fn
            task_step_func = self._valid_step_fn
            task_max_steps = self._valid_max_steps
            params = duplicate_params(
                params[None, :], self._n_evaluations, False)
        else:
            n_repeats = self._n_repeats
            task_reset_func = self._train_reset_fn
            task_step_func = self._train_step_fn
            task_max_steps = self._train_max_steps

        params = duplicate_params(params, n_repeats, self._ma_training)

        # Start rollout.
        self._key, reset_keys = get_task_reset_keys(
            self._key, test, self._pop_size, self._n_evaluations, n_repeats,
            self._ma_training)
        task_state = task_reset_func(reset_keys)
        policy_state = policy_reset_func(task_state)
        scores = jnp.zeros(params.shape[0])
        valid_mask = jnp.ones(params.shape[0])
        start_time = time.perf_counter()
        rollout_steps = 0
        sim_steps = 0
        for i in range(task_max_steps):
            actions, policy_state = policy_act_func(
                task_state, params, policy_state)
            task_state, reward, done = task_step_func(task_state, actions)
            scores, valid_mask = update_score_and_mask(
                scores, reward, valid_mask, done)
            rollout_steps += 1
            sim_steps = sim_steps + valid_mask
            if all_done(valid_mask):
                break
        time_cost = time.perf_counter() - start_time
        self._logger.debug('{} steps/s, mean.steps={}'.format(
            int(rollout_steps * task_state.obs.shape[0] / time_cost),
            sim_steps.sum() / task_state.obs.shape[0]))

        return report_score(scores, n_repeats), task_state

    def _scan_loop_eval(self,
                        params: jnp.ndarray,
                        test: bool) -> Tuple[jnp.ndarray, TaskState]:
        """Rollout using jax.lax.scan."""
        policy_reset_func = self._policy_reset_fn
        if test:
            n_repeats = self._test_n_repeats
            task_reset_func = self._valid_reset_fn
            rollout_func = self._valid_rollout_fn
            params = duplicate_params(
                params[None, :], self._n_evaluations, False)
        else:
            n_repeats = self._n_repeats
            task_reset_func = self._train_reset_fn
            rollout_func = self._train_rollout_fn

        # Suppose pop_size=2 and n_repeats=3.
        # For multi-agents training, params become
        #   a1, a2, ..., an  (individual 1 params)
        #   b1, b2, ..., bn  (individual 2 params)
        #   a1, a2, ..., an  (individual 1 params)
        #   b1, b2, ..., bn  (individual 2 params)
        #   a1, a2, ..., an  (individual 1 params)
        #   b1, b2, ..., bn  (individual 2 params)
        # For non-ma training, params become
        #   a1, a2, ..., an  (individual 1 params)
        #   a1, a2, ..., an  (individual 1 params)
        #   a1, a2, ..., an  (individual 1 params)
        #   b1, b2, ..., bn  (individual 2 params)
        #   b1, b2, ..., bn  (individual 2 params)
        #   b1, b2, ..., bn  (individual 2 params)
        params = duplicate_params(params, n_repeats, self._ma_training)

        self._key, reset_keys = get_task_reset_keys(
            self._key, test, self._pop_size, self._n_evaluations, n_repeats,
            self._ma_training)

        # Reset the tasks and the policy.
        task_state = task_reset_func(reset_keys)
        policy_state = policy_reset_func(task_state)
        if self._num_device > 1:
            params = split_params_for_pmap(params)
            task_state = split_states_for_pmap(task_state)
            policy_state = split_states_for_pmap(policy_state)

        # Do the rollouts.
        scores, all_obs, masks, final_states = rollout_func(
            task_state, policy_state, params, self.obs_params)
        if self._num_device > 1:
            all_obs = reshape_data_from_pmap(all_obs)
            masks = reshape_data_from_pmap(masks)
            final_states = merge_state_from_pmap(final_states)

        if not test and not self.obs_normalizer.is_dummy:
            self.obs_params = self.obs_normalizer.update_normalization_params(
                obs_buffer=all_obs, obs_mask=masks, obs_params=self.obs_params)

        if self._ma_training:
            if not test:
                # In training, each agent has different parameters.
                scores = jnp.mean(
                    scores.ravel().reshape((n_repeats, -1)), axis=0)
            else:
                # In tests, they share the same parameters.
                scores = jnp.mean(
                    scores.ravel().reshape((n_repeats, -1)), axis=1)
        else:
            scores = jnp.mean(scores.ravel().reshape((-1, n_repeats)), axis=-1)

        # Note: QD methods do not support ma_training for now.
        if not self._ma_training:
            final_states = tree_map(
                lambda x: x.reshape((scores.shape[0], n_repeats, *x.shape[1:])),
                final_states)

        return scores, self._bd_summarize_fn(final_states)

=== ./evojax/task/flocking.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
This flocking task is based on the following colab notebook:
https://github.com/google/jax-md/blob/main/notebooks/flocking.ipynb
"""

from PIL import Image, ImageDraw
from functools import partial
from typing import Tuple

import jax
from jax import vmap
import jax.numpy as jnp
from flax.struct import dataclass

from evojax.task.base import VectorizedTask
from evojax.task.base import TaskState

DT = 0.2
SPEED = 0.12
J_ALIGN = 0.2
D_ALIGN = 0.15
J_AVOID = 0.1
D_AVOID = 0.12
J_COHESION = 0.3
D_COHESION = 0.3
ALPHA = 3

SCREEN_W = 400
SCREEN_H = 300
FISH_SIZE = 20.
BOIDS_NUM = 30
NEIGHBOR_NUM = 5


@dataclass
class State(TaskState):
    obs: jnp.ndarray
    state: jnp.ndarray
    steps: jnp.int32
    key: jnp.ndarray


def sample_position(key: jnp.ndarray, n: jnp.ndarray) -> jnp.ndarray:
    return jax.random.uniform(key, shape=(n, 2), minval=0.0, maxval=1.0)


def sample_theta(key: jnp.ndarray, n: jnp.ndarray) -> jnp.ndarray:
    return jax.random.uniform(key, shape=(n, 1), maxval=2. * jnp.pi)


def unpack_obs(obs: jnp.ndarray) -> jnp.ndarray:
    position, theta = obs[..., :2], obs[..., 2:]
    return position, theta


def unpack_act(action: jnp.ndarray) -> jnp.ndarray:
    d_theta, d_speed = action[..., :1], action[..., 1:2]
    return d_theta, d_speed


def pack_obs(position: jnp.ndarray, theta: jnp.ndarray) -> jnp.ndarray:
    return jnp.concatenate([position, theta], axis=-1)


def displacement(p1: jnp.ndarray, p2: jnp.ndarray) -> jnp.ndarray:
    dR = p1 - p2
    return jnp.mod(dR + 0.5, 1) - 0.5


def map_product(displacement):
    return vmap(vmap(displacement, (0, None), 0), (None, 0), 0)


def calc_distance(dR: jnp.ndarray) -> jnp.ndarray:
    dr = jnp.sqrt(jnp.sum(dR**2, axis=-1))
    return dr


def select_xy(xy: jnp.ndarray, ix: jnp.ndarray) -> jnp.ndarray:
    return jnp.take(xy, ix, axis=0)


_select_xy = jax.vmap(select_xy, in_axes=(None, 0))


def choose_neighbor(state: jnp.ndarray) -> jnp.ndarray:
    position, _ = unpack_obs(state)
    dR = map_product(displacement)(position, position)
    dr = calc_distance(dR)
    sort_idx = jnp.argsort(dr, axis=1)
    neighbor_ix = sort_idx[:, :NEIGHBOR_NUM]
    neighbors = _select_xy(state, neighbor_ix)
    return jnp.reshape(neighbors, (len(state), -1))


def normal(theta: jnp.ndarray) -> jnp.ndarray:
    return jnp.concatenate([jnp.cos(theta), jnp.sin(theta)], axis=-1)


def field_of_view_mask(dR, N, theta_min, theta_max):
    dr = calc_distance(dR)
    dR_hat = dR / dr
    ctheta = jnp.dot(dR_hat, N)
    return jnp.logical_and(ctheta > jnp.cos(theta_max),
                           ctheta < jnp.cos(theta_min))


def align_fn(dR, N_1, N_2, J_align, D_align, alpha):
    dr = calc_distance(dR) / D_align
    energy = J_align / alpha * (1. - dr)**alpha * (1 - jnp.dot(N_1, N_2))**2
    return jnp.where(dr < 1.0, energy, 0.)


def avoid_fn(dR, J_avoid, D_avoid, alpha):
    dr = calc_distance(dR) / D_avoid
    energy = J_avoid / alpha * (1. - dr)**alpha
    return jnp.where(dr < 1.0, energy, 0.)


def cohesion_fn(dR, N, mask, J_cohesion, D_cohesion, eps=1e-7):
    dr = calc_distance(dR)

    mask = jnp.reshape(mask, mask.shape + (1, ))
    dr = jnp.reshape(dr, dr.shape + (1, ))
    mask = jnp.logical_and(dr < D_cohesion, mask)

    N_com = jnp.where(mask, 1.0, 0)
    dR_com = jnp.where(mask, dR, 0)
    dR_com = jnp.sum(dR_com, axis=1) / (jnp.sum(N_com, axis=1) + eps)
    dR_com = dR_com / jnp.linalg.norm(dR_com + eps, axis=1, keepdims=True)
    return 0.5 * J_cohesion * (1 - jnp.sum(dR_com * N, axis=1))**2


def calc_energy(position, theta):
    E_align = partial(align_fn, J_align=J_ALIGN, D_align=D_ALIGN, alpha=ALPHA)
    E_align = vmap(vmap(E_align, (0, None, 0)), (0, 0, None))

    E_avoid = partial(avoid_fn, J_avoid=J_AVOID, D_avoid=D_AVOID, alpha=ALPHA)
    E_avoid = vmap(vmap(E_avoid))

    E_cohesion = partial(cohesion_fn,
                         J_cohesion=J_COHESION,
                         D_cohesion=D_COHESION)

    dR = map_product(displacement)(position, position)
    N = normal(theta)

    fov = partial(field_of_view_mask, theta_min=0., theta_max=jnp.pi / 3.)
    fov = vmap(vmap(fov, (0, None)))
    mask = fov(dR, N)

    return 0.5 * jnp.sum(E_align(dR, N, N) + E_avoid(dR)) + jnp.sum(
        E_cohesion(dR, N, mask))


def update_state(state, action, action_type):
    position, theta = unpack_obs(state.state)
    action = jnp.concatenate([action, jnp.ones_like(action)], axis=1)
    d_theta, d_speed = unpack_act(action)
    N = normal(theta)
    d_speed = jax.lax.cond(
        action_type,
        lambda x: (x + 1) / 2 * 0.4 + 0.8,
        lambda x: x,
        d_speed
        )
    new_obs = pack_obs(jnp.mod(position + DT * SPEED * N * d_speed, 1),
                       theta + DT * d_theta)
    return new_obs


def get_reward(state: State, max_steps: jnp.int32, reward_type: jnp.int32):
    position, theta = unpack_obs(state.state)
    reward = calc_energy(position, theta)
    reward = jax.lax.cond(
        reward_type == 0,
        lambda x: -x,
        lambda x: -x * (state.steps / max_steps) ** 2,
        reward)
    return reward


def to_pillow_coordinate(position, width, height):
    # Fix the format for drawing with pillow.
    return jnp.stack([position[:, 0] * width,
                      (1.0 - position[:, 1]) * height], axis=1)


def rotate(px, py, cx, cy, angle):
    R = jnp.array([[jnp.cos(angle), jnp.sin(angle)],
                   [-jnp.sin(angle), jnp.cos(angle)]])
    u = jnp.array([px - cx, py - cy])
    x, y = jnp.dot(R, u) + jnp.array([cx, cy])
    return x, y


def render_fishes(position, theta, width, height):
    position = to_pillow_coordinate(position, width, height)
    colors = [(88, 167, 210), (0, 153, 255), (56, 96, 123)]
    color_map = [i % len(colors) for i in range(BOIDS_NUM)]

    image = Image.new("RGB", (width, height), (255, 255, 255))
    draw = ImageDraw.Draw(image)

    fish_size = FISH_SIZE
    fish_size_half = (fish_size + 1) // 2

    for i in range(BOIDS_NUM):
        x, y = position[i]
        angle = theta[i][0]

        polygon_xy = [
            (x - fish_size_half, y - fish_size_half / 3),
            (x - fish_size_half / 5 * 3, y - fish_size_half / 4),
            (x, y - fish_size_half / 2),
            (x + fish_size_half / 3 * 2, y - fish_size_half / 3),
            (x + fish_size_half, y),
            (x + fish_size_half / 3 * 2, y + fish_size_half / 3),
            (x, y + fish_size_half / 2),
            (x - fish_size_half / 5 * 3, y + fish_size_half / 4),
            (x - fish_size_half, y + fish_size_half / 3),
        ]

        polygon_xy = [rotate(px, py, x, y, angle) for px, py in polygon_xy]
        draw.polygon(xy=polygon_xy, fill=colors[color_map[i]])
    return image


def render_single(obs_single):
    position, theta = unpack_obs(obs_single)
    image = render_fishes(position, theta, SCREEN_W * 2, SCREEN_H * 2)  # anti-aliasing
    image = image.resize((SCREEN_W, SCREEN_H), resample=Image.LANCZOS)
    return image


class FlockingTask(VectorizedTask):

    def __init__(
            self,
            max_steps: int = 150,
            reward_type: int = 0,  # (0: as it is, 1: increase rewards for late step)
            action_type: int = 0   # (0: theta, 1: theta/speed)
            ):
        self.max_steps = max_steps
        self.obs_shape = tuple([NEIGHBOR_NUM * 3, BOIDS_NUM])
        self.act_shape = tuple([action_type + 1, ])

        def reset_fn(key):
            next_key, key = jax.random.split(key)
            position = sample_position(key, BOIDS_NUM)
            theta = sample_theta(key, BOIDS_NUM)
            state = pack_obs(position, theta)
            return State(obs=choose_neighbor(state),
                         state=state,
                         steps=jnp.zeros((), dtype=jnp.int32),
                         key=next_key)
        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            new_state = update_state(state, action, action_type)
            new_obs = choose_neighbor(new_state)
            new_steps = jnp.int32(state.steps + 1)
            next_key, _ = jax.random.split(state.key)
            reward = get_reward(state, max_steps, reward_type)
            done = jnp.where(max_steps <= new_steps, True, False)
            return State(obs=new_obs,
                         state=new_state,
                         steps=new_steps,
                         key=next_key), reward, done
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def step(self, state: State,
             action: jnp.ndarray) -> Tuple[State, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

    def reset(self, key: jnp.ndarray):
        return self._reset_fn(key)

    @staticmethod
    def render(state: State, task_id: int) -> Image:
        return render_single(state.state[task_id])

=== ./evojax/task/brax_task.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
from typing import Optional
from typing import Tuple
from typing import Union

import jax
import jax.numpy as jnp
from flax.struct import dataclass

from evojax.task.base import VectorizedTask
from evojax.task.base import TaskState
from evojax.task.base import BDExtractor

try:
    from brax.envs import create
    from brax.envs import State as BraxState
except ModuleNotFoundError:
    print('You need to install brax for Brax tasks:')
    print('  pip install git+https://github.com/google/brax.git@main')
    sys.exit(1)


@dataclass
class State(TaskState):
    state: BraxState
    obs: jnp.ndarray


@dataclass
class BDExtractorState(TaskState):
    # Add feet_contact state for map elites here.
    state: BraxState
    obs: jnp.ndarray
    feet_contact: jnp.ndarray


def get_state_dataclass(**states):
    if 'feet_contact' in states:
        return BDExtractorState(**states)
    else:
        return State(**states)


class BraxTask(VectorizedTask):
    """Tasks from the Brax simulator."""

    def __init__(self,
                 env_name: str,
                 max_steps: int = 1000,
                 legacy_spring: bool = True,
                 bd_extractor: Optional[BDExtractor] = None,
                 test: bool = False):
        self.max_steps = max_steps
        self.bd_extractor = bd_extractor
        self.test = test
        brax_env = create(
            env_name=env_name,
            episode_length=max_steps,
            legacy_spring=legacy_spring,
        )
        self.obs_shape = tuple([brax_env.observation_size, ])
        self.act_shape = tuple([brax_env.action_size, ])

        def detect_feet_contact(state, action):
            _, info = brax_env.sys.step(state.qp, action)
            has_contacts = jnp.abs(
                jnp.clip(info.contact.vel, a_min=-1., a_max=1.)
            ).sum(axis=-1) > 0
            return has_contacts[2::2].astype(jnp.int32)

        def set_feet_contact_points():
            # TODO: Got values through trail and error, need to verify if actually correct.
            contact_point_mapping = {"ant": 4,
                                     "halfcheetah": 3,
                                     "humanoid": 5,
                                     "fetch": 6}
            return contact_point_mapping[env_name]

        def reset_fn(key):
            state = brax_env.reset(key)
            if bd_extractor is not None:
                state = get_state_dataclass(state=state, obs=state.obs,
                                            feet_contact=jnp.zeros(set_feet_contact_points(), dtype=jnp.int32))
                state = bd_extractor.init_extended_state(state)
            else:
                state = get_state_dataclass(state=state, obs=state.obs)
            return state

        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            if bd_extractor is not None:
                feet_contact = detect_feet_contact(state.state, action)
                brax_state = brax_env.step(state.state, action)
                state = state.replace(
                    state=brax_state, obs=brax_state.obs, feet_contact=feet_contact)
                state = bd_extractor.update(
                    state, action, brax_state.reward, brax_state.done)
            else:
                brax_state = brax_env.step(state.state, action)
                state = state.replace(
                    state=brax_state, obs=brax_state.obs)
            return state, brax_state.reward, brax_state.done

        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.ndarray) -> Union[State, BDExtractorState]:
        return self._reset_fn(key)

    def step(self,
             state: Union[State, BDExtractorState],
             action: jnp.ndarray) -> Tuple[Union[State, BDExtractorState], jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)


class AntBDExtractor(BDExtractor):
    """Behavior descriptor extractor for the Ant locomotion task."""

    def __init__(self, logger):
        # Each BD represents the quantized foot-ground contact time ratio.
        bd_spec = [
            ('feet1_contact', 10),
            ('feet2_contact', 10),
            ('feet3_contact', 10),
            ('feet4_contact', 10),
        ]
        bd_state_spec = [
            ('feet_contact_times', jnp.ndarray),
            ('step_cnt', jnp.int32),
            ('valid_mask', jnp.int32),
        ]
        super(AntBDExtractor, self).__init__(bd_spec, bd_state_spec, BDExtractorState)
        self._logger = logger

    def init_state(self, extended_task_state):
        return {
            'feet_contact_times': jnp.zeros_like(
                extended_task_state.feet_contact, dtype=jnp.int32),
            'step_cnt': jnp.zeros((), dtype=jnp.int32),
            'valid_mask': jnp.ones((), dtype=jnp.int32),
        }

    def update(self, extended_task_state, action, reward, done):
        valid_mask = (
                extended_task_state.valid_mask * (1 - done)).astype(jnp.int32)
        return extended_task_state.replace(
            feet_contact_times=(
                    extended_task_state.feet_contact_times +
                    extended_task_state.feet_contact * valid_mask),
            step_cnt=extended_task_state.step_cnt + valid_mask,
            valid_mask=valid_mask)

    def summarize(self, extended_task_state):
        feet_contact_ratio = (
                extended_task_state.feet_contact_times /
                extended_task_state.step_cnt[..., None]).mean(axis=1)
        bds = {bd[0]: (feet_contact_ratio[:, i] * bd[1]).astype(jnp.int32)
               for i, bd in enumerate(self.bd_spec)}
        return extended_task_state.replace(**bds)

=== ./evojax/task/__init__.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .base import VectorizedTask


__all__ = ['VectorizedTask']

=== ./evojax/task/procgen_task.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import multiprocessing as mp
import numpy as np
from typing import Tuple

import jax
import jax.numpy as jnp
from flax.struct import dataclass

from evojax.task.base import VectorizedTask
from evojax.task.base import TaskState


@jax.jit
def to_jnp_array(data: np.ndarray):
    return jnp.array(data, dtype=jnp.float32)


@dataclass
class State(TaskState):
    obs: jnp.ndarray


class ProcgenTask(VectorizedTask):
    """Wrapper of the procgen env."""

    def __init__(self,
                 env_name: str,
                 num_envs: int = 1024,
                 n_repeats: int = 1,
                 num_levels: int = 200,
                 distribution_mode: str = 'easy',
                 max_steps: int = 1000,
                 num_threads: int = -1,
                 test: bool = False):
        self.max_steps = max_steps
        self.test = test

        try:
            from procgen import ProcgenEnv
        except ModuleNotFoundError:
            print('You need to install procgen for this task:')
            print('  pip install procgen')
            sys.exit(1)

        procgen_venv = ProcgenEnv(
            num_envs=4,
            env_name=env_name,
            num_levels=0 if test else num_levels,
            start_level=num_levels if test else 0,
            distribution_mode=distribution_mode,
            num_threads=num_threads if num_threads > 0 else mp.cpu_count(),
        )
        obs_space = procgen_venv.observation_space['rgb']
        act_space = procgen_venv.action_space
        self.procgen_venv = procgen_venv
        self.obs_shape = obs_space.shape
        self.act_shape = act_space.shape
        self.num_acts = act_space.n
        assert num_envs % n_repeats == 0
        pop_size = num_envs // n_repeats

        def reset_fn(key):
            rand_seed = int(jax.random.randint(
                key[0], shape=(), minval=0, maxval=2 ** 31 - 1))
            self.procgen_venv = ProcgenEnv(
                num_envs=num_envs,
                env_name=env_name,
                num_levels=0 if test else num_levels,
                start_level=num_levels if test else 0,
                distribution_mode=distribution_mode,
                num_threads=num_threads if num_threads > 0 else mp.cpu_count(),
                rand_seed=rand_seed,
                use_backgrounds=True,
            )
            if not self.test:
                state = self.procgen_venv.env.get_state()
                assert len(state) == num_envs, '{} vs {}'.format(
                    len(state), num_envs)
                ss = [x for _ in range(pop_size) for x in state[:n_repeats]]
                self.procgen_venv.env.set_state(ss)

            obs, _, _, _ = self.procgen_venv.step(np.zeros(num_envs))
            return State(obs=to_jnp_array(obs['rgb']))
        self._reset_fn = reset_fn

        def step_fn(state, action):
            action_np = np.array(action)
            obs, reward, done, info = self.procgen_venv.step(action_np)
            return (State(obs=to_jnp_array(obs['rgb'])),
                    to_jnp_array(reward), to_jnp_array(done))
        self._step_fn = step_fn

    def reset(self, key: jnp.ndarray) -> TaskState:
        return self._reset_fn(key)

    def step(self,
             state: TaskState,
             action: jnp.ndarray) -> Tuple[TaskState, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

=== ./evojax/task/cartpole.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Tuple
from PIL import Image
from PIL import ImageDraw
import numpy as np

import jax
import jax.numpy as jnp
from jax import random
from flax.struct import dataclass

from evojax.task.base import TaskState
from evojax.task.base import VectorizedTask


GRAVITY = 9.82
CART_MASS = 0.5
POLE_MASS = 0.5
POLE_LEN = 0.6
FRICTION = 0.1
FORCE_SCALING = 10.0
DELTA_T = 0.01
CART_X_LIMIT = 2.4

SCREEN_W = 600
SCREEN_H = 600
CART_W = 40
CART_H = 20
VIZ_SCALE = 100
WHEEL_RAD = 5


@dataclass
class State(TaskState):
    obs: jnp.ndarray
    state: jnp.ndarray
    steps: jnp.int32
    key: jnp.ndarray


def get_init_state_easy(key: jnp.ndarray) -> jnp.ndarray:
    return (random.normal(key, shape=(4,)) * 0.2 +
            jnp.array([0, 0, jnp.pi, 0]))


def get_init_state_hard(key: jnp.ndarray) -> jnp.ndarray:
    return (jnp.multiply(random.uniform(key, shape=(4,)) * 2 - 1,
                         jnp.array([CART_X_LIMIT, 10., jnp.pi / 2., 10.])) +
            jnp.array([0, 0, jnp.pi, 0]))


def get_obs(state: jnp.ndarray) -> jnp.ndarray:
    x, x_dot, theta, theta_dot = state
    return jnp.array([x, x_dot, jnp.cos(theta), jnp.sin(theta), theta_dot])


def get_reward(state: jnp.ndarray) -> jnp.float32:
    x, x_dot, theta, theta_dot = state
    reward_theta = (jnp.cos(theta) + 1.0) / 2.0
    reward_x = jnp.cos((x / CART_X_LIMIT) * (jnp.pi / 2.0))
    return reward_theta * reward_x


def update_state(action: jnp.ndarray, state: jnp.ndarray) -> jnp.ndarray:
    action = jnp.clip(action, -1.0, 1.0)[0] * FORCE_SCALING
    x, x_dot, theta, theta_dot = state
    s = jnp.sin(theta)
    c = jnp.cos(theta)
    total_m = CART_MASS + POLE_MASS
    m_p_l = POLE_MASS * POLE_LEN
    x_dot_update = (
            (-2 * m_p_l * (theta_dot ** 2) * s +
             3 * POLE_MASS * GRAVITY * s * c +
             4 * action - 4 * FRICTION * x_dot) /
            (4 * total_m - 3 * POLE_MASS * c ** 2)
    )
    theta_dot_update = (
            (-3 * m_p_l * (theta_dot ** 2) * s * c +
             6 * total_m * GRAVITY * s +
             6 * (action - FRICTION * x_dot) * c) /
            (4 * POLE_LEN * total_m - 3 * m_p_l * c ** 2)
    )
    x = x + x_dot * DELTA_T
    theta = theta + theta_dot * DELTA_T
    x_dot = x_dot + x_dot_update * DELTA_T
    theta_dot = theta_dot + theta_dot_update * DELTA_T
    return jnp.array([x, x_dot, theta, theta_dot])


def out_of_screen(state: jnp.ndarray) -> jnp.float32:
    x = state[0]
    beyond_boundary_l = jnp.where(x < -CART_X_LIMIT, 1, 0)
    beyond_boundary_r = jnp.where(x > CART_X_LIMIT, 1, 0)
    return jnp.bitwise_or(beyond_boundary_l, beyond_boundary_r)


class CartPoleSwingUp(VectorizedTask):
    """Cart-pole swing up task."""

    def __init__(self,
                 max_steps: int = 1000,
                 harder: bool = False,
                 test: bool = False):

        self.max_steps = max_steps
        self.obs_shape = tuple([5, ])
        self.act_shape = tuple([1, ])
        self.test = test
        if harder:
            get_init_state_fn = get_init_state_hard
        else:
            get_init_state_fn = get_init_state_easy

        def reset_fn(key):
            next_key, key = random.split(key)
            state = get_init_state_fn(key)
            return State(state=state, obs=get_obs(state),
                         steps=jnp.zeros((), dtype=int), key=next_key)
        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            cur_state = update_state(action=action, state=state.state)
            reward = get_reward(state=cur_state)
            steps = state.steps + 1
            done = jnp.bitwise_or(out_of_screen(cur_state), steps >= max_steps)
            steps = jnp.where(done, jnp.zeros((), jnp.int32), steps)
            next_key, key = random.split(state.key)
            cur_state = jax.lax.cond(
                done, lambda x: get_init_state_fn(key), lambda x: x, cur_state)
            return State(state=cur_state, obs=get_obs(state=cur_state),
                         steps=steps, key=next_key), reward, done
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.ndarray) -> State:
        return self._reset_fn(key)

    def step(self,
             state: State,
             action: jnp.ndarray) -> Tuple[State, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

    @staticmethod
    def render(state: State, task_id: int) -> Image:
        """Render a specified task."""
        img = Image.new('RGB', (SCREEN_W, SCREEN_H), (255, 255, 255))
        draw = ImageDraw.Draw(img)
        x, _, theta, _ = np.array(state.state[task_id])
        cart_y = SCREEN_H // 2 + 100
        cart_x = x * VIZ_SCALE + SCREEN_W // 2
        # Draw the horizon.
        draw.line(
            (0, cart_y + CART_H // 2 + WHEEL_RAD,
             SCREEN_W, cart_y + CART_H // 2 + WHEEL_RAD),
            fill=(0, 0, 0), width=1)
        # Draw the cart.
        draw.rectangle(
            (cart_x - CART_W // 2, cart_y - CART_H // 2,
             cart_x + CART_W // 2, cart_y + CART_H // 2),
            fill=(255, 0, 0), outline=(0, 0, 0))
        # Draw the wheels.
        draw.ellipse(
            (cart_x - CART_W // 2 - WHEEL_RAD,
             cart_y + CART_H // 2 - WHEEL_RAD,
             cart_x - CART_W // 2 + WHEEL_RAD,
             cart_y + CART_H // 2 + WHEEL_RAD),
            fill=(220, 220, 220), outline=(0, 0, 0))
        draw.ellipse(
            (cart_x + CART_W // 2 - WHEEL_RAD,
             cart_y + CART_H // 2 - WHEEL_RAD,
             cart_x + CART_W // 2 + WHEEL_RAD,
             cart_y + CART_H // 2 + WHEEL_RAD),
            fill=(220, 220, 220), outline=(0, 0, 0))
        # Draw the pole.
        draw.line(
            (cart_x, cart_y,
             cart_x + POLE_LEN * VIZ_SCALE * np.cos(theta - np.pi / 2),
             cart_y + POLE_LEN * VIZ_SCALE * np.sin(theta - np.pi / 2)),
            fill=(0, 0, 255), width=6)
        return img

=== ./evojax/task/README.md ===
# Tasks

| Tasks                | Description                                                                                                                                                                                                                                                                                                                                                                                         | Contributors                                            | Example                                                                                                                                                                                                                                                                                                          |
|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| MNIST                | MNIST classification ([source](https://github.com/google/evojax/blob/main/evojax/task/mnist.py)).                                                                                                                                                                                                                                                                                                   | EvoJAX team                                             | [script](https://github.com/google/evojax/blob/main/examples/train_mnist.py)                                                                                                                                                                                                                                     |
| Seq2seq              | A simple addition problem in a [seq2seq](https://github.com/google/flax/tree/main/examples/seq2seq) setting ([source](https://github.com/google/evojax/blob/main/evojax/task/seq2seq.py)).  <br>E.g., the agent may see '012+345', and then upon receiving the prompt '=', it outputs '357'.                                                                                                        | EvoJAX team                                             | [script](https://github.com/google/evojax/blob/main/examples/train_seq2seq.py)                                                                                                                                                                                                                                   |
| CartPole Swing Up    | The classic control task where the goal is to swing up a pole mounted on a cart ([source](https://github.com/google/evojax/blob/main/evojax/task/cartpole.py)).<br>We provide easy and hard levels where the latter has its initial states sampled from a wider range.                                                                                                                              | EvoJAX team                                             | [script](https://github.com/google/evojax/blob/main/examples/train_cartpole.py)                                                                                                                                                                                                                                  |
| Robot Control (Brax) | Robot control training in the [Brax](https://github.com/google/brax) simulator ([source](https://github.com/google/evojax/blob/main/evojax/task/brax_task.py)).                                                                                                                                                                                                                                     | EvoJAX team                                             | [notebook](https://github.com/google/evojax/blob/main/examples/notebooks/BraxTasks.ipynb)                                                                                                                                                                                                                        |
| Water World          | In this [task](https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html), the agent learns to catch as much food as possible while avoiding poisons ([source](https://github.com/google/evojax/blob/main/evojax/task/waterworld.py)).<br>We also show that it is possible for [multi-agent training](https://github.com/google/evojax/blob/main/evojax/task/ma_waterworld.py) in EvoJAX. | EvoJAX team                                             | [script1](https://github.com/google/evojax/blob/main/examples/train_waterworld.py)<br>[script2](https://github.com/google/evojax/blob/main/examples/train_waterworld_ma.py)                                                                                                                                      |
| Abstract Painting    | We reproduce a [computational creativity work](https://es-clip.github.io/) and highlight EvoJAX's modularity design in this task.                                                                                                                                                                                                                                                                   | EvoJAX team                                             | [notebook1](https://github.com/google/evojax/blob/main/examples/notebooks/AbstractPainting01.ipynb)<br>[notebook2](https://github.com/google/evojax/blob/main/examples/notebooks/AbstractPainting02.ipynb)<br>[notebook3](https://github.com/google/evojax/blob/main/examples/notebooks/HighResGIFfromSVG.ipynb) |
| Flocking             | We demonstrate a flocking system in EvoJAX where fishes learn to move in a coherent fashion ([source](https://github.com/google/evojax/blob/main/evojax/task/flocking.py)).<br>In this task, all fishes move at constant speed, they observe the states of the nearest K fishes and learn to orient.                                                                                                | [すずめ](https://github.com/mayu-snba19)<br>EvoJAX team | [notebook](https://github.com/google/evojax/blob/main/examples/notebooks/FlockingSimple.ipynb)                                                                                                                                                                                                                   |
| Slime Volleyball     | Slime Volleyball is a game created in the early 2000s by an unknown author. This task is designed for testing single and multi-agent learning algorithms. ([source](https://github.com/hardmaru/slimevolleygym)). | EvoJAX team | [script](https://github.com/google/evojax/blob/main/examples/train_slimevolley.py)                                                                                                                                                                                                                   |

=== ./evojax/task/mdkp.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import numpy as np
from typing import Tuple

import jax
import jax.numpy as jnp
from flax.struct import dataclass

from evojax.task.base import TaskState
from evojax.task.base import VectorizedTask


@dataclass
class State(TaskState):
    obs: jnp.ndarray
    sel: jnp.ndarray


class MDKP(VectorizedTask):
    """Multi-Dimensional Knapsack Problem (MDKP).

    Many real-world problems are MDKP in different contexts, and this task
    allows a user to simulate one by using their own data in the form of csv
    files. In addition, users can also use synthesized data.

    The format of the csv files are:

    1. capacity_csv should have N lines and S columns.
        bin1_attr1_cap, bin1_attr2_cap, ..., bin1_attrS_cap
        bin2_attr1_cap, bin2_attr2_cap, ..., bin2_attrS_cap
        ...
        binN_attr1_cap, binN_attr2_cap, ..., binN_attrS_cap

    2. items_csv should have M lines and S + 1 columns.
        item1_attr1, item1_attr2, ..., item1_attrS, item1_value
        item2_attr1, item2_attr2, ..., item2_attrS, item2_value
        ...
        itemM_attr1, itemM_attr2, ..., itemM_attrS, itemM_value

    A valid solution should select K<=M items that maximizes sum(item{k}_value),
    while keeping sum(item{k}_attr{j}) < bin{i}_attr{j}_cap
    if item{k} is assigned to bin{i}, for i in [1, N] and j in [1, S].
    """

    def __init__(self,
                 items_csv=None,
                 capacity_csv=None,
                 use_synthesized_data=True,
                 test=False):
        self.max_steps = 1
        self.test = test

        if use_synthesized_data:
            rnd = np.random.RandomState(seed=0)
            num_attrs = 3
            num_items = 2000
            num_bins = 5
            upper_bound = 100
            items = rnd.randint(
                low=0, high=upper_bound, size=(num_items, num_attrs + 1))

            num_items_per_bin = int(num_items * 0.5 / num_bins)
            num_sel_items = num_items_per_bin * num_bins
            ix = np.arange(num_items)
            rnd.shuffle(ix)
            sel_ix = ix[:num_sel_items]
            sel_items = items[sel_ix, :-1]
            caps = np.array(
                [sel_items[
                 (i * num_items_per_bin):(i + 1) * num_items_per_bin
                 ].sum(axis=0) for i in range(num_bins)])
        else:
            try:
                import pandas as pd
            except ModuleNotFoundError:
                print('This task requires pandas, '
                      'run "pip install -U pandas" to install.')
                sys.exit(1)

            caps = pd.read_csv(capacity_csv, header=None).values
            num_bins, num_attrs = caps.shape
            items = pd.read_csv(items_csv, header=None).values
            num_items, num_cols = items.shape
            assert num_cols == num_attrs + 1

        self.num_items = num_items
        self.num_attrs = num_attrs
        self.num_bins = num_bins
        self.items = items
        self.caps = caps
        items = jnp.array(items)
        caps = jnp.array(caps)
        self.obs_shape = tuple([num_items, num_attrs + 1])
        self.act_shape = tuple([num_bins, num_items])

        def reset_fn(key):
            return State(obs=items, sel=jnp.zeros([num_bins, num_items]))
        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            # selection.shape = (N, M), obs.shape = (M, S + 1)
            selection = jnp.where(action > 0.5, 1., 0.)
            # bin_items_attr_sum.shape = (N, S + 1)
            bin_items_attr_sum = jnp.matmul(selection, state.obs)
            reward = bin_items_attr_sum[:, -1].sum()
            # If any item is assigned more than once, we set reward to zero.
            times_selected = selection.sum(axis=0)
            assignment_violation = jnp.prod(jnp.where(times_selected > 1, 0, 1))
            reward = reward * assignment_violation
            # If there is any limit violation, we set reward to zero.
            violation = jnp.prod(
                jnp.where(bin_items_attr_sum[:, :-1] > caps, 0, 1).ravel())
            reward = reward * violation
            return State(obs=state.obs, sel=action), reward, jnp.ones(())
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.array) -> TaskState:
        return self._reset_fn(key)

    def step(self,
             state: TaskState,
             action: jnp.ndarray) -> Tuple[TaskState, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

=== ./evojax/task/slimevolley.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of the Slime Volley environment in EvoJAX

Slime Volleyball is a game created in the early 2000s by unknown author.

The game is very simple: the agent's goal is to get the ball to land on
the ground of its opponent's side, causing its opponent to lose a life.

Each agent starts off with five lives. The episode ends when either agent
loses all five lives, or after 3000 timesteps has passed. An agent receives
a reward of +1 when its opponent loses or -1 when it loses a life.

An agent loses when it loses 5 times in the Test environment, or if it
loses based on score count after 3000 time steps.

During Training, the game is simply played for 3000 time steps, not
terminating even when one player loses 5 times.

This task is based on:
https://otoro.net/slimevolley/

The implementation is based on:
https://github.com/hardmaru/slimevolleygym
"""

import math
import numpy as np

from typing import Tuple

import jax
import jax.numpy as jnp
from jax import random
from flax.struct import dataclass

from evojax.task.base import TaskState
from evojax.task.base import VectorizedTask

import cv2

from PIL import Image

# game settings:

RENDER_MODE = True

REF_W = 24*2
REF_H = REF_W
REF_U = 1.5  # ground height
REF_WALL_WIDTH = 1.0  # wall width
REF_WALL_HEIGHT = 3.5
PLAYER_SPEED_X = 10*1.75
PLAYER_SPEED_Y = 10*1.35
MAX_BALL_SPEED = 15*1.5
TIMESTEP = 1/30.
NUDGE = 0.1
FRICTION = 1.0  # 1 means no FRICTION, less means FRICTION
GRAVITY = -9.8*2*1.5

MAXLIVES = 5  # game ends when one agent loses this many games

WINDOW_WIDTH = 1200
WINDOW_HEIGHT = 500

FACTOR = WINDOW_WIDTH / REF_W

# if set to true, renders using cv2 directly on numpy array
# (otherwise uses pyglet / opengl -> much smoother for human player)
PIXEL_MODE = True
PIXEL_SCALE = 2  # Render at multiple of Pixel Obs resolution, then downscale.

PIXEL_WIDTH = 84*2*2
PIXEL_HEIGHT = 84*2


def setNightColors():
    # night time color:
    global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR
    global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR
    global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR
    BALL_COLOR = (217, 79, 0)
    AGENT_LEFT_COLOR = (35, 93, 188)
    AGENT_RIGHT_COLOR = (255, 236, 0)
    PIXEL_AGENT_LEFT_COLOR = (0, 87, 184)  # AZURE BLUE
    PIXEL_AGENT_RIGHT_COLOR = (254, 221, 0)  # YELLOW

    BACKGROUND_COLOR = (11, 16, 19)
    FENCE_COLOR = (102, 56, 35)
    COIN_COLOR = FENCE_COLOR
    GROUND_COLOR = (116, 114, 117)


def setDayColors():
    # day time color:
    # note: do not use day time colors for pixel-obs training.
    global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR
    global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR
    global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR
    global PIXEL_SCALE, PIXEL_WIDTH, PIXEL_HEIGHT
    PIXEL_SCALE = int(4*1.0)
    PIXEL_WIDTH = int(84*2*1.0)
    PIXEL_HEIGHT = int(84*1.0)
    BALL_COLOR = (255, 200, 20)
    AGENT_LEFT_COLOR = (240, 75, 0)
    AGENT_RIGHT_COLOR = (0, 150, 255)
    PIXEL_AGENT_LEFT_COLOR = (240, 75, 0)
    PIXEL_AGENT_RIGHT_COLOR = (0, 150, 255)

    BACKGROUND_COLOR = (255, 255, 255)
    FENCE_COLOR = (240, 210, 130)
    COIN_COLOR = FENCE_COLOR
    GROUND_COLOR = (128, 227, 153)


setNightColors()


def setPixelObsMode():
    """
    used for experimental pixel-observation mode
    note: new dim's chosen to be PIXEL_SCALE (2x) as Pixel Obs dims
          (will be downsampled)
    """
    global WINDOW_WIDTH, WINDOW_HEIGHT, FACTOR
    global AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR, PIXEL_MODE
    PIXEL_MODE = True
    WINDOW_WIDTH = PIXEL_WIDTH * PIXEL_SCALE
    WINDOW_HEIGHT = PIXEL_HEIGHT * PIXEL_SCALE
    FACTOR = WINDOW_WIDTH / REF_W
    AGENT_LEFT_COLOR = PIXEL_AGENT_LEFT_COLOR
    AGENT_RIGHT_COLOR = PIXEL_AGENT_RIGHT_COLOR


setPixelObsMode()


def upsize_image(img):
    return cv2.resize(img, (PIXEL_WIDTH*PIXEL_SCALE, PIXEL_HEIGHT*PIXEL_SCALE),
                      interpolation=cv2.INTER_NEAREST)


def downsize_image(img):
    return cv2.resize(img, (PIXEL_WIDTH, PIXEL_HEIGHT),
                      interpolation=cv2.INTER_AREA)


# conversion from space to pixels (allows us to render to diff resolutions)
def toX(x):
    return (x+REF_W/2)*FACTOR


def toP(x):
    return (x)*FACTOR


def toY(y):
    return y*FACTOR


def create_canvas(c):
    result = np.ones((WINDOW_HEIGHT, WINDOW_WIDTH, 3), dtype=np.uint8)
    for channel in range(3):
        result[:, :, channel] *= c[channel]
    return result


def rect(canvas, x, y, width, height, color):
    """ Processing style function to make it easy to port p5.js to python """
    return cv2.rectangle(canvas, (round(x), round(WINDOW_HEIGHT-y)),
                         (round(x+width), round(WINDOW_HEIGHT-y+height)),
                         color, thickness=-1, lineType=cv2.LINE_AA)


def half_circle(canvas, x, y, r, color):
    """ Processing style function to make it easy to port p5.js to python """
    return cv2.ellipse(canvas, (round(x), WINDOW_HEIGHT-round(y)),
                       (round(r), round(r)), 0, 0, -180, color, thickness=-1,
                       lineType=cv2.LINE_AA)


def circle(canvas, x, y, r, color):
    """ Processing style function to make it easy to port p5.js to python """
    return cv2.circle(canvas, (round(x), round(WINDOW_HEIGHT-y)), round(r),
                      color, thickness=-1, lineType=cv2.LINE_AA)


@dataclass
class BaselinePolicyParams(object):
    w: jnp.ndarray
    b: jnp.ndarray


def initBaselinePolicyParams():
    nGameInput = 8  # 8 states for agent
    nGameOutput = 3  # 3 buttons (forward, backward, jump)
    nRecurrentState = 4  # extra recurrent states for feedback.
    """See training details:
    https://blog.otoro.net/2015/03/28/neural-slime-volleyball/
    """
    weight = jnp.array(
        [7.5719, 4.4285, 2.2716, -0.3598, -7.8189, -2.5422, -3.2034, 0.3935,
         1.2202, -0.49, -0.0316, 0.5221, 0.7026, 0.4179, -2.1689, 1.646,
         -13.3639, 1.5151, 1.1175, -5.3561, 5.0442, 0.8451, 0.3987, -2.9501,
         -3.7811, -5.8994, 6.4167, 2.5014, 7.338, -2.9887, 2.4586, 13.4191,
         2.7395, -3.9708, 1.6548, -2.7554, -1.5345, -6.4708, 9.2426, -0.7392,
         0.4452, 1.8828, -2.6277, -10.851, -3.2353, -4.4653, -3.1153, -1.3707,
         7.318, 16.0902, 1.4686, 7.0391, 1.7765, -1.155, 2.6697, -8.8877,
         1.1958, -3.2839, -5.4425, 1.6809, 7.6812, -2.4732, 1.738, 0.3781,
         0.8718, 2.5886, 1.6911, 1.2953, -9.0052, -4.6038, -6.7447, -2.5528,
         0.4391, -4.9278, -3.6695, -4.8673, -1.6035, 1.5011, -5.6124, 4.9747,
         1.8998, 3.0359, 6.2983, -4.8568, -2.1888, -4.1143, -3.9874, -0.0459,
         4.7134, 2.8952, -9.3627, -4.685, 0.3601, -1.3699, 9.7294, 11.5596,
         0.1918, 3.0783, 0.0329, -0.1362, -0.1188, -0.7579, 0.3278, -0.977,
         -0.9377])
    weight = weight.reshape(nGameOutput+nRecurrentState,
                            nGameInput+nGameOutput+nRecurrentState)
    bias = jnp.array([2.2935, -2.0353, -1.7786, 5.4567,
                      -3.6368, 3.4996, -0.0685])

    return BaselinePolicyParams(weight, bias)


def initBaselinePolicyState():
    return jnp.zeros(7)


@dataclass
class ParticleState(object):
    x: jnp.float32
    y: jnp.float32
    prev_x: jnp.float32
    prev_y: jnp.float32
    vx: jnp.float32
    vy: jnp.float32
    r: jnp.float32


def initParticleState(x, y, vx, vy, r):
    return ParticleState(jnp.float32(x), jnp.float32(y),
                         jnp.float32(x), jnp.float32(y),
                         jnp.float32(vx), jnp.float32(vy),
                         jnp.float32(r))


@dataclass
class AgentState(object):
    direction: jnp.int32  # -1 means left, 1 means right player.
    x: jnp.float32
    y: jnp.float32
    r: jnp.float32
    vx: jnp.float32
    vy: jnp.float32
    desired_vx: jnp.float32
    desired_vy: jnp.float32
    life: jnp.int32


def initAgentState(direction, x, y):
    return AgentState(direction, x, y, 1.5, 0, 0, 0, 0, MAXLIVES)


@dataclass
class GameState(object):
    ball: ParticleState
    agent_left: AgentState
    agent_right: AgentState
    hidden_left: jnp.ndarray  # rnn hidden state for internal policy
    hidden_right: jnp.ndarray
    action_left_flag: jnp.int32  # if 1, then use the action action_left
    action_left: jnp.ndarray
    action_right_flag: jnp.int32  # same as above
    action_right: jnp.ndarray


@dataclass
class State(TaskState):
    game_state: GameState
    obs: jnp.ndarray
    steps: jnp.int32
    key: jnp.ndarray


@dataclass
class Observation(object):  # is also the "RelativeState" in the original code
    """
    keeps track of the obs.
    Note: the observation is from the perspective of the agent.
    an agent playing either side of the fence must see obs the same way
    """
    x: jnp.float32  # agent
    y: jnp.float32
    vx: jnp.float32
    vy: jnp.float32
    bx: jnp.float32  # ball
    by: jnp.float32
    bvx: jnp.float32
    bvy: jnp.float32
    ox: jnp.float32  # opponent
    oy: jnp.float32
    ovx: jnp.float32
    ovy: jnp.float32


def getZeroObs() -> Observation:
    return Observation(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)


def getObsArray(rs: Observation):
    # scale inputs to be in the order
    # of magnitude of 10 for neural network. (legacy)
    scaleFactor = 10.0
    result = jnp.array([rs.x, rs.y, rs.vx, rs.vy,
                        rs.bx, rs.by, rs.bvx, rs.bvy,
                        rs.ox, rs.oy, rs.ovx, rs.ovy]) / scaleFactor
    return result


class Particle:
    """ used for the ball, and also for the round stub above the fence """
    def __init__(self, p: ParticleState, c):
        self.p = p
        self.c = c

    def display(self, canvas):
        return circle(canvas, toX(float(self.p.x)), toY(float(self.p.y)),
                      toP(float(self.p.r)), color=self.c)

    def move(self):
        self.p = ParticleState(self.p.x+self.p.vx*TIMESTEP,
                               self.p.y+self.p.vy*TIMESTEP,
                               self.p.x, self.p.y,
                               self.p.vx, self.p.vy, r=self.p.r)

    def applyAcceleration(self, ax, ay):
        self.p = ParticleState(self.p.x, self.p.y,
                               self.p.prev_x, self.p.prev_y,
                               self.p.vx+ax*TIMESTEP, self.p.vy+ay*TIMESTEP,
                               r=self.p.r)

    def checkEdges(self):
        oldp = self.p
        return_sign = jnp.where(oldp.x <= 0, -1, 1)
        newx = oldp.x
        newy = oldp.y
        newpx = oldp.prev_x
        newpy = oldp.prev_y
        newvx = oldp.vx
        newvy = oldp.vy

        newx = jnp.where(oldp.x <= (oldp.r-REF_W/2),
                         oldp.r-REF_W/2+NUDGE*TIMESTEP, newx)
        newvx = jnp.where(oldp.x <= (oldp.r-REF_W/2),
                          oldp.vx*(-FRICTION), newvx)

        newx = jnp.where(oldp.x >= (REF_W/2-oldp.r),
                         REF_W/2-oldp.r-NUDGE*TIMESTEP, newx)
        newvx = jnp.where(oldp.x >= (REF_W/2-oldp.r),
                          oldp.vx*(-FRICTION), newvx)

        return_value = jnp.where(oldp.y <= (oldp.r+REF_U), 1, 0)

        newy = jnp.where(oldp.y <= (oldp.r+REF_U),
                         oldp.r+REF_U+NUDGE*TIMESTEP, newy)
        newvy = jnp.where(oldp.y <= (oldp.r+REF_U),
                          oldp.vy*(-FRICTION), newvy)

        newy = jnp.where(oldp.y >= (REF_H-oldp.r),
                         REF_H-oldp.r-NUDGE*TIMESTEP, newy)

        newvy = jnp.where(oldp.y >= (REF_H-oldp.r),
                          oldp.vy*(-FRICTION), newvy)

        # fence:

        newx = jnp.where((oldp.x <= (REF_WALL_WIDTH/2+oldp.r)) &
                         (oldp.prev_x > (REF_WALL_WIDTH/2+oldp.r)) &
                         (oldp.y <= REF_WALL_HEIGHT),
                         REF_WALL_WIDTH/2+oldp.r+NUDGE*TIMESTEP, newx)
        newvx = jnp.where((oldp.x <= (REF_WALL_WIDTH/2+oldp.r)) &
                          (oldp.prev_x > (REF_WALL_WIDTH/2+oldp.r)) &
                          (oldp.y <= REF_WALL_HEIGHT),
                          oldp.vx*(-FRICTION), newvx)

        newx = jnp.where((oldp.x >= (-REF_WALL_WIDTH/2-oldp.r)) &
                         (oldp.prev_x < (-REF_WALL_WIDTH/2-oldp.r)) &
                         (oldp.y <= REF_WALL_HEIGHT),
                         -REF_WALL_WIDTH/2-oldp.r-NUDGE*TIMESTEP, newx)
        newvx = jnp.where((oldp.x >= (-REF_WALL_WIDTH/2-oldp.r)) &
                          (oldp.prev_x < (-REF_WALL_WIDTH/2-oldp.r)) &
                          (oldp.y <= REF_WALL_HEIGHT),
                          oldp.vx*(-FRICTION), newvx)

        self.p = ParticleState(newx, newy, newpx, newpy, newvx, newvy, oldp.r)
        return return_value*return_sign

    def bounce(self, p):  # bounce two balls that have collided (this and that)
        oldp = self.p
        abx = oldp.x-p.x
        aby = oldp.y-p.y
        abd = jnp.sqrt(abx*abx+aby*aby)
        abx /= abd  # normalize
        aby /= abd
        nx = abx  # reuse calculation
        ny = aby
        abx *= NUDGE
        aby *= NUDGE

        new_y = oldp.y
        new_x = oldp.x
        dy = (new_y - p.x)
        dx = (new_x - p.y)

        total_r = oldp.r+p.r
        total_r2 = total_r*total_r

        # this was a while loop in the orig code, but most cases < 15.
        for i in range(15):
            total_d2 = (dy*dy + dx*dx)
            new_x = jnp.where(total_d2 < total_r2, new_x+abx, new_x)
            new_y = jnp.where(total_d2 < total_r2, new_y+aby, new_y)
            dy = (p.y - new_y)
            dx = (p.x - new_x)

        ux = oldp.vx - p.vx
        uy = oldp.vy - p.vy
        un = ux*nx + uy*ny
        unx = nx*(un*2.)  # added factor of 2
        uny = ny*(un*2.)  # added factor of 2
        ux -= unx
        uy -= uny
        return ParticleState(x=new_x, y=new_y,
                             prev_x=oldp.prev_x, prev_y=oldp.prev_y,
                             vx=ux + p.vx, vy=uy + p.vy, r=oldp.r)

    def bounceIfColliding(self, p):
        dy = p.y - self.p.y
        dx = p.x - self.p.x
        d2 = (dx*dx+dy*dy)
        r = self.p.r+p.r
        r2 = r*r
        newp = self.bounce(p)

        # make if condition work with jax:
        newx = jnp.where(d2 < r2, newp.x, self.p.x)
        newy = jnp.where(d2 < r2, newp.y, self.p.y)
        newprev_x = jnp.where(d2 < r2, newp.prev_x, self.p.prev_x)
        newprev_y = jnp.where(d2 < r2, newp.prev_y, self.p.prev_y)
        newvx = jnp.where(d2 < r2, newp.vx, self.p.vx)
        newvy = jnp.where(d2 < r2, newp.vy, self.p.vy)
        self.p = ParticleState(x=newx, y=newy,
                               prev_x=newprev_x, prev_y=newprev_y,
                               vx=newvx, vy=newvy, r=self.p.r)

    def limitSpeed(self, maxSpeed):
        oldp = self.p
        mag2 = oldp.vx*oldp.vx+oldp.vy*oldp.vy
        mag = jnp.sqrt(mag2)

        newvx = oldp.vx
        newvy = oldp.vy
        newvx = jnp.where(mag2 > (maxSpeed*maxSpeed), newvx/mag, newvx)
        newvy = jnp.where(mag2 > (maxSpeed*maxSpeed), newvy/mag, newvy)
        newvx = jnp.where(mag2 > (maxSpeed*maxSpeed), newvx*maxSpeed, newvx)
        newvy = jnp.where(mag2 > (maxSpeed*maxSpeed), newvy*maxSpeed, newvy)

        self.p = ParticleState(x=oldp.x, y=oldp.y,
                               prev_x=oldp.prev_x, prev_y=oldp.prev_y,
                               vx=newvx, vy=newvy, r=oldp.r)


class Agent:
    """ keeps track of the agent in the game. note: not the policy network """
    def __init__(self, agent, c):
        self.p = agent
        self.state = getZeroObs()
        self.c = c

    def setAction(self, action):
        forward = jnp.int32(0)
        backward = jnp.int32(0)
        jump = jnp.int32(0)

        forward = jnp.where(action[0] > 0, 1, forward)
        backward = jnp.where(action[1] > 0, 1, backward)
        jump = jnp.where(action[2] > 0, 1, jump)

        new_desired_vx = jnp.float32(0.0)
        new_desired_vy = jnp.float32(0.0)

        new_desired_vx = jnp.where(forward & (1-backward),
                                   -PLAYER_SPEED_X, new_desired_vx)
        new_desired_vx = jnp.where(backward & (1-forward),
                                   PLAYER_SPEED_X, new_desired_vx)

        new_desired_vy = jnp.where(jump, PLAYER_SPEED_Y, new_desired_vy)

        p = self.p
        self.p = AgentState(p.direction, p.x, p.y, p.r, p.vx, p.vy,
                            new_desired_vx, new_desired_vy, p.life)

    def move(self):
        p = self.p
        self.p = AgentState(p.direction, p.x+p.vx*TIMESTEP, p.y+p.vy*TIMESTEP,
                            p.r, p.vx, p.vy,
                            p.desired_vx, p.desired_vy,
                            p.life)

    def update(self):
        p = self.p
        new_vy = p.vy + GRAVITY*TIMESTEP

        new_vy = jnp.where(p.y <= REF_U+NUDGE*TIMESTEP, p.desired_vy, new_vy)

        new_vx = p.desired_vx*p.direction

        self.p = AgentState(p.direction, p.x, p.y, p.r, new_vx, new_vy,
                            p.desired_vx, p.desired_vy, p.life)

        self.move()

        p = self.p

        # stay in their own half:

        new_y = p.y
        new_vy = p.vy
        new_y = jnp.where(p.y <= REF_U, REF_U, new_y)
        new_vy = jnp.where(p.y <= REF_U, 0, new_vy)

        # stay in their own half:
        new_vx = p.vx
        new_x = p.x

        new_vx = jnp.where(p.x*p.direction <= (REF_WALL_WIDTH/2+p.r),
                           0, new_vx)
        new_x = jnp.where(p.x*p.direction <= (REF_WALL_WIDTH/2+p.r),
                          p.direction*(REF_WALL_WIDTH/2+p.r), new_x)

        new_vx = jnp.where(p.x*p.direction >= (REF_W/2-p.r), 0, new_vx)
        new_x = jnp.where(p.x*p.direction >= (REF_W/2-p.r),
                          p.direction*(REF_W/2-p.r), new_x)

        self.p = AgentState(p.direction, new_x, new_y, p.r,
                            new_vx, new_vy, p.desired_vx, p.desired_vy,
                            p.life)

    def updateLife(self, result):
        """ updates the life based on result and internal direction """
        p = self.p
        updateAmount = p.direction*result  # only update if this value is -1
        new_life = jnp.where(updateAmount < 0, p.life-1, p.life)
        self.p = AgentState(p.direction, p.x, p.y, p.r, p.vx, p.vy,
                            p.desired_vx, p.desired_vy, new_life)

    def updateState(self, ball: ParticleState, opponent: AgentState):
        """ normalized to side, customized for each agent's perspective"""
        p = self.p
        # agent's self
        x = p.x*p.direction
        y = p.y
        vx = p.vx*p.direction
        vy = p.vy
        # ball
        bx = ball.x*p.direction
        by = ball.y
        bvx = ball.vx*p.direction
        bvy = ball.vy
        # opponent
        ox = opponent.x*(-p.direction)
        oy = opponent.y
        ovx = opponent.vx*(-p.direction)
        ovy = opponent.vy

        self.state = Observation(x, y, vx, vy, bx, by,
                                 bvx, bvy, ox, oy, ovx, ovy)

    def getObservation(self):
        return getObsArray(self.state)

    def display(self, canvas, ball_x, ball_y):
        bx = float(ball_x)
        by = float(ball_y)
        p = self.p
        x = float(p.x)
        y = float(p.y)
        r = float(p.r)
        direction = int(p.direction)

        angle = math.pi * 60 / 180
        if direction == 1:
            angle = math.pi * 120 / 180
        eyeX = 0
        eyeY = 0

        canvas = half_circle(canvas, toX(x), toY(y), toP(r), color=self.c)

        # track ball with eyes (replace with observed info later):
        c = math.cos(angle)
        s = math.sin(angle)
        ballX = bx-(x+(0.6)*r*c)
        ballY = by-(y+(0.6)*r*s)

        dist = math.sqrt(ballX*ballX+ballY*ballY)
        eyeX = ballX/dist
        eyeY = ballY/dist

        canvas = circle(canvas, toX(x+(0.6)*r*c), toY(y+(0.6)*r*s),
                        toP(r)*0.3, color=(255, 255, 255))
        canvas = circle(canvas, toX(x+(0.6)*r*c+eyeX*0.15*r),
                        toY(y+(0.6)*r*s+eyeY*0.15*r), toP(r)*0.1,
                        color=(0, 0, 0))

        # draw coins (lives) left
        num_lives = int(p.life)
        for i in range(1, num_lives):
            canvas = circle(canvas, toX(direction*(REF_W/2+0.5-i*2.)),
                            WINDOW_HEIGHT-toY(1.5), toP(0.5),
                            color=COIN_COLOR)

        return canvas


class Wall:
    """ used for the fence, and also the ground """
    def __init__(self, x, y, w, h, c):
        self.x = x
        self.y = y
        self.w = w
        self.h = h
        self.c = c

    def display(self, canvas):
        return rect(canvas, toX(self.x-self.w/2), toY(self.y+self.h/2),
                    toP(self.w), toP(self.h), color=self.c)


def baselinePolicy(obs: jnp.ndarray, state: jnp.ndarray,
                   params: BaselinePolicyParams):
    """ take obs, prev rnn state, return updated rnn state, action """
    nGameInput = 8  # 8 states that policy cares about (ignores last 4)
    nGameOutput = 3  # 3 buttons (forward, backward, jump)

    weight = params.w
    bias = params.b
    inputState = jnp.concatenate([obs[:nGameInput], state])
    outputState = jnp.tanh(jnp.dot(weight, inputState)+bias)
    action = jnp.zeros(nGameOutput)
    action = jnp.where(outputState[:nGameOutput] > 0.75, 1, action)
    return outputState, action


class Game:
    """
    the main slime volley game.
    can be used in various settings,
    such as ai vs ai, ai vs human, human vs human
    """
    def __init__(self, gameState):
        self.baselineParams = initBaselinePolicyParams()
        self.ground = None
        self.fence = None
        self.fenceStub = None
        self.reset(gameState)

    def reset(self, gameState):
        self.ground = Wall(0, 0.75, REF_W, REF_U, c=GROUND_COLOR)
        self.fence = Wall(0, 0.75 + REF_WALL_HEIGHT/2, REF_WALL_WIDTH,
                          (REF_WALL_HEIGHT-1.5), c=FENCE_COLOR)
        fenceStubParticle = initParticleState(0, REF_WALL_HEIGHT,
                                              0, 0, REF_WALL_WIDTH/2)
        self.fenceStub = Particle(fenceStubParticle, c=FENCE_COLOR)
        self.setGameState(gameState)

    def setGameState(self, gameState):
        self.ball = Particle(gameState.ball, c=BALL_COLOR)
        self.agent_left = Agent(gameState.agent_left, c=AGENT_LEFT_COLOR)
        self.agent_right = Agent(gameState.agent_right, c=AGENT_RIGHT_COLOR)
        self.agent_left.updateState(self.ball.p, self.agent_right.p)
        self.agent_right.updateState(self.ball.p, self.agent_left.p)
        self.hidden_left = gameState.hidden_left
        self.hidden_right = gameState.hidden_right
        self.action_left_flag = gameState.action_left_flag
        self.action_left = gameState.action_left
        self.action_right_flag = gameState.action_right_flag
        self.action_right = gameState.action_right

    def setLeftAction(self, action):
        self.action_left_flag = jnp.int32(1)
        self.action_left = action

    def setRightAction(self, action):
        self.action_right_flag = jnp.int32(1)
        self.action_right = action

    def setAction(self):
        obs_left = self.agent_left.getObservation()
        obs_right = self.agent_right.getObservation()
        self.hidden_left, action_left = baselinePolicy(
            obs_left, self.hidden_left, self.baselineParams)
        self.hidden_right, action_right = baselinePolicy(
            obs_right, self.hidden_right, self.baselineParams)
        # overwrite internal AI actions if the flags are turned on:
        action_left = jnp.where(
            self.action_left_flag, self.action_left, action_left)
        action_right = jnp.where(
            self.action_right_flag, self.action_right, action_right)
        self.agent_left.setAction(action_left)
        self.agent_right.setAction(action_right)

    def getGameState(self):
        return GameState(self.ball.p, self.agent_left.p, self.agent_right.p,
                         self.hidden_left, self.hidden_right,
                         self.action_left_flag, self.action_left,
                         self.action_right_flag, self.action_right)

    def step(self):
        """ main game loop """

        self.agent_left.update()
        self.agent_right.update()

        self.ball.applyAcceleration(0, GRAVITY)
        self.ball.limitSpeed(MAX_BALL_SPEED)
        self.ball.move()

        self.ball.bounceIfColliding(self.agent_left.p)
        self.ball.bounceIfColliding(self.agent_right.p)
        self.ball.bounceIfColliding(self.fenceStub.p)

        # negated, since we want reward to be from the persepctive
        # of right agent being trained.
        result = -self.ball.checkEdges()

        self.agent_left.updateLife(result)
        self.agent_right.updateLife(result)

        self.agent_left.updateState(self.ball.p, self.agent_right.p)
        self.agent_right.updateState(self.ball.p, self.agent_left.p)

        return result

    def display(self):
        canvas = create_canvas(c=BACKGROUND_COLOR)
        canvas = self.fence.display(canvas)
        canvas = self.fenceStub.display(canvas)
        canvas = self.agent_left.display(canvas, self.ball.p.x, self.ball.p.y)
        canvas = self.agent_right.display(
            canvas, self.ball.p.x, self.ball.p.y)
        canvas = self.ball.display(canvas)
        canvas = self.ground.display(canvas)
        canvas = downsize_image(canvas)
        # canvas = upsize_image(canvas)  # removed to save memory for render.
        return canvas


def initGameState(ball_vx, ball_vy):
    ball = initParticleState(0, REF_W/4, ball_vx, ball_vy, 0.5)
    agent_left = initAgentState(-1, -REF_W/4, 1.5)
    agent_right = initAgentState(1, REF_W/4, 1.5)
    hidden_left = initBaselinePolicyState()
    hidden_right = initBaselinePolicyState()
    action_left_flag = jnp.int32(0)  # left is the built-in AI
    action_left = jnp.array([0, 0, 1], dtype=jnp.float32)
    action_right_flag = jnp.int32(1)  # right is the agent being trained.
    action_right = jnp.array([0, 0, 1], dtype=jnp.float32)
    return GameState(ball, agent_left, agent_right,
                     hidden_left, hidden_right,
                     action_left_flag, action_left,
                     action_right_flag, action_right)


def newMatch(prevGameState: GameState, ball_vx, ball_vy) -> GameState:
    ball = initParticleState(0, REF_W/4, ball_vx, ball_vy, 0.5)
    p = prevGameState
    return GameState(ball, p.agent_left, p.agent_right,
                     p.hidden_left, p.hidden_right,
                     p.action_left_flag, p.action_left,
                     p.action_right_flag, p.action_right)


def get_random_ball_v(key: jnp.ndarray):
    result = random.uniform(key, shape=(2,)) * 2 - 1
    ball_vx = result[1]*20
    ball_vy = result[2]*7.5+17.5
    return ball_vx, ball_vy


def get_init_game_state_fn(key: jnp.ndarray):
    ball_vx, ball_vy = get_random_ball_v(key)
    return initGameState(ball_vx, ball_vy)


def get_new_match_state_fn(game_state: GameState,
                           key: jnp.ndarray) -> GameState:
    ball_vx, ball_vy = get_random_ball_v(key)
    return newMatch(game_state, ball_vx, ball_vy)


def update_state_for_new_match(game_state: GameState,
                               reward, key: jnp.ndarray):
    old_ball = game_state.ball
    ball_vx, ball_vy = get_random_ball_v(key)
    new_ball = initParticleState(0, REF_W/4, ball_vx, ball_vy, 0.5)
    x = jnp.where(reward == 0, old_ball.x, new_ball.x)
    y = jnp.where(reward == 0, old_ball.y, new_ball.y)
    prev_x = jnp.where(reward == 0, old_ball.prev_x, new_ball.prev_x)
    prev_y = jnp.where(reward == 0, old_ball.prev_y, new_ball.prev_y)
    vx = jnp.where(reward == 0, old_ball.vx, new_ball.vx)
    vy = jnp.where(reward == 0, old_ball.vy, new_ball.vy)
    ball = ParticleState(x, y, prev_x, prev_y, vx, vy, old_ball.r)
    p = game_state
    return GameState(ball, p.agent_left, p.agent_right,
                     p.hidden_left, p.hidden_right,
                     p.action_left_flag, p.action_left,
                     p.action_right_flag, p.action_right)


def update_state(action: jnp.ndarray, game_state: GameState, key: jnp.array):
    game = Game(game_state)
    game.setRightAction(action)
    game.setAction()
    reward = game.step()  # from perspective of the agent on the right
    updated_game_state = game.getGameState()
    obs = game.agent_right.getObservation()

    updated_game_state = update_state_for_new_match(
        updated_game_state, reward, key)

    return updated_game_state, reward, obs


def detect_done(game_state: GameState):
    result = jnp.bitwise_or(
        game_state.agent_left.life <= 0, game_state.agent_right.life <= 0)
    return result


def get_obs(game_state: GameState):
    game = Game(game_state)
    return game.agent_right.getObservation()


class SlimeVolley(VectorizedTask):
    """Neural Slime Volleyball Environment."""

    def __init__(self,
                 max_steps: int = 3000,
                 test: bool = False):

        self.max_steps = max_steps
        self.obs_shape = tuple([12, ])
        self.act_shape = tuple([3, ])
        self.test = test

        def reset_fn(key):
            next_key, key = random.split(key)
            game_state = get_init_game_state_fn(key)
            return State(game_state=game_state, obs=get_obs(game_state),
                         steps=jnp.zeros((), dtype=int), key=next_key)
        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            next_key, key = random.split(state.key)
            cur_state, reward, obs = update_state(
                action=action, game_state=state.game_state, key=key)
            steps = state.steps + 1
            done_test = jnp.bitwise_or(
                detect_done(cur_state), steps >= max_steps)
            # during training, go for all 3000 steps.
            done = jnp.where(self.test, done_test, steps >= max_steps)
            steps = jnp.where(done, jnp.zeros((), jnp.int32), steps)
            return State(game_state=cur_state, obs=obs,
                         steps=steps, key=next_key), reward, done
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.ndarray) -> State:
        return self._reset_fn(key)

    def step(self,
             state: State,
             action: jnp.ndarray) -> Tuple[State, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

    @staticmethod
    def render(state: State, task_id: int = 0) -> Image:
        """Render a specified task."""
        game = Game(state.game_state)
        canvas = game.display()
        img = Image.fromarray(canvas)
        return img

=== ./evojax/task/waterworld.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of the WaterWorld task in JAX.

Ref: https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html
"""

from typing import Tuple
from functools import partial
from PIL import Image
from PIL import ImageDraw
import numpy as np

import jax
import jax.numpy as jnp
from jax import random
from jax import tree_util
from flax.struct import dataclass

from evojax.task.base import TaskState
from evojax.task.base import VectorizedTask


SCREEN_W = 600
SCREEN_H = 600
BUBBLE_RADIUS = 10
MIN_DIST = 2 * BUBBLE_RADIUS
MAX_RANGE = 120
NUM_RANGE_SENSORS = 30
DELTA_ANG = 2 * 3.14 / NUM_RANGE_SENSORS

TYPE_VOID = 0
TYPE_WALL = 1
TYPE_FOOD = 2
TYPE_POISON = 3
TYPE_AGENT = 4
SENSOR_DATA_DIM = 5  # dist_wall, dist_food, dist_poison, item_vel_x, item_vel_y

ACT_UP = 0
ACT_DOWN = 1
ACT_LEFT = 2
ACT_RIGHT = 3


@dataclass
class BubbleStatus(object):
    pos_x: jnp.float32
    pos_y: jnp.float32
    vel_x: jnp.float32
    vel_y: jnp.float32
    bubble_type: jnp.int32
    valid: jnp.int32
    poison_cnt: jnp.int32


@dataclass
class State(TaskState):
    agent_state: BubbleStatus
    item_state: BubbleStatus
    obs: jnp.ndarray
    steps: jnp.int32
    key: jnp.ndarray


@partial(jax.vmap, in_axes=(0, None))
def create_bubbles(key: jnp.ndarray, is_agent: bool) -> BubbleStatus:
    k_pos_x, k_pos_y, k_vel, k_bubble_type = random.split(key, 4)
    if is_agent:
        bubble_type = TYPE_AGENT
        vel_x = vel_y = 0.
    else:
        bubble_type = jnp.where(
            random.uniform(k_bubble_type) > 0.5, TYPE_FOOD, TYPE_POISON)
        vel_x, vel_y = random.uniform(
            k_vel, shape=(2,), minval=-2.5, maxval=2.5)
    return BubbleStatus(
        pos_x=random.uniform(
            k_pos_x, shape=(), dtype=jnp.float32,
            minval=MIN_DIST, maxval=SCREEN_W - MIN_DIST),
        pos_y=random.uniform(
            k_pos_y, shape=(), dtype=jnp.float32,
            minval=MIN_DIST, maxval=SCREEN_H - MIN_DIST),
        vel_x=vel_x, vel_y=vel_y, bubble_type=bubble_type,
        valid=1, poison_cnt=0)


def get_reward(agent: BubbleStatus,
               items: BubbleStatus) -> Tuple[BubbleStatus,
                                             BubbleStatus,
                                             jnp.float32]:
    dist = jnp.sqrt(jnp.square(agent.pos_x - items.pos_x) +
                    jnp.square(agent.pos_y - items.pos_y))
    rewards = (jnp.where(items.bubble_type == TYPE_FOOD, 1., -1.) *
               items.valid * jnp.where(dist < MIN_DIST, 1, 0))
    poison_cnt = jnp.sum(jnp.where(rewards == -1., 1, 0)) + agent.poison_cnt
    reward = jnp.sum(rewards)
    items_valid = (dist >= MIN_DIST) * items.valid
    agent_state = BubbleStatus(
        pos_x=agent.pos_x, pos_y=agent.pos_y,
        vel_x=agent.vel_x, vel_y=agent.vel_y,
        bubble_type=agent.bubble_type,
        valid=agent.valid, poison_cnt=poison_cnt)
    items_state = BubbleStatus(
        pos_x=items.pos_x, pos_y=items.pos_y,
        vel_x=items.vel_x, vel_y=items.vel_y,
        bubble_type=items.bubble_type,
        valid=items_valid, poison_cnt=items.poison_cnt)
    return agent_state, items_state, reward


@jax.vmap
def update_item_state(item: BubbleStatus) -> BubbleStatus:
    vel_x = item.vel_x
    vel_y = item.vel_y
    pos_x = item.pos_x + vel_x
    pos_y = item.pos_y + vel_y
    # Collide with the west wall.
    vel_x = jnp.where(pos_x < 1, -vel_x, vel_x)
    pos_x = jnp.where(pos_x < 1, 1, pos_x)
    # Collide with the east wall.
    vel_x = jnp.where(pos_x > SCREEN_W - 1, -vel_x, vel_x)
    pos_x = jnp.where(pos_x > SCREEN_W - 1, SCREEN_W - 1, pos_x)
    # Collide with the north wall.
    vel_y = jnp.where(pos_y < 1, -vel_y, vel_y)
    pos_y = jnp.where(pos_y < 1, 1, pos_y)
    # Collide with the south wall.
    vel_y = jnp.where(pos_y > SCREEN_H - 1, -vel_y, vel_y)
    pos_y = jnp.where(pos_y > SCREEN_H - 1, SCREEN_H - 1, pos_y)
    return BubbleStatus(
        pos_x=pos_x, pos_y=pos_y, vel_x=vel_x, vel_y=vel_y,
        bubble_type=item.bubble_type, valid=item.valid,
        poison_cnt=item.poison_cnt)


def update_agent_state(agent: BubbleStatus,
                       direction: jnp.int32) -> BubbleStatus:
    vel_x = agent.vel_x
    vel_x = jnp.where(direction == ACT_RIGHT, vel_x + 1, vel_x)
    vel_x = jnp.where(direction == ACT_LEFT, vel_x - 1, vel_x)
    vel_x = vel_x * 0.95

    vel_y = agent.vel_y
    vel_y = jnp.where(direction == ACT_UP, vel_y - 1, vel_y)
    vel_y = jnp.where(direction == ACT_DOWN, vel_y + 1, vel_y)
    vel_y = vel_y * 0.95

    pos_x = agent.pos_x + vel_x
    pos_y = agent.pos_y + vel_y
    # Collide with the west wall.
    vel_x = jnp.where(pos_x < 1, 0, vel_x)
    vel_y = jnp.where(pos_x < 1, 0, vel_y)
    pos_x = jnp.where(pos_x < 1, 1, pos_x)
    # Collide with the east wall.
    vel_x = jnp.where(pos_x > SCREEN_W - 1, 0, vel_x)
    vel_y = jnp.where(pos_x > SCREEN_W - 1, 0, vel_y)
    pos_x = jnp.where(pos_x > SCREEN_W - 1, SCREEN_W - 1, pos_x)
    # Collide with the north wall.
    vel_x = jnp.where(pos_y < 1, 0, vel_x)
    vel_y = jnp.where(pos_y < 1, 0, vel_y)
    pos_y = jnp.where(pos_y < 1, 1, pos_y)
    # Collide with the south wall.
    vel_x = jnp.where(pos_y > SCREEN_H - 1, 0, vel_x)
    vel_y = jnp.where(pos_y > SCREEN_H - 1, 0, vel_y)
    pos_y = jnp.where(pos_y > SCREEN_H - 1, SCREEN_H - 1, pos_y)

    return BubbleStatus(
        pos_x=pos_x, pos_y=pos_y, vel_x=vel_x, vel_y=vel_y,
        bubble_type=agent.bubble_type, valid=agent.valid,
        poison_cnt=agent.poison_cnt)


@jax.vmap
def get_line_seg_intersection(x1: jnp.float32,
                              y1: jnp.float32,
                              x2: jnp.float32,
                              y2: jnp.float32,
                              x3: jnp.float32,
                              y3: jnp.float32,
                              x4: jnp.float32,
                              y4: jnp.float32) -> Tuple[bool, jnp.ndarray]:
    """Determine if line segment (x1, y1, x2, y2) intersects with line
    segment (x3, y3, x4, y4), and return the intersection coordinate.
    """
    denominator = (y4 - y3) * (x2 - x1) - (x4 - x3) * (y2 - y1)
    ua = jnp.where(
        jnp.isclose(denominator, 0.0), 0,
        ((x4 - x3) * (y1 - y3) - (y4 - y3) * (x1 - x3)) / denominator)
    mask1 = jnp.bitwise_and(ua > 0., ua < 1.)
    ub = jnp.where(
        jnp.isclose(denominator, 0.0), 0,
        ((x2 - x1) * (y1 - y3) - (y2 - y1) * (x1 - x3)) / denominator)
    mask2 = jnp.bitwise_and(ub > 0., ub < 1.)
    intersected = jnp.bitwise_and(mask1, mask2)
    x_intersection = x1 + ua * (x2 - x1)
    y_intersection = y1 + ua * (y2 - y1)
    up = jnp.where(intersected,
                   jnp.array([x_intersection, y_intersection]),
                   jnp.array([SCREEN_W, SCREEN_W]))
    return intersected, up


@jax.vmap
def get_line_dot_intersection(x1: jnp.float32,
                              y1: jnp.float32,
                              x2: jnp.float32,
                              y2: jnp.float32,
                              x3: jnp.float32,
                              y3: jnp.float32) -> Tuple[bool, jnp.ndarray]:
    """Determine if a line segment (x1, y1, x2, y2) intersects with a dot at
    (x3, y3) with radius BUBBLE_RADIUS, if so return the point of intersection.
    """
    point_xy = jnp.array([x3, y3])
    v = jnp.array([y2 - y1, x1 - x2])
    v_len = jnp.linalg.norm(v)
    d = jnp.abs((x2 - x1) * (y1 - y3) - (x1 - x3) * (y2 - y1)) / v_len
    up = point_xy + v / v_len * d
    ua = jnp.where(jnp.abs(x2 - x1) > jnp.abs(y2 - y1),
                   (up[0] - x1) / (x2 - x1),
                   (up[1] - y1) / (y2 - y1))
    ua = jnp.where(d > BUBBLE_RADIUS, 0, ua)
    intersected = jnp.bitwise_and(ua > 0., ua < 1.)
    return intersected, up


def get_obs(agent: BubbleStatus,
            items: BubbleStatus,
            walls: jnp.ndarray) -> jnp.ndarray:
    sensor_obs = []
    agent_xy = jnp.array([agent.pos_x, agent.pos_y]).ravel()
    for i in jnp.arange(NUM_RANGE_SENSORS):
        ang = i * DELTA_ANG
        range_xy = jnp.array([agent.pos_x + MAX_RANGE * jnp.cos(ang),
                              agent.pos_y + MAX_RANGE * jnp.sin(ang)])
        # Check for intersections with the 4 walls.
        intersected_with_walls, wall_intersections = get_line_seg_intersection(
            jnp.ones(4) * agent_xy[0], jnp.ones(4) * agent_xy[1],
            jnp.ones(4) * range_xy[0], jnp.ones(4) * range_xy[1],
            walls[:, 0], walls[:, 1], walls[:, 2], walls[:, 3])
        dist_to_walls = jnp.where(
            intersected_with_walls,
            jnp.sqrt(jnp.square(wall_intersections[:, 1] - agent_xy[1]) +
                     jnp.square(wall_intersections[:, 0] - agent_xy[0])),
            MAX_RANGE,
        )
        ix_walls = jnp.argmin(dist_to_walls)
        # Check for intersections with the items.
        n_items = len(items.valid)
        intersected_with_items, item_intersections = get_line_dot_intersection(
            jnp.ones(n_items) * agent_xy[0], jnp.ones(n_items) * agent_xy[1],
            jnp.ones(n_items) * range_xy[0], jnp.ones(n_items) * range_xy[1],
            items.pos_x, items.pos_y)
        dist_to_items = jnp.where(
            jnp.bitwise_and(items.valid, intersected_with_items),
            jnp.sqrt(jnp.square(item_intersections[:, 1] - agent_xy[1]) +
                     jnp.square(item_intersections[:, 0] - agent_xy[0])),
            MAX_RANGE,
        )
        ix_items = jnp.argmin(dist_to_items)
        # Fill in the sensor data.
        detected_xy = jnp.where(
            jnp.min(dist_to_walls) < jnp.min(dist_to_items),
            jnp.where(
                intersected_with_walls[ix_walls],
                jnp.array([
                    dist_to_walls[ix_walls], MAX_RANGE, MAX_RANGE, 0, 0]),
                jnp.array([
                    MAX_RANGE, MAX_RANGE, MAX_RANGE, 0, 0])),
            jnp.where(
                intersected_with_items[ix_items],
                jnp.where(
                    items.bubble_type[ix_items] == TYPE_FOOD,
                    jnp.array([
                        MAX_RANGE, dist_to_items[ix_items], MAX_RANGE,
                        items.vel_x[ix_items], items.vel_y[ix_items]]),
                    jnp.array([
                        MAX_RANGE, MAX_RANGE, dist_to_items[ix_items],
                        items.vel_x[ix_items], items.vel_y[ix_items]]),
                ),
                jnp.array([
                    MAX_RANGE, MAX_RANGE, MAX_RANGE, 0, 0])),
            )
        sensor_obs.append(detected_xy)

    sensor_obs = jnp.stack(sensor_obs)
    sensor_obs = sensor_obs.at[:, :3].divide(MAX_RANGE)  # Normalized distances.
    vel_xy = jnp.array([agent.vel_x, agent.vel_y]).ravel()
    return jnp.concatenate([sensor_obs.ravel(), vel_xy], axis=0)


class WaterWorld(VectorizedTask):
    """Water world.
    ref: https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html
    """

    def __init__(self,
                 num_items: int = 50,
                 max_steps: int = 1000,
                 test: bool = False):

        self.max_steps = max_steps
        self.test = test
        self.obs_shape = tuple([NUM_RANGE_SENSORS * SENSOR_DATA_DIM + 2, ])
        self.act_shape = tuple([4, ])
        walls = jnp.array([[0, 0, 0, SCREEN_H],
                           [0, SCREEN_H, SCREEN_W, SCREEN_H],
                           [SCREEN_W, SCREEN_H, SCREEN_W, 0],
                           [SCREEN_W, 0, 0, 0]])

        def reset_fn(key):
            next_key, key = random.split(key)
            ks = random.split(key, 1 + num_items)
            agent = create_bubbles(ks[0][None, :], True)
            items = create_bubbles(ks[1:], False)
            obs = get_obs(agent, items, walls)
            return State(agent_state=agent, item_state=items, obs=obs,
                         steps=jnp.zeros((), dtype=jnp.int32), key=next_key)
        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            next_key, key = random.split(state.key)
            direction = random.choice(
                key, 4, (), replace=False, p=action.ravel())
            agent = update_agent_state(state.agent_state, direction)
            items = update_item_state(state.item_state)
            agent, items, reward = get_reward(agent, items)
            steps = state.steps + 1
            done = jnp.where(steps >= max_steps, 1, 0)
            obs = get_obs(agent, items, walls)
            return State(agent_state=agent, item_state=items, obs=obs,
                         steps=steps, key=next_key), reward, done
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.array) -> State:
        return self._reset_fn(key)

    def step(self,
             state: State,
             action: jnp.ndarray) -> Tuple[State, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

    @staticmethod
    def render(state: State, task_id: int = 0) -> Image:
        img = Image.new('RGB', (SCREEN_W, SCREEN_H), (255, 255, 255))
        draw = ImageDraw.Draw(img)
        state = tree_util.tree_map(lambda s: s[task_id], state)
        # Draw the agent.
        agent = state.agent_state
        x, y = agent.pos_x, agent.pos_y
        sensor_data = np.array(state.obs)[:-2].reshape(
            NUM_RANGE_SENSORS, SENSOR_DATA_DIM)
        for i, obs in enumerate(sensor_data):
            ang = i * DELTA_ANG
            dist = np.min(obs[:3])
            x_end = x + dist * MAX_RANGE * np.cos(ang)
            y_end = y + dist * MAX_RANGE * np.sin(ang)
            draw.line((x, y, x_end, y_end), fill=(0, 0, 0), width=1)
        draw.ellipse(
            (x - BUBBLE_RADIUS, y - BUBBLE_RADIUS,
             x + BUBBLE_RADIUS, y + BUBBLE_RADIUS),
            fill=(255, 255, 0), outline=(0, 0, 0))
        # Draw the items.
        items = state.item_state
        for v, t, x, y in zip(np.array(items.valid, dtype=bool),
                              np.array(items.bubble_type, dtype=int),
                              np.array(items.pos_x),
                              np.array(items.pos_y)):
            if v:
                color = (0, 255, 0) if t == TYPE_FOOD else (255, 0, 0)
                draw.ellipse(
                    (x - BUBBLE_RADIUS, y - BUBBLE_RADIUS,
                     x + BUBBLE_RADIUS, y + BUBBLE_RADIUS,),
                    fill=color, outline=(0, 0, 0))
        return img

=== ./evojax/task/base.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from abc import ABC
from abc import abstractmethod
from typing import Dict
from typing import List
from typing import Tuple
from typing import TypeVar
import dataclasses

import jax.numpy as jnp
from flax.struct import dataclass


class TaskState(ABC):
    """A template of the task state."""
    obs: jnp.ndarray


class VectorizedTask(ABC):
    """Interface for all the EvoJAX tasks."""

    max_steps: int
    obs_shape: Tuple
    act_shape: Tuple
    test: bool
    multi_agent_training: bool = False

    @abstractmethod
    def reset(self, key: jnp.array) -> TaskState:
        """This resets the vectorized task.

        Args:
            key - A jax random key.
        Returns:
            TaskState. Initial task state.
        """
        raise NotImplementedError()

    @abstractmethod
    def step(self,
             state: TaskState,
             action: jnp.ndarray) -> Tuple[TaskState, jnp.ndarray, jnp.ndarray]:
        """This steps once the simulation.

        Args:
            state - System internal states of shape (num_tasks, *).
            action - Vectorized actions of shape (num_tasks, action_size).
        Returns:
            TaskState. Task states.
            jnp.ndarray. Reward.
            jnp.ndarray. Task termination flag: 1 for done, 0 otherwise.
        """
        raise NotImplementedError()


T = TypeVar('T')


class BDExtractor(object):
    """Behavior descriptor extractor."""

    def __init__(self,
                 bd_spec: List[Tuple[str, int]],
                 bd_state_spec: List[Tuple[str, T]],
                 task_state_def: T):
        """Initialization of a behavior descriptor extractor.

        Args:
            bd_spec - A list of behavior descriptors, each of which gives the
                      name and the number of bins. E.g. [('bd1', 10), ...]
            bd_state_spec - A list of states that record rollout statistics to
                            help calculate the behavior descriptors.
                            E.g. [('bd1_stat1', jnp.int32), ...]
            task_state_def - The original task definition. BDExtractor extends
                             that definition to record data in rollouts.
        """
        self.bd_spec = bd_spec
        if bd_state_spec is None:
            self.bd_state_spec = []
        else:
            self.bd_state_spec = bd_state_spec
        self.extended_task_state = self.get_extended_task_state_def(
            task_state_def)

    def get_extended_task_state_def(self, task_state_def: T) -> T:
        """Augment the original task state definition with more entries.

        Args:
            task_state_def - This should be a flax.struct.dataclass instance.
        Returns:
            A flax.struct.dataclass type of class definition with extra entries.
        """
        if self.bd_spec is None:
            return task_state_def
        else:
            fields = []
            # Keep what is in the original task_state.
            data_fields = task_state_def.__dict__['__annotations__']
            for name, field in data_fields.items():
                fields.append((name, field))
            # Add bd fields.
            fields.extend([(x[0], jnp.int32) for x in self.bd_spec])
            # Add extra bd state fields to help the calculation.
            fields.extend(self.bd_state_spec)
            return dataclass(dataclasses.make_dataclass(
                type(task_state_def).__name__,
                fields=fields, bases=task_state_def.__bases__, init=False))

    def init_extended_state(self, task_state: TaskState) -> T:
        """Return an extended task_state that includes bd_state fields.

        Args:
            task_state - Original task state.
        Returns:
            An instance of the extended task state.
        """
        bd_fields = {x[0]: jnp.zeros((), dtype=jnp.int32) for x in self.bd_spec}
        bd_state = self.init_state(task_state)
        return self.extended_task_state(
            **bd_state,    # These keep track of the stats we need.
            **bd_fields,   # These will be updated in self.summarize.
            **task_state.__dict__)

    def init_state(self, extended_task_state: T) -> Dict[str, T]:
        """A task initializes some behavior descriptor related states here.

        Args:
            extended_task_state - An instance of the extended task state, with
                                  dummy behavior descriptor related states.
        Returns:
            A dictionary that contains the initial values for each of the
            behavior descriptor related states.
        """
        raise NotImplementedError()

    def update(self,
               extended_task_state: T,
               action: jnp.ndarray,
               reward: jnp.float32,
               done: jnp.int32) -> T:
        """Update behavior descriptor calculation states.

        Args:
            extended_task_state - An instance of extended task state.
            action - The action taken at this step.
            reward - The reward acquired at this step.
            done - The termination flag from this step.
        Returns:
            The same instance of the extended task state, but with behavior
            descriptor related states updated.
        """
        raise NotImplementedError()

    def summarize(self, extended_task_state: T) -> T:
        """Summarize the behavior descriptor related states to calculate BDs.

        Args:
            extended_task_state - An instance of the extended task state.
        Returns:
            The same instance, but with behavior descriptions calculated within.
        """
        raise NotImplementedError()

=== ./evojax/task/ma_waterworld.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of a multi-agents WaterWorld task.

Ref: https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html
"""

from typing import Tuple
from functools import partial
from PIL import Image
from PIL import ImageDraw
import numpy as np

import jax
import jax.numpy as jnp
from jax import random
from jax import tree_util
from flax.struct import dataclass

from evojax.task.base import TaskState
from evojax.task.base import VectorizedTask


SCREEN_W = 600
SCREEN_H = 600
BUBBLE_RADIUS = 5
MIN_DIST = 2 * BUBBLE_RADIUS
MAX_RANGE = 60
NUM_RANGE_SENSORS = 30
DELTA_ANG = 2 * 3.14 / NUM_RANGE_SENSORS

TYPE_VOID = 0
TYPE_WALL = 1
TYPE_FOOD = 2
TYPE_POISON = 3
TYPE_AGENT = 4
SENSOR_DATA_DIM = 6

ACT_UP = 0
ACT_DOWN = 1
ACT_LEFT = 2
ACT_RIGHT = 3


@dataclass
class BubbleStatus(object):
    pos_x: jnp.float32
    pos_y: jnp.float32
    vel_x: jnp.float32
    vel_y: jnp.float32
    bubble_type: jnp.int32
    valid: jnp.int32
    poison_cnt: jnp.int32


@dataclass
class State(TaskState):
    agent_state: BubbleStatus
    item_state: BubbleStatus
    obs: jnp.ndarray
    steps: jnp.int32
    key: jnp.ndarray


@partial(jax.vmap, in_axes=(0, None))
def create_bubbles(key: jnp.ndarray, is_agent: bool) -> BubbleStatus:
    k_pos_r, k_pos_theta, k_vel, k_bubble_type = random.split(key, 4)
    if is_agent:
        bubble_type = TYPE_AGENT
        vel_x = vel_y = 0.
        r = 270
    else:
        bubble_type = jnp.where(
            random.uniform(k_bubble_type) > 0.5, TYPE_FOOD, TYPE_POISON)
        vel_x, vel_y = random.uniform(
            k_vel, shape=(2,), minval=-2.5, maxval=2.5)
        r = random.uniform(k_pos_r, shape=(), minval=0, maxval=200)
    theta = random.uniform(k_pos_theta, shape=(), minval=0, maxval=2 * jnp.pi)
    pos_x = r * jnp.cos(theta) + SCREEN_W // 2
    pos_y = r * jnp.sin(theta) + SCREEN_H // 2
    return BubbleStatus(pos_x=pos_x, pos_y=pos_y, vel_x=vel_x, vel_y=vel_y,
                        bubble_type=bubble_type, valid=1, poison_cnt=0)


def get_reward(agent: BubbleStatus,
               items: BubbleStatus) -> Tuple[BubbleStatus,
                                             BubbleStatus,
                                             jnp.float32]:
    dist = jnp.sqrt(jnp.square(agent.pos_x - items.pos_x) +
                    jnp.square(agent.pos_y - items.pos_y))
    rewards = (jnp.where(items.bubble_type == TYPE_FOOD, 1., -1.) *
               items.valid * jnp.where(dist < MIN_DIST, 1, 0))
    poison_cnt = jnp.sum(jnp.where(rewards == -1., 1, 0)) + agent.poison_cnt
    reward = jnp.sum(rewards)
    items_valid = (dist >= MIN_DIST) * items.valid
    agent_state = BubbleStatus(
        pos_x=agent.pos_x, pos_y=agent.pos_y,
        vel_x=agent.vel_x, vel_y=agent.vel_y,
        bubble_type=agent.bubble_type,
        valid=agent.valid, poison_cnt=poison_cnt)
    items_state = BubbleStatus(
        pos_x=items.pos_x, pos_y=items.pos_y,
        vel_x=items.vel_x, vel_y=items.vel_y,
        bubble_type=items.bubble_type,
        valid=items_valid, poison_cnt=items.poison_cnt)
    return agent_state, items_state, reward


@partial(jax.vmap, in_axes=(0, None))
def get_rewards(agents: BubbleStatus,
                items: BubbleStatus) -> Tuple[BubbleStatus,
                                              BubbleStatus,
                                              jnp.ndarray]:
    return get_reward(agents, items)


@jax.vmap
def update_item_state(item: BubbleStatus) -> BubbleStatus:
    vel_x = item.vel_x
    vel_y = item.vel_y
    pos_x = item.pos_x + vel_x
    pos_y = item.pos_y + vel_y
    # Collide with the west wall.
    vel_x = jnp.where(pos_x < 1, -vel_x, vel_x)
    pos_x = jnp.where(pos_x < 1, 1, pos_x)
    # Collide with the east wall.
    vel_x = jnp.where(pos_x > SCREEN_W - 1, -vel_x, vel_x)
    pos_x = jnp.where(pos_x > SCREEN_W - 1, SCREEN_W - 1, pos_x)
    # Collide with the north wall.
    vel_y = jnp.where(pos_y < 1, -vel_y, vel_y)
    pos_y = jnp.where(pos_y < 1, 1, pos_y)
    # Collide with the south wall.
    vel_y = jnp.where(pos_y > SCREEN_H - 1, -vel_y, vel_y)
    pos_y = jnp.where(pos_y > SCREEN_H - 1, SCREEN_H - 1, pos_y)
    return BubbleStatus(
        pos_x=pos_x, pos_y=pos_y, vel_x=vel_x, vel_y=vel_y,
        bubble_type=item.bubble_type, valid=item.valid,
        poison_cnt=item.poison_cnt)


@jax.vmap
def update_agent_state(agent: BubbleStatus,
                       direction: jnp.int32) -> BubbleStatus:
    vel_x = agent.vel_x
    vel_x = jnp.where(direction == ACT_RIGHT, vel_x + 1, vel_x)
    vel_x = jnp.where(direction == ACT_LEFT, vel_x - 1, vel_x)
    vel_x = vel_x * 0.95

    vel_y = agent.vel_y
    vel_y = jnp.where(direction == ACT_UP, vel_y - 1, vel_y)
    vel_y = jnp.where(direction == ACT_DOWN, vel_y + 1, vel_y)
    vel_y = vel_y * 0.95

    pos_x = agent.pos_x + vel_x
    pos_y = agent.pos_y + vel_y
    # Collide with the west wall.
    vel_x = jnp.where(pos_x < 1, 0, vel_x)
    vel_y = jnp.where(pos_x < 1, 0, vel_y)
    pos_x = jnp.where(pos_x < 1, 1, pos_x)
    # Collide with the east wall.
    vel_x = jnp.where(pos_x > SCREEN_W - 1, 0, vel_x)
    vel_y = jnp.where(pos_x > SCREEN_W - 1, 0, vel_y)
    pos_x = jnp.where(pos_x > SCREEN_W - 1, SCREEN_W - 1, pos_x)
    # Collide with the north wall.
    vel_x = jnp.where(pos_y < 1, 0, vel_x)
    vel_y = jnp.where(pos_y < 1, 0, vel_y)
    pos_y = jnp.where(pos_y < 1, 1, pos_y)
    # Collide with the south wall.
    vel_x = jnp.where(pos_y > SCREEN_H - 1, 0, vel_x)
    vel_y = jnp.where(pos_y > SCREEN_H - 1, 0, vel_y)
    pos_y = jnp.where(pos_y > SCREEN_H - 1, SCREEN_H - 1, pos_y)

    return BubbleStatus(
        pos_x=pos_x, pos_y=pos_y, vel_x=vel_x, vel_y=vel_y,
        bubble_type=agent.bubble_type, valid=agent.valid,
        poison_cnt=agent.poison_cnt)


@jax.vmap
def get_line_seg_intersection(x1: jnp.float32,
                              y1: jnp.float32,
                              x2: jnp.float32,
                              y2: jnp.float32,
                              x3: jnp.float32,
                              y3: jnp.float32,
                              x4: jnp.float32,
                              y4: jnp.float32) -> Tuple[np.bool_, jnp.ndarray]:
    """Determine if line segment (x1, y1, x2, y2) intersects with line
    segment (x3, y3, x4, y4), and return the intersection coordinate.
    """
    denominator = (y4 - y3) * (x2 - x1) - (x4 - x3) * (y2 - y1)
    ua = jnp.where(
        jnp.isclose(denominator, 0.0), 0,
        ((x4 - x3) * (y1 - y3) - (y4 - y3) * (x1 - x3)) / denominator)
    mask1 = jnp.bitwise_and(ua > 0., ua < 1.)
    ub = jnp.where(
        jnp.isclose(denominator, 0.0), 0,
        ((x2 - x1) * (y1 - y3) - (y2 - y1) * (x1 - x3)) / denominator)
    mask2 = jnp.bitwise_and(ub > 0., ub < 1.)
    intersected = jnp.bitwise_and(mask1, mask2)
    x_intersection = x1 + ua * (x2 - x1)
    y_intersection = y1 + ua * (y2 - y1)
    up = jnp.where(intersected,
                   jnp.array([x_intersection, y_intersection]),
                   jnp.array([SCREEN_W, SCREEN_W]))
    return intersected, up


@jax.vmap
def get_line_dot_intersection(x1: jnp.float32,
                              y1: jnp.float32,
                              x2: jnp.float32,
                              y2: jnp.float32,
                              x3: jnp.float32,
                              y3: jnp.float32) -> Tuple[np.bool_, jnp.ndarray]:
    """Determine if a line segment (x1, y1, x2, y2) intersects with a dot at
    (x3, y3) with radius BUBBLE_RADIUS, if so return the point of intersection.
    """
    point_xy = jnp.array([x3, y3])
    v = jnp.array([y2 - y1, x1 - x2])
    v_len = jnp.linalg.norm(v)
    d = jnp.abs((x2 - x1) * (y1 - y3) - (x1 - x3) * (y2 - y1)) / v_len
    up = point_xy + v / v_len * d
    ua = jnp.where(jnp.abs(x2 - x1) > jnp.abs(y2 - y1),
                   (up[0] - x1) / (x2 - x1),
                   (up[1] - y1) / (y2 - y1))
    ua = jnp.where(d > BUBBLE_RADIUS, 0, ua)
    intersected = jnp.bitwise_and(ua > 0., ua < 1.)
    return intersected, up


@partial(jax.vmap, in_axes=(0, None, None, None))
def get_obs(agent: BubbleStatus,
            agents: BubbleStatus,
            items: BubbleStatus,
            walls: jnp.ndarray) -> jnp.ndarray:
    sensor_obs = []
    agent_xy = jnp.array([agent.pos_x, agent.pos_y]).ravel()
    for i in jnp.arange(NUM_RANGE_SENSORS):
        ang = i * DELTA_ANG
        range_xy = jnp.array([agent.pos_x + MAX_RANGE * jnp.cos(ang),
                              agent.pos_y + MAX_RANGE * jnp.sin(ang)])
        # Check for intersections with the 4 walls.
        intersected_with_walls, wall_intersections = get_line_seg_intersection(
            jnp.ones(4) * agent_xy[0], jnp.ones(4) * agent_xy[1],
            jnp.ones(4) * range_xy[0], jnp.ones(4) * range_xy[1],
            walls[:, 0], walls[:, 1], walls[:, 2], walls[:, 3])
        dist_to_walls = jnp.where(
            intersected_with_walls,
            jnp.sqrt(jnp.square(wall_intersections[:, 1] - agent_xy[1]) +
                     jnp.square(wall_intersections[:, 0] - agent_xy[0])),
            MAX_RANGE,
        )
        ix_walls = jnp.argmin(dist_to_walls)
        # Check for intersections with the items and agents.
        items_and_agents = tree_util.tree_map(
            lambda x, y: jnp.concatenate([x, y], axis=0),
            items, agents)
        n_items = len(items_and_agents.valid)
        intersected_with_items, item_intersections = get_line_dot_intersection(
            jnp.ones(n_items) * agent_xy[0], jnp.ones(n_items) * agent_xy[1],
            jnp.ones(n_items) * range_xy[0], jnp.ones(n_items) * range_xy[1],
            items_and_agents.pos_x, items_and_agents.pos_y)
        dist_to_item = jnp.where(
            jnp.bitwise_and(items_and_agents.valid, intersected_with_items),
            jnp.sqrt(jnp.square(item_intersections[:, 1] - agent_xy[1]) +
                     jnp.square(item_intersections[:, 0] - agent_xy[0])),
            MAX_RANGE,
        )
        dist_to_item = jnp.where(dist_to_item == 0., MAX_RANGE, dist_to_item)
        ix_items = jnp.argmin(dist_to_item)
        # Fill in the sensor data.
        detected_xy = jnp.where(
            jnp.min(dist_to_walls) < jnp.min(dist_to_item),
            jnp.where(
                intersected_with_walls[ix_walls],
                jnp.array([
                    dist_to_walls[ix_walls], MAX_RANGE, MAX_RANGE, MAX_RANGE,
                    0, 0]),
                jnp.array([
                    MAX_RANGE, MAX_RANGE, MAX_RANGE, MAX_RANGE, 0, 0])),
            jnp.where(
                intersected_with_items[ix_items],
                jnp.where(
                    items.bubble_type[ix_items] == TYPE_FOOD,
                    jnp.array([
                        MAX_RANGE, dist_to_item[ix_items], MAX_RANGE, MAX_RANGE,
                        items.vel_x[ix_items], items.vel_y[ix_items]]),
                    jnp.where(
                        items.bubble_type[ix_items] == TYPE_POISON,
                        jnp.array([
                            MAX_RANGE, MAX_RANGE, dist_to_item[ix_items],
                            MAX_RANGE,
                            items.vel_x[ix_items], items.vel_y[ix_items]]),
                        jnp.array([
                            MAX_RANGE, MAX_RANGE, MAX_RANGE,
                            dist_to_item[ix_items],
                            items.vel_x[ix_items], items.vel_y[ix_items]]),
                    ),
                ),
                jnp.array([
                    MAX_RANGE, MAX_RANGE, MAX_RANGE, MAX_RANGE, 0, 0])),
            )
        sensor_obs.append(detected_xy)

    sensor_obs = jnp.stack(sensor_obs)
    sensor_obs = sensor_obs.at[:, :4].divide(MAX_RANGE)  # Normalized distances.
    vel_xy = jnp.array([agent.vel_x, agent.vel_y]).ravel()
    return jnp.concatenate([sensor_obs.ravel(), vel_xy], axis=0)


@jax.vmap
def select_direction(key: jnp.ndarray, action_prob: jnp.ndarray) -> jnp.int32:
    return random.choice(key, 4, replace=False, p=action_prob.ravel())


class MultiAgentWaterWorld(VectorizedTask):
    """Water world, multi-agents training version."""

    def __init__(self,
                 num_agents: int = 16,
                 num_items: int = 100,
                 max_steps: int = 1000,
                 test: bool = False):

        self.multi_agent_training = True
        self.max_steps = max_steps
        self.test = test
        self.obs_shape = tuple([
            num_agents, NUM_RANGE_SENSORS * SENSOR_DATA_DIM + 2, ])
        self.act_shape = tuple([num_agents, 4])
        walls = jnp.array([[0, 0, 0, SCREEN_H],
                           [0, SCREEN_H, SCREEN_W, SCREEN_H],
                           [SCREEN_W, SCREEN_H, SCREEN_W, 0],
                           [SCREEN_W, 0, 0, 0]])

        def reset_fn(key):
            next_key, key = random.split(key)
            ks = random.split(key, num_agents + num_items)
            agents = create_bubbles(ks[:num_agents], True)
            items = create_bubbles(ks[num_agents:], False)
            obs = get_obs(agents, agents, items, walls)
            return State(agent_state=agents, item_state=items, obs=obs,
                         steps=jnp.zeros((), dtype=jnp.int32), key=next_key)
        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            next_key, key = random.split(state.key)
            action_keys = random.split(key, num_agents)
            directions = select_direction(action_keys, action)
            agents = update_agent_state(state.agent_state, directions)
            items = update_item_state(state.item_state)
            agents, items, rewards = get_rewards(agents, items)
            # items_state.shape=(num_agents, num_items), merge to (num_items, ).
            items = BubbleStatus(
                pos_x=items.pos_x[0], pos_y=items.pos_y[0],
                vel_x=items.vel_x[0], vel_y=items.vel_y[0],
                bubble_type=items.bubble_type[0],
                poison_cnt=items.poison_cnt[0],
                valid=jnp.prod(items.valid, axis=0))
            steps = state.steps + 1
            done = jnp.where(steps >= max_steps, 1, 0)
            obs = get_obs(agents, agents, items, walls)
            return State(agent_state=agents, item_state=items, obs=obs,
                         steps=steps, key=next_key), rewards, done
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.array) -> State:
        return self._reset_fn(key)

    def step(self,
             state: State,
             action: jnp.ndarray) -> Tuple[State, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

    @staticmethod
    def render(state: State, task_id: int = 0) -> Image:
        img = Image.new('RGB', (SCREEN_W, SCREEN_H), (255, 255, 255))
        draw = ImageDraw.Draw(img)
        state = tree_util.tree_map(lambda s: s[task_id], state)
        # Draw the agent.
        agents = state.agent_state
        sensor_data = np.array(state.obs[:, :-2].reshape(
            -1, NUM_RANGE_SENSORS, SENSOR_DATA_DIM))
        for i, (x, y) in enumerate(zip(agents.pos_x, agents.pos_y)):
            for j, obs in enumerate(sensor_data[i]):
                ang = j * DELTA_ANG
                dist = np.min(obs[:4])
                x_end = x + dist * MAX_RANGE * np.cos(ang)
                y_end = y + dist * MAX_RANGE * np.sin(ang)
                draw.line((x, y, x_end, y_end), fill=(0, 0, 0), width=1)
            draw.ellipse(
                (x - BUBBLE_RADIUS, y - BUBBLE_RADIUS,
                 x + BUBBLE_RADIUS, y + BUBBLE_RADIUS),
                fill=(255, 255, 0), outline=(0, 0, 0))
        # Draw the items.
        items = state.item_state
        for v, t, x, y in zip(np.array(items.valid, dtype=bool),
                              np.array(items.bubble_type, dtype=int),
                              np.array(items.pos_x),
                              np.array(items.pos_y)):
            if v:
                color = (0, 255, 0) if t == TYPE_FOOD else (255, 0, 0)
                draw.ellipse(
                    (x - BUBBLE_RADIUS, y - BUBBLE_RADIUS,
                     x + BUBBLE_RADIUS, y + BUBBLE_RADIUS,),
                    fill=color, outline=(0, 0, 0))
        return img

=== ./evojax/task/mnist.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import numpy as np
from typing import Tuple

import jax
import jax.numpy as jnp
from jax import random
from flax.struct import dataclass

from evojax.task.base import VectorizedTask
from evojax.task.base import TaskState


@dataclass
class State(TaskState):
    obs: jnp.ndarray
    labels: jnp.ndarray


def sample_batch(key: jnp.ndarray,
                 data: jnp.ndarray,
                 labels: jnp.ndarray,
                 batch_size: int) -> Tuple:
    ix = random.choice(
        key=key, a=data.shape[0], shape=(batch_size,), replace=False)
    return (jnp.take(data, indices=ix, axis=0),
            jnp.take(labels, indices=ix, axis=0))


def loss(prediction: jnp.ndarray, target: jnp.ndarray) -> jnp.float32:
    target = jax.nn.one_hot(target, 10)
    return -jnp.mean(jnp.sum(prediction * target, axis=1))


def accuracy(prediction: jnp.ndarray, target: jnp.ndarray) -> jnp.float32:
    predicted_class = jnp.argmax(prediction, axis=1)
    return jnp.mean(predicted_class == target)


class MNIST(VectorizedTask):
    """MNIST classification task."""

    def __init__(self,
                 batch_size: int = 1024,
                 test: bool = False):

        self.max_steps = 1
        self.obs_shape = tuple([28, 28, 1])
        self.act_shape = tuple([10, ])

        # Delayed importing of torchvision

        try:
            from torchvision import datasets
        except ModuleNotFoundError:
            print('You need to install torchvision for this task.')
            print('  pip install torchvision')
            sys.exit(1)

        dataset = datasets.MNIST('./data', train=not test, download=True)
        data = np.expand_dims(dataset.data.numpy() / 255., axis=-1)
        labels = dataset.targets.numpy()

        def reset_fn(key):
            if test:
                batch_data, batch_labels = data, labels
            else:
                batch_data, batch_labels = sample_batch(
                    key, data, labels, batch_size)
            return State(obs=batch_data, labels=batch_labels)
        self._reset_fn = jax.jit(jax.vmap(reset_fn))

        def step_fn(state, action):
            if test:
                reward = accuracy(action, state.labels)
            else:
                reward = -loss(action, state.labels)
            return state, reward, jnp.ones(())
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.ndarray) -> State:
        return self._reset_fn(key)

    def step(self,
             state: TaskState,
             action: jnp.ndarray) -> Tuple[TaskState, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

=== ./evojax/task/seq2seq.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""A seq2seq task (simple addition).

Ref: https://github.com/google/flax/tree/main/examples/seq2seq
"""

import numpy as np
from typing import Tuple

import jax
import jax.numpy as jnp
from jax import random
from flax.struct import dataclass

from evojax.task.base import VectorizedTask
from evojax.task.base import TaskState


@dataclass
class State(TaskState):
    obs: jnp.ndarray
    labels: jnp.ndarray


class CharacterTable(object):
    """Encode/decodes between strings and integer representations."""

    def __init__(self):
        self._chars = '0123456789+= '
        self.pad_id = len(self._chars)
        self.eos_id = self.pad_id + 1
        self.vocab_size = len(self._chars) + 2
        self._indices_char = dict(
            (idx, ch) for idx, ch in enumerate(self._chars))
        self._indices_char[self.pad_id] = '_'

    def encode(self, inputs: jnp.ndarray) -> jnp.ndarray:
        return jnp.concatenate([inputs, jnp.array([self.eos_id])])

    def decode(self, inputs):
        """Decode from list of integers to string."""
        chars = []
        for elem in inputs.tolist():
            if elem == self.eos_id:
                break
            chars.append(self._indices_char[elem])
        return ''.join(chars)


class Seq2seqTask(VectorizedTask):
    """Seq2seq task: encoder's input is "x+y", decoder's output is "=z"."""

    def __init__(self,
                 batch_size: int = 128,
                 max_len_query_digit: int = 3,
                 test: bool = False):

        char_table = CharacterTable()
        max_input_len = max_len_query_digit + 2 + 2
        max_output_len = max_len_query_digit + 3
        max_num = pow(10, max_len_query_digit)
        self.obs_shape = tuple([max_input_len, char_table.vocab_size])
        self.act_shape = tuple([max_output_len, char_table.vocab_size])
        self.max_steps = 1

        def encode_onehot(batch_inputs, max_len):
            def encode_str(s):
                tokens = char_table.encode(s)
                org_len = len(tokens)
                tokens = jnp.pad(
                    tokens, [(0, max_len - org_len)], mode='constant')
                return jax.nn.one_hot(
                    tokens, char_table.vocab_size, dtype=jnp.float32)
            return jnp.array([encode_str(inp) for inp in batch_inputs])

        def decode_onehot(batch_inputs):
            return np.array(list(map(
                lambda x: char_table.decode(x.argmax(axis=-1)), batch_inputs)))
        self.decode_embeddings = decode_onehot

        def breakdown_int(n):
            return jnp.where(
                n == 0,
                jnp.zeros(max_len_query_digit),
                jnp.array([(n // (10**i)) % 10
                           for i in range(max_len_query_digit - 1, -1, -1)]))

        def get_batch_data(key):
            keys = random.split(key, 2)
            add_op1 = random.randint(
                keys[0], minval=0, maxval=100, shape=(batch_size,))
            add_op2 = random.randint(
                keys[1], minval=0, maxval=max_num, shape=(batch_size,))
            for op1, op2 in zip(add_op1, add_op2):
                inputs = jnp.concatenate([
                    breakdown_int(op1)[1:],
                    jnp.array([10, ]),   # 10 is '+'
                    breakdown_int(op2)], axis=0)
                outputs = jnp.concatenate([
                    jnp.array([11, ]),  # 11 is '='
                    breakdown_int(op1 + op2)], axis=0)
                yield inputs, outputs

        def reset_fn(key):
            inputs, outputs = zip(*get_batch_data(key[0]))
            batch_data = encode_onehot(inputs, max_input_len)
            batch_labels = encode_onehot(outputs, max_output_len)
            return State(
                obs=jnp.repeat(batch_data[None, :], key.shape[0], axis=0),
                labels=jnp.repeat(batch_labels[None, :], key.shape[0], axis=0))
        self._reset_fn = jax.jit(reset_fn)

        def get_sequence_lengths(sequence_batch):
            # sequence_batch.shape = (batch_size, seq_length, vocab_size)
            eos_row = sequence_batch[:, :, char_table.eos_id]
            eos_idx = jnp.argmax(eos_row, axis=-1)
            return jnp.where(
                eos_row[jnp.arange(eos_row.shape[0]), eos_idx],
                eos_idx + 1, sequence_batch.shape[1])

        def mask_sequences(sequence_batch, lengths):
            return sequence_batch * (
                lengths[:, np.newaxis] >
                np.arange(sequence_batch.shape[1])[np.newaxis])

        def cross_entropy_loss(logits, labels, lengths):
            xe = jnp.sum(jax.nn.log_softmax(logits) * labels, axis=-1)
            return -jnp.mean(mask_sequences(xe, lengths))

        def step_fn(state, action):
            labels = state.labels[:, 1:]
            lengths = get_sequence_lengths(labels)
            if test:
                token_acc = jnp.argmax(action, -1) == jnp.argmax(labels, -1)
                sequence_acc = (jnp.sum(
                    mask_sequences(token_acc, lengths), axis=-1) == lengths)
                reward = jnp.mean(sequence_acc)
            else:
                reward = -cross_entropy_loss(action, labels, lengths)
            done = jnp.ones((), dtype=jnp.int32)
            return state, reward, done
        self._step_fn = jax.jit(jax.vmap(step_fn))

    def reset(self, key: jnp.array) -> State:
        return self._reset_fn(key)

    def step(self,
             state: State,
             action: jnp.ndarray) -> Tuple[State, jnp.ndarray, jnp.ndarray]:
        return self._step_fn(state, action)

=== ./evojax/trainer.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import time
from typing import Optional, Callable

import jax.numpy as jnp
import numpy as np

from evojax.task.base import VectorizedTask
from evojax.policy import PolicyNetwork
from evojax.algo import NEAlgorithm
from evojax.algo import QualityDiversityMethod
from evojax.sim_mgr import SimManager
from evojax.obs_norm import ObsNormalizer
from evojax.util import create_logger
from evojax.util import load_model
from evojax.util import save_model
from evojax.util import save_lattices


class Trainer(object):
    """A trainer that organizes the training logistics."""

    def __init__(self,
                 policy: PolicyNetwork,
                 solver: NEAlgorithm,
                 train_task: VectorizedTask,
                 test_task: VectorizedTask,
                 max_iter: int = 1000,
                 log_interval: int = 20,
                 test_interval: int = 100,
                 n_repeats: int = 1,
                 test_n_repeats: int = 1,
                 n_evaluations: int = 100,
                 seed: int = 42,
                 debug: bool = False,
                 use_for_loop: bool = False,
                 normalize_obs: bool = False,
                 model_dir: str = None,
                 log_dir: str = None,
                 logger: logging.Logger = None,
                 log_scores_fn: Optional[Callable[[int, jnp.ndarray, str], None]] = None):
        """Initialization.

        Args:
            policy - The policy network to use.
            solver - The ES algorithm for optimization.
            train_task - The task for training.
            test_task - The task for evaluation.
            max_iter - Maximum number of training iterations.
            log_interval - Interval for logging.
            test_interval - Interval for tests.
            n_repeats - Number of rollout repetitions.
            n_evaluations - Number of tests to conduct.
            seed - Random seed to use.
            debug - Whether to turn on the debug flag.
            use_for_loop - Use for loop for rollouts.
            normalize_obs - Whether to use an observation normalizer.
            model_dir - Directory to save/load model.
            log_dir - Directory to dump logs.
            logger - Logger.
            log_scores_fn - custom function to log the scores array. Expects input:
                `current_iter`: int, `scores`: jnp.ndarray, 'stage': str = "train" | "test"
        """

        if logger is None:
            self._logger = create_logger(
                name='Trainer', log_dir=log_dir, debug=debug)
        else:
            self._logger = logger

        self._log_interval = log_interval
        self._test_interval = test_interval
        self._max_iter = max_iter
        self.model_dir = model_dir
        self._log_dir = log_dir

        self._log_scores_fn = log_scores_fn or (lambda x, y, z: None)

        self._obs_normalizer = ObsNormalizer(
            obs_shape=train_task.obs_shape,
            dummy=not normalize_obs,
        )

        self.solver = solver
        self.sim_mgr = SimManager(
            n_repeats=n_repeats,
            test_n_repeats=test_n_repeats,
            pop_size=solver.pop_size,
            n_evaluations=n_evaluations,
            policy_net=policy,
            train_vec_task=train_task,
            valid_vec_task=test_task,
            seed=seed,
            obs_normalizer=self._obs_normalizer,
            use_for_loop=use_for_loop,
            logger=self._logger,
        )

    def run(self, demo_mode: bool = False) -> float:
        """Start the training / test process."""

        if self.model_dir is not None:
            params, obs_params = load_model(model_dir=self.model_dir)
            self.sim_mgr.obs_params = obs_params
            self._logger.info(
                'Loaded model parameters from {}.'.format(self.model_dir))
        else:
            params = None

        if demo_mode:
            if params is None:
                raise ValueError('No policy parameters to evaluate.')
            self._logger.info('Start to test the parameters.')
            scores = np.array(
                self.sim_mgr.eval_params(params=params, test=True)[0])
            self._logger.info(
                '[TEST] #tests={0}, max={1:.4f}, avg={2:.4f}, min={3:.4f}, '
                'std={4:.4f}'.format(scores.size, scores.max(), scores.mean(),
                                     scores.min(), scores.std()))
            return scores.mean()
        else:
            self._logger.info(
                'Start to train for {} iterations.'.format(self._max_iter))

            if params is not None:
                # Continue training from the breakpoint.
                self.solver.best_params = params

            best_score = -float('Inf')

            for i in range(self._max_iter):
                start_time = time.perf_counter()
                params = self.solver.ask()
                self._logger.debug('solver.ask time: {0:.4f}s'.format(
                    time.perf_counter() - start_time))

                start_time = time.perf_counter()
                scores, bds = self.sim_mgr.eval_params(
                    params=params, test=False)
                self._logger.debug('sim_mgr.eval_params time: {0:.4f}s'.format(
                    time.perf_counter() - start_time))

                start_time = time.perf_counter()
                if isinstance(self.solver, QualityDiversityMethod):
                    self.solver.observe_bd(bds)
                self.solver.tell(fitness=scores)
                self._logger.debug('solver.tell time: {0:.4f}s'.format(
                    time.perf_counter() - start_time))

                if i > 0 and i % self._log_interval == 0:
                    scores = np.array(scores)
                    self._logger.info(
                        'Iter={0}, size={1}, max={2:.4f}, '
                        'avg={3:.4f}, min={4:.4f}, std={5:.4f}'.format(
                            i, scores.size, scores.max(), scores.mean(),
                            scores.min(), scores.std()))
                    self._log_scores_fn(i, scores, "train")

                if i > 0 and i % self._test_interval == 0:
                    best_params = self.solver.best_params
                    test_scores, _ = self.sim_mgr.eval_params(
                        params=best_params, test=True)
                    self._logger.info(
                        '[TEST] Iter={0}, #tests={1}, max={2:.4f}, avg={3:.4f}, '
                        'min={4:.4f}, std={5:.4f}'.format(
                            i, test_scores.size, test_scores.max(),
                            test_scores.mean(), test_scores.min(),
                            test_scores.std()))
                    self._log_scores_fn(i, test_scores, "test")
                    mean_test_score = test_scores.mean()
                    save_model(
                        model_dir=self._log_dir,
                        model_name='iter_{}'.format(i),
                        params=best_params,
                        obs_params=self.sim_mgr.obs_params,
                        best=mean_test_score > best_score,
                    )
                    best_score = max(best_score, mean_test_score)

            # Test and save the final model.
            best_params = self.solver.best_params
            test_scores, _ = self.sim_mgr.eval_params(
                params=best_params, test=True)
            self._logger.info(
                '[TEST] Iter={0}, #tests={1}, max={2:.4f}, avg={3:.4f}, '
                'min={4:.4f}, std={5:.4f}'.format(
                    self._max_iter, test_scores.size, test_scores.max(),
                    test_scores.mean(), test_scores.min(), test_scores.std()))
            mean_test_score = test_scores.mean()
            save_model(
                model_dir=self._log_dir,
                model_name='final',
                params=best_params,
                obs_params=self.sim_mgr.obs_params,
                best=mean_test_score > best_score,
            )
            best_score = max(best_score, mean_test_score)
            if isinstance(self.solver, QualityDiversityMethod):
                save_lattices(
                    log_dir=self._log_dir,
                    file_name='qd_lattices',
                    fitness_lattice=self.solver.fitness_lattice,
                    params_lattice=self.solver.params_lattice,
                    occupancy_lattice=self.solver.occupancy_lattice,
                )
            self._logger.info(
                'Training done, best_score={0:.4f}'.format(best_score))

            return best_score

=== ./evojax/obs_norm.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import numpy as np
from typing import Tuple
from functools import partial

import jax
import jax.numpy as jnp


def normalize(obs: jnp.ndarray,
              obs_params: jnp.ndarray,
              obs_shape: Tuple,
              clip_value: float,
              std_min_value: float,
              std_max_value: float) -> jnp.ndarray:
    """Normalize the given observation."""

    obs_steps = obs_params[0]
    running_mean, running_var = jnp.split(obs_params[1:], 2)
    running_mean = running_mean.reshape(obs_shape)
    running_var = running_var.reshape(obs_shape)

    variance = running_var / (obs_steps + 1.0)
    variance = jnp.clip(variance, std_min_value, std_max_value)
    return jnp.clip(
        (obs - running_mean) / jnp.sqrt(variance), -clip_value, clip_value)


def update_obs_params(obs_buffer: jnp.ndarray,
                      obs_mask: jnp.ndarray,
                      obs_params: jnp.ndarray) -> jnp.ndarray:
    """Update observation normalization parameters."""

    obs_steps = obs_params[0]
    running_mean, running_var = jnp.split(obs_params[1:], 2)
    if obs_mask.ndim != obs_buffer.ndim:
        obs_mask = obs_mask.reshape(
            obs_mask.shape + (1,) * (obs_buffer.ndim - obs_mask.ndim))

    new_steps = jnp.sum(obs_mask)
    total_steps = obs_steps + new_steps

    input_to_old_mean = (obs_buffer - running_mean) * obs_mask
    mean_diff = jnp.sum(input_to_old_mean / total_steps, axis=(0, 1))
    new_mean = running_mean + mean_diff

    input_to_new_mean = (obs_buffer - new_mean) * obs_mask
    var_diff = jnp.sum(input_to_new_mean * input_to_old_mean, axis=(0, 1))
    new_var = running_var + var_diff

    return jnp.concatenate([jnp.ones(1) * total_steps, new_mean, new_var])


class ObsNormalizer(object):
    """Observation normalizer."""

    def __init__(self,
                 obs_shape: Tuple,
                 clip_value: float = 5.,
                 std_min_value: float = 1e-6,
                 std_max_value: float = 1e6,
                 dummy: bool = False):
        """Initialization.

        Args:
            obs_shape - Shape of the observations.
            std_min_value - Minimum standard deviation.
            std_max_value - Maximum standard deviation.
            dummy - Whether this is a dummy normalizer.
        """

        self._obs_shape = obs_shape
        self._obs_size = np.prod(obs_shape)
        self._std_min_value = std_min_value
        self._std_max_value = std_max_value
        self._clip_value = clip_value
        self.is_dummy = dummy

    @partial(jax.jit, static_argnums=(0,))
    def normalize_obs(self,
                      obs: jnp.ndarray,
                      obs_params: jnp.ndarray) -> jnp.ndarray:
        """Normalize the given observation.

        Args:
            obs - The observation to be normalized.
        Returns:
            Normalized observation.
        """

        if self.is_dummy:
            return obs
        else:
            return normalize(
                obs=obs,
                obs_params=obs_params,
                obs_shape=self._obs_shape,
                clip_value=self._clip_value,
                std_min_value=self._std_min_value,
                std_max_value=self._std_max_value)

    @partial(jax.jit, static_argnums=(0,))
    def update_normalization_params(self,
                                    obs_buffer: jnp.ndarray,
                                    obs_mask: jnp.ndarray,
                                    obs_params: jnp.ndarray) -> jnp.ndarray:
        """Update internal parameters."""

        if self.is_dummy:
            return jnp.zeros_like(obs_params)
        else:
            return update_obs_params(
                obs_buffer=obs_buffer,
                obs_mask=obs_mask,
                obs_params=obs_params,
            )

    @partial(jax.jit, static_argnums=(0,))
    def get_init_params(self) -> jnp.ndarray:
        return jnp.zeros(1 + self._obs_size * 2)

=== ./evojax/policy/tensorneat.py ===
import sys
import os

# Get the absolute path to the directory containing evojax and tensorneat
current_dir = os.path.dirname(os.path.abspath(__file__))
evojax_dir = os.path.dirname(os.path.dirname(current_dir))
project_root = os.path.dirname(evojax_dir)
sys.path.append(project_root)

import jax
import jax.numpy as jnp
from typing import Tuple, Optional
from evojax.policy.base import PolicyNetwork, PolicyState
from evojax.task.base import TaskState
from evojax.util import create_logger
from tensorneat.src.tensorneat.genome import DefaultGenome, BiasNode
from tensorneat.src.tensorneat.common import ACT, AGG, State


class NEATPolicy(PolicyNetwork):
    """Custom PolicyNetwork that works with TensorNEAT's networks."""

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        hidden_size: int = 20,
        max_nodes: int = 50,
        max_conns: int = 200,
        logger=None,
    ):
        super().__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.max_nodes = max_nodes
        self.max_conns = max_conns
        self._logger = logger or create_logger("NEATPolicy")

        # Create genome template
        self.genome = DefaultGenome(
            num_inputs=input_dim,
            num_outputs=output_dim,
            max_nodes=max_nodes,
            max_conns=max_conns,
            node_gene=BiasNode(
                activation_options=ACT.tanh,
                aggregation_options=AGG.sum,
            ),
            output_transform=ACT.tanh,
        )

        # Initialize state with randkey
        initial_key = jax.random.PRNGKey(0)
        self.state = State(randkey=initial_key)
        self.state = self.genome.setup(self.state)

        # Initialize nodes and connections
        init_key, new_key = jax.random.split(initial_key)
        self.nodes, self.conns = self.genome.initialize(self.state, init_key)
        self.state = State(randkey=new_key)  # Update state with new key

        # Store default params
        self.params = (self.nodes, self.conns, self.state)

    def reset(self, task_state: Optional[TaskState] = None) -> PolicyState:
        """Reset policy state."""
        batch_size = task_state.obs.shape[0] if task_state is not None else 1
        return jnp.zeros((batch_size, 1))  # Simple policy state

    @property
    def num_params(self) -> int:
        """Return total number of parameters."""
        return self.max_nodes * 3 + self.max_conns * 4  # Approximate parameter count

    def init_params(self, init_scale: float = 0.1) -> jnp.ndarray:
        """Initialize parameters."""
        # Return indices that will be used to look up actual parameters
        return jnp.zeros(1)

    def set_params(self, params):
        """Set current network parameters."""
        if isinstance(params, tuple) and len(params) == 3:
            self.nodes, self.conns, self.state = params
            self.params = params
        else:
            raise ValueError("Expected params to be tuple of (nodes, conns, state)")

    def get_actions(
        self, t_states: TaskState, params: jnp.ndarray, p_states: PolicyState
    ) -> Tuple[jnp.ndarray, PolicyState]:
        """Get actions from observations using current network."""
        obs = t_states.obs
        # Ensure obs has correct batch shape
        if len(obs.shape) == 1:
            obs = obs[None, :]  # Add batch dimension if not present
        elif len(obs.shape) > 2:
            # Reshape if needed for batch processing
            batch_size = obs.shape[0]
            obs = obs.reshape(batch_size, -1)

        # Use current parameters
        nodes, conns, neat_state = self.params

        # Transform network
        transformed = self.genome.transform(neat_state, nodes, conns)

        # Forward pass
        actions = self.genome.forward(neat_state, transformed, obs)

        return actions, p_states

=== ./evojax/policy/convnet.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
from typing import Tuple

import jax
import jax.numpy as jnp
from jax import random
from flax import linen as nn

from evojax.policy.base import PolicyNetwork
from evojax.policy.base import PolicyState
from evojax.task.base import TaskState
from evojax.util import create_logger
from evojax.util import get_params_format_fn


class CNN(nn.Module):
    """CNN for MNIST."""

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(features=8, kernel_size=(5, 5), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = nn.Conv(features=16, kernel_size=(5, 5), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = x.reshape((x.shape[0], -1))  # flatten
        x = nn.Dense(features=10)(x)
        x = nn.log_softmax(x)
        return x


class ConvNetPolicy(PolicyNetwork):
    """A convolutional neural network for the MNIST classification task."""

    def __init__(self, logger: logging.Logger = None):
        if logger is None:
            self._logger = create_logger('ConvNetPolicy')
        else:
            self._logger = logger

        model = CNN()
        params = model.init(random.PRNGKey(0), jnp.zeros([1, 28, 28, 1]))
        self.num_params, format_params_fn = get_params_format_fn(params)
        self._logger.info(
            'ConvNetPolicy.num_params = {}'.format(self.num_params))
        self._format_params_fn = jax.vmap(format_params_fn)
        self._forward_fn = jax.vmap(model.apply)

    def get_actions(self,
                    t_states: TaskState,
                    params: jnp.ndarray,
                    p_states: PolicyState) -> Tuple[jnp.ndarray, PolicyState]:
        params = self._format_params_fn(params)
        return self._forward_fn(params, t_states.obs), p_states

=== ./evojax/policy/mlp_pi.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""AttentionNeuron augmented MLP.

Ref: https://attentionneuron.github.io/
"""

import logging
from typing import Tuple
import numpy as np

import jax
import jax.numpy as jnp
from jax import random
from flax import linen as nn
from flax.struct import dataclass

from evojax.task.base import TaskState
from evojax.policy.base import PolicyNetwork
from evojax.policy.base import PolicyState
from evojax.util import create_logger
from evojax.util import get_params_format_fn


@dataclass
class State(PolicyState):
    prev_actions: jnp.ndarray
    lstm_states: Tuple[jnp.ndarray, jnp.ndarray]


def pos_table(n, dim):
    """Create a table of positional encodings."""

    def get_angle(x, h):
        return x / np.power(10000, 2 * (h // 2) / dim)

    def get_angle_vec(x):
        return [get_angle(x, j) for j in range(dim)]

    tab = np.array([get_angle_vec(i) for i in range(n)]).astype(float)
    tab[:, 0::2] = np.sin(tab[:, 0::2])
    tab[:, 1::2] = np.cos(tab[:, 1::2])
    return tab


class AttentionNeuronMLP(nn.Module):

    act_dim: int
    msg_dim: int
    hidden_dim: int
    pos_em_dim: int
    pos_em: np.ndarray

    @nn.compact
    def __call__(self, obs, prev_act, lstm_h):
        obs_dim = obs.shape[0]
        # obs.shape: (obs_dim,) to (obs_dim, 1)
        obs = jnp.expand_dims(obs, axis=-1)
        # prev_act.shape: (act_dim,) to (obs_dim, act_dim)
        prev_act = jnp.repeat(
            jnp.expand_dims(prev_act, axis=0), repeats=obs_dim, axis=0)
        # x_aug.shape: (obs_dim, act_dim + 1)
        x_aug = jnp.concatenate([obs, prev_act], axis=-1)

        # q.shape: (hidden_dim, msg_dim)
        q = nn.Dense(self.msg_dim)(self.pos_em)

        # x_key.shape: (obs_dim, pos_em_dim)
        pos_em_dim = lstm_h[0].shape[-1]
        new_lstm_h, x_key = nn.LSTMCell(features=pos_em_dim)(lstm_h, x_aug)
        # k.shape: (obs_dim, msg_dim)
        k = nn.Dense(self.msg_dim)(x_key)

        # att.shape: (hidden_dim, obs_dim)
        att = nn.tanh(jnp.matmul(q, k.T))
        # x.shape: (hidden_dim,)
        x = nn.tanh(jnp.matmul(att, obs).squeeze(-1))
        # act.shape: (act_dim,)
        act = nn.tanh(nn.Dense(self.act_dim)(x))

        return act, new_lstm_h


class PermutationInvariantPolicy(PolicyNetwork):
    """A permutation invariant model."""

    def __init__(self,
                 act_dim: int,
                 hidden_dim: int,
                 msg_dim: int = 32,
                 pos_em_dim: int = 8,
                 logger: logging.Logger = None):
        if logger is None:
            self._logger = create_logger(name='PermutationInvariantPolicy')
        else:
            self._logger = logger

        self.act_dim = act_dim
        self.pos_em_dim = pos_em_dim
        model = AttentionNeuronMLP(
            act_dim=act_dim,
            msg_dim=msg_dim,
            pos_em_dim=pos_em_dim,
            hidden_dim=hidden_dim,
            pos_em=pos_table(hidden_dim, pos_em_dim),
        )
        obs_dim = 5
        params = model.init(
            random.PRNGKey(0),
            obs=jnp.ones([obs_dim, ]),
            prev_act=jnp.zeros([act_dim, ]),
            lstm_h=(jnp.zeros([obs_dim, pos_em_dim]),
                    jnp.zeros([obs_dim, pos_em_dim])),
        )
        self.num_params, format_params_fn = get_params_format_fn(params)
        self._logger.info(
            'PermutationInvariantPolicy.num_params = {}'.format(self.num_params)
        )
        self._format_params_fn = jax.vmap(format_params_fn)
        self._forward_fn = jax.vmap(model.apply)

    def reset(self, states: TaskState) -> PolicyState:
        keys = random.split(random.PRNGKey(0), states.obs.shape[0])
        b_size, obs_dim = states.obs.shape
        prev_act = jnp.zeros([b_size, self.act_dim])
        lstm_h = (jnp.zeros([b_size, obs_dim, self.pos_em_dim]),
                  jnp.zeros([b_size, obs_dim, self.pos_em_dim]))
        return State(keys=keys, prev_actions=prev_act, lstm_states=lstm_h)

    def get_actions(self,
                    t_tasks: TaskState,
                    params: jnp.ndarray,
                    p_states: State) -> Tuple[jnp.ndarray, State]:
        params = self._format_params_fn(params)
        act, lstm_h = self._forward_fn(
            params,
            obs=t_tasks.obs,
            prev_act=p_states.prev_actions,
            lstm_h=p_states.lstm_states)
        return act, State(p_states.keys, act, lstm_h)

=== ./evojax/policy/__init__.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .base import PolicyNetwork
from .mlp import MLPPolicy
from .mlp_pi import PermutationInvariantPolicy
from .convnet import ConvNetPolicy
from .seq2seq import Seq2seqPolicy


__all__ = ['PolicyNetwork', 'MLPPolicy', 'PermutationInvariantPolicy',
           'ConvNetPolicy', 'Seq2seqPolicy']

=== ./evojax/policy/mlp.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
from typing import Sequence
from typing import Tuple

import jax
import jax.numpy as jnp
from jax import random
from flax import linen as nn

from evojax.policy.base import PolicyNetwork
from evojax.policy.base import PolicyState
from evojax.task.base import TaskState
from evojax.util import create_logger
from evojax.util import get_params_format_fn


class MLP(nn.Module):
    feat_dims: Sequence[int]
    out_dim: int
    out_fn: str

    @nn.compact
    def __call__(self, x):
        for hidden_dim in self.feat_dims:
            x = nn.tanh(nn.Dense(hidden_dim)(x))
        x = nn.Dense(self.out_dim)(x)
        if self.out_fn == 'tanh':
            x = nn.tanh(x)
        elif self.out_fn == 'softmax':
            x = nn.softmax(x, axis=-1)
        else:
            raise ValueError(
                'Unsupported output activation: {}'.format(self.out_fn))
        return x


class MLPPolicy(PolicyNetwork):
    """A general purpose multi-layer perceptron model."""

    def __init__(self,
                 input_dim: int,
                 hidden_dims: Sequence[int],
                 output_dim: int,
                 output_act_fn: str = 'tanh',
                 logger: logging.Logger = None):
        if logger is None:
            self._logger = create_logger(name='MLPPolicy')
        else:
            self._logger = logger

        model = MLP(
            feat_dims=hidden_dims, out_dim=output_dim, out_fn=output_act_fn)
        params = model.init(random.PRNGKey(0), jnp.ones([1, input_dim]))
        self.num_params, format_params_fn = get_params_format_fn(params)
        self._logger.info('MLPPolicy.num_params = {}'.format(self.num_params))
        self._format_params_fn = jax.vmap(format_params_fn)
        self._forward_fn = jax.vmap(model.apply)

    def get_actions(self,
                    t_states: TaskState,
                    params: jnp.ndarray,
                    p_states: PolicyState) -> Tuple[jnp.ndarray, PolicyState]:
        params = self._format_params_fn(params)
        return self._forward_fn(params, t_states.obs), p_states

=== ./evojax/policy/base.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from abc import ABC
from abc import abstractmethod
from typing import Tuple
import jax.numpy as jnp
import jax.random
from flax.struct import dataclass

from evojax.task.base import TaskState


@dataclass
class PolicyState(object):
    """Policy internal states."""

    keys: jnp.ndarray


class PolicyNetwork(ABC):
    """Interface for all policy networks in EvoJAX."""

    num_params: int

    def reset(self, states: TaskState) -> PolicyState:
        """Reset the policy.

        Args:
            TaskState - Initial observations.
        Returns:
            PolicyState. Policy internal states.
        """
        keys = jax.random.split(jax.random.PRNGKey(0), states.obs.shape[0])
        return PolicyState(keys=keys)

    @abstractmethod
    def get_actions(self,
                    t_states: TaskState,
                    params: jnp.ndarray,
                    p_states: PolicyState) -> Tuple[jnp.ndarray, PolicyState]:
        """Get vectorized actions.

        Args:
            t_states - Task states.
            params - A batch of parameters, shape is (num_envs, param_size).
            p_states - Policy internal states.
        Returns:
            jnp.ndarray. Vectorized actions.
            PolicyState. Internal states.
        """
        raise NotImplementedError()

=== ./evojax/policy/seq2seq.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementation of seq2seq model.

The model is based on: https://github.com/google/flax/tree/main/examples/seq2seq
"""

import logging
import numpy as np
from typing import Tuple
from typing import Any
from functools import partial

import jax
import jax.numpy as jnp
from jax import random
from flax import linen as nn

from evojax.policy.base import PolicyNetwork
from evojax.policy.base import PolicyState
from evojax.task.base import TaskState
from evojax.util import create_logger
from evojax.util import get_params_format_fn


class CharacterTable(object):
    """Encode/decodes between strings and integer representations."""

    def __init__(self):
        self._chars = '0123456789+= '
        self.pad_id = len(self._chars)
        self.eos_id = self.pad_id + 1
        self.vocab_size = len(self._chars) + 2
        self._indices_char = dict(
            (idx, ch) for idx, ch in enumerate(self._chars))
        self._indices_char[self.pad_id] = '_'

    def encode(self, inputs: jnp.ndarray) -> jnp.ndarray:
        return jnp.concatenate([inputs, jnp.array([self.eos_id])])

    def decode(self, inputs):
        """Decode from list of integers to string."""
        chars = []
        for elem in inputs.tolist():
            if elem == self.eos_id:
                break
            chars.append(self._indices_char[elem])
        return ''.join(chars)


char_table = CharacterTable()


class EncoderLSTM(nn.Module):
    """LSTM in the encoder part of the seq2seq model."""

    @partial(
        nn.transforms.scan,
        variable_broadcast='params',
        in_axes=1,
        out_axes=1,
        split_rngs={'params': False})
    @nn.compact
    def __call__(self, carry, x):
        lstm_state, is_eos = carry
        features = lstm_state[0].shape[-1]
        new_lstm_state, y = nn.LSTMCell(features)(lstm_state, x)

        # Pass forward the previous state if EOS has already been reached.
        def select_carried_state(new_state, old_state):
            return jnp.where(is_eos[:, np.newaxis], old_state, new_state)

        # LSTM state is a tuple (c, h).
        carried_lstm_state = tuple(
            select_carried_state(*s) for s in zip(new_lstm_state, lstm_state))
        # Update `is_eos`.
        is_eos = jnp.logical_or(is_eos, x[:, char_table.eos_id])
        return (carried_lstm_state, is_eos), y

    @staticmethod
    def initialize_carry(batch_size, hidden_size):
        # use dummy key since default state init fn is just zeros.
        return nn.LSTMCell(hidden_size, parent=None).initialize_carry(
            jax.random.PRNGKey(0), (batch_size, hidden_size))


class Encoder(nn.Module):
    """LSTM encoder, returning state after EOS is input."""

    hidden_size: int

    @nn.compact
    def __call__(self, inputs):
        # inputs.shape = (batch_size, seq_length, vocab_size).
        batch_size = inputs.shape[0]
        lstm = EncoderLSTM(name='encoder_lstm')
        init_lstm_state = lstm.initialize_carry(batch_size, self.hidden_size)
        init_is_eos = jnp.zeros(batch_size, dtype=np.bool_)
        init_carry = (init_lstm_state, init_is_eos)
        (final_state, _), _ = lstm(init_carry, inputs)
        return final_state


class DecoderLSTM(nn.Module):
    """LSTM in the decoder part of the seq2seq model."""

    teacher_force: bool

    @partial(
        nn.transforms.scan,
        variable_broadcast='params',
        in_axes=1,
        out_axes=1,
        split_rngs={'params': False})
    @nn.compact
    def __call__(self, carry, x):
        lstm_state, last_prediction = carry
        if not self.teacher_force:
            x = last_prediction
        features = lstm_state[0].shape[-1]
        lstm_state, y = nn.LSTMCell(features)(lstm_state, x)
        logits = nn.Dense(features=char_table.vocab_size)(y)
        predicted_token = jnp.argmax(logits, axis=-1)
        prediction = jax.nn.one_hot(
            predicted_token, char_table.vocab_size, dtype=jnp.float32)

        return (lstm_state, prediction), (logits, prediction)


class Decoder(nn.Module):
    """LSTM decoder."""

    init_state: Tuple[Any]
    teacher_force: bool

    @nn.compact
    def __call__(self, inputs):
        # inputs.shape = (seq_length, vocab_size).
        lstm = DecoderLSTM(teacher_force=self.teacher_force)
        init_carry = (self.init_state, inputs[:, 0])
        _, (logits, predictions) = lstm(init_carry, inputs)
        return logits, predictions


class Seq2seq(nn.Module):
    """Sequence-to-sequence class using encoder/decoder architecture."""

    teacher_force: bool
    hidden_size: int

    @nn.compact
    def __call__(self, encoder_inputs, decoder_inputs):
        # Encode inputs.
        init_decoder_state = Encoder(
            hidden_size=self.hidden_size)(encoder_inputs)
        # Decode outputs.
        logits, predictions = Decoder(
            init_state=init_decoder_state,
            teacher_force=self.teacher_force)(decoder_inputs[:, :-1])
        return logits, predictions


class Seq2seqPolicy(PolicyNetwork):
    """A seq2seq policy that deals with simple additions."""

    def __init__(self,
                 hidden_size: int = 256,
                 teacher_force: bool = False,
                 max_len_query_digit: int = 3,
                 logger: logging.Logger = None):
        if logger is None:
            self._logger = create_logger('Seq2seqPolicy')
        else:
            self._logger = logger

        max_input_len = max_len_query_digit + 2 + 2
        max_output_len = max_len_query_digit + 3
        encoder_shape = jnp.ones(
            (1, max_input_len, char_table.vocab_size), dtype=jnp.float32)
        decoder_shape = jnp.ones(
            (1, max_output_len, char_table.vocab_size), dtype=jnp.float32)
        model = Seq2seq(hidden_size=hidden_size, teacher_force=teacher_force)
        key = random.PRNGKey(0)
        params = model.init({'params': key, 'lstm': key},
                            encoder_shape, decoder_shape)['params']
        self.num_params, format_params_fn = get_params_format_fn(params)
        self._logger.info(
            'Seq2seqPolicy.num_params = {}'.format(self.num_params))
        self._format_params_fn = jax.vmap(format_params_fn)

        def forward_fn(p, o):
            x = jax.nn.one_hot(
                char_table.encode(jnp.array([11]))[0:1], char_table.vocab_size,
                dtype=jnp.float32)
            x = jnp.tile(x, (o.shape[0], max_output_len, 1))
            logits, predictions = model.apply({'params': p}, o, x)
            return logits
        self._forward_fn = jax.vmap(forward_fn)

    def get_actions(self,
                    t_states: TaskState,
                    params: jnp.ndarray,
                    p_states: PolicyState) -> Tuple[jnp.ndarray, PolicyState]:
        params = self._format_params_fn(params)
        return self._forward_fn(params, t_states.obs), p_states

=== ./README_DEVLEOPMENT.md ===
# EvoJAX Development Workflow

The EvoJAX development workflow is centralized on github repo, in the form of adding contribution through PR (Pull Request) or direct commitment to the repo. This part is not covered in this doc. Refer to Github docs [about pull request reviews](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/about-pull-request-reviews) for more information.
Also, we configure `CI/CD` as Github workflows [here](.github/workflows).

When we are doing a simple updating (e.g. simply updating `README.md` or adding a new example notebook)
- Simply commit and push.
- `[CI/CD Behavior]` When there is a push to **any** branch, CI/CD invokes `flake8` (for linting) and `pytest` (for testing, test cases under `./test`)

When we incorporated the contributions:
  - Bump version and commit: 
    - Edit the version specified on `evojax/version.py`. We follow the Package version schema in PEP 440, where as a minor we update the last digit, e.g. `0.2.0` -> `0.2.1`. 
    - Commit this change, e.g. `git commit -m <CommitMessage>` and push (don’t forget!) `git push origin`.
  - Add a git (lightweight) tag to the corresponding commit and push the tag to GitHub.
    - Pull the remote tags back to local git, e.g. git pull --tags just to ensure.
    - Ensure we are on the right commit.
    - Make a local git tag and push it to the remote. e.g. `git tag v0.2.1` and `git push origin --tags`. 
    - `[CI/CD Behavior]` When a tag is pushed, CI/CD will build the package and upload it to TestPyPI.
    - `[CI/CD Artifact]` we can test TestPyPI package with `pip install --index-url https://test.pypi.org/simple/ --no-deps --upgrade evojax`
  - Since we already have a tag, we create a "github release" corresponding to this tag on Github. 
    - `[CI/CD Behavior]` When a release is created, CI will build the package and upload it to PyPI.
    - `[CI/CD Artifact]` We can use the (public, official) PyPI package with `pip install evojax`.

=== ./tests/test_import.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


class TestClass:
    def test_one(self):
        import evojax
        assert evojax.__name__ == 'evojax'

=== ./tests/test_init_extra.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

TEST_EVOSAX = False


class TestTask:
    def test_mnist(self):
        import sys
        if sys.version_info.major == 3 and sys.version_info.minor <= 9:
            # python<=3.9, required by the optional torchvision (see https://pypi.org/project/torchvision/)
            from evojax.task.mnist import MNIST
            _ = MNIST()
            assert True

    def test_mdkp(self):
        from evojax.task.mdkp import MDKP
        _ = MDKP()
        assert True

    def test_procgen(self):
        from evojax.task.procgen_task import ProcgenTask
        _ = ProcgenTask(env_name='starpilot')
        assert True


class TestPolicy:
    pass


class TestAlgo:
    def test_cma(self):
        from evojax.algo import CMA
        _ = CMA(pop_size=16, param_size=16)
        assert True

    def test_simple_ga(self):
        from evojax.algo import SimpleGA
        _ = SimpleGA(pop_size=16, param_size=16)
        assert True

    if TEST_EVOSAX:
        def test_open_es(self):
            import sys
            if sys.version_info.major == 3 and sys.version_info.minor >= 7:
                # python>=3.7, required by the optional evosax
                from evojax.algo import OpenES
                _ = OpenES(pop_size=16, param_size=16)
                assert True

        def test_ars(self):
            import sys
            if sys.version_info.major == 3 and sys.version_info.minor >= 7:
                # python>=3.7, required by the optional evosax
                from evojax.algo import ARS
                _ = ARS(pop_size=16, param_size=16)
                assert True

        def test_iamalgam(self):
            import sys
            if sys.version_info.major == 3 and sys.version_info.minor >= 7:
                # python>=3.7, required by the optional evosax
                from evojax.algo import iAMaLGaM
                _ = iAMaLGaM(pop_size=16, param_size=16)
                assert True

=== ./tests/test_init.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


class TestTask:
    def test_cartpole(self):
        from evojax.task.cartpole import CartPoleSwingUp
        _ = CartPoleSwingUp()
        assert True

    def test_seq2seq(self):
        from evojax.task.seq2seq import Seq2seqTask
        _ = Seq2seqTask()
        assert True

    def test_waterworld(self):
        from evojax.task.waterworld import WaterWorld
        _ = WaterWorld()
        assert True

    def test_waterworld_ma(self):
        from evojax.task.ma_waterworld import MultiAgentWaterWorld
        _ = MultiAgentWaterWorld()
        assert True

    def test_flocing(self):
        from evojax.task.flocking import FlockingTask
        _ = FlockingTask()
        assert True


class TestPolicy:
    def test_seq2seq(self):
        from evojax.policy import Seq2seqPolicy
        _ = Seq2seqPolicy()
        assert True

    def test_mlp(self):
        from evojax.policy import MLPPolicy
        _ = MLPPolicy(input_dim=16, hidden_dims=(16, 16), output_dim=16)
        assert True

    def test_mlp_pi(self):
        from evojax.policy import PermutationInvariantPolicy
        _ = PermutationInvariantPolicy(act_dim=16, hidden_dim=16)
        assert True

    def test_convnet(self):
        from evojax.policy import ConvNetPolicy
        _ = ConvNetPolicy()
        assert True


class TestAlgo:
    def test_pgpe(self):
        from evojax.algo import PGPE
        _ = PGPE(pop_size=16, param_size=16)
        assert True

    def test_cma_es_jax(self):
        from evojax.algo import CMA_ES_JAX
        _ = CMA_ES_JAX(pop_size=16, param_size=16)
        assert True

=== ./tests/test_algo.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

class TestAlgo:
    def test_cma_es_jax_save_and_load_state(self):
        from evojax.algo import CMA_ES_JAX
        from jax import numpy as jnp
        solver = CMA_ES_JAX(pop_size=16, param_size=16)
        # one step
        _ = solver.ask()
        solver.tell(jnp.arange(16, dtype=jnp.float32))
        # one step
        state = solver.save_state()
        internal_state_0 = solver.state._asdict()
        # one step
        _ = solver.ask()
        solver.tell(-jnp.arange(16, dtype=jnp.float32))
        internal_state_1 = solver.state._asdict()
        solver.load_state(state)
        internal_state_2 = solver.state._asdict()

        keys = list(internal_state_0.keys())

        assert not all([jnp.all(internal_state_0[key] == internal_state_1[key]) for key in keys])
        assert all([jnp.all(internal_state_0[key] == internal_state_2[key]) for key in keys])

=== ./README.md ===
# EvoJAX: Hardware-Accelerated Neuroevolution

EvoJAX is a scalable, general purpose, hardware-accelerated [neuroevolution](https://en.wikipedia.org/wiki/Neuroevolution) toolkit. Built on top of the JAX library, this toolkit enables neuroevolution algorithms to work with neural networks running in parallel across multiple TPU/GPUs. EvoJAX achieves very high performance by implementing the evolution algorithm, neural network and task all in NumPy, which is compiled just-in-time to run on accelerators.

This repo also includes several extensible examples of EvoJAX for a wide range of tasks, including supervised learning, reinforcement learning and generative art, demonstrating how EvoJAX can run your evolution experiments within minutes on a single accelerator, compared to hours or days when using CPUs.

EvoJAX paper: https://arxiv.org/abs/2202.05008 (presentation [video](https://youtu.be/TMkft3wWpb8))

Please use this BibTeX if you wish to cite this project in your publications:

```
@article{evojax2022,
  title={EvoJAX: Hardware-Accelerated Neuroevolution},
  author={Tang, Yujin and Tian, Yingtao and Ha, David},
  journal={arXiv preprint arXiv:2202.05008},
  year={2022}
}
```

List of publications using EvoJAX (please open a PR to add missing entries):

- [Modern Evolution Strategies for Creativity: Fitting Concrete Images and Abstract Concepts](https://es-clip.github.io/) (NeurIPS Creativity Workshop 2021, EvoMUSART 2022)

## Installation

EvoJAX is implemented in [JAX](https://github.com/google/jax) which needs to be installed first.

**Install JAX**: 
Please first follow JAX's [installation instruction](https://github.com/google/jax#installation) with optional GPU/TPU backend support.
In case JAX is not set up, EvoJAX installation will still try pulling a CPU-only version of JAX.
Note that Colab runtimes come with JAX pre-installed.


**Install EvoJAX**:
```shell
# Install from PyPI.
pip install evojax

# Or, install from our GitHub repo.
pip install git+https://github.com/google/evojax.git@main
```

If you also want to install the extra dependencies required for certain optional functionalities, use
```shell
pip install evojax[extra]
# Or
pip install git+https://github.com/google/evojax.git@main#egg=evojax[extra]
```

## Code Overview

EvoJAX is a framework with three major components, which we expect the users to extend.
1. **Neuroevolution Algorithms** All neuroevolution algorithms should implement the `evojax.algo.base.NEAlgorithm` interface and reside in `evojax/algo/`.
See [here](https://github.com/google/evojax/blob/main/evojax/algo/README.md) for the available algorithms in EvoJAX.
2. **Policy Networks** All neural networks should implement the `evojax.policy.base.PolicyNetwork` interface and be saved in `evojax/policy/`.
In this repo, we give example implementations of the MLP, ConvNet, Seq2Seq and [PermutationInvariant](https://attentionneuron.github.io/) models.
3. **Tasks** All tasks should implement `evojax.task.base.VectorizedTask` and be in `evojax/task/`.

These components can be used either independently, or orchestrated by `evojax.trainer` and `evojax.sim_mgr` that manage the training pipeline.
While they should be sufficient for the currently provided policies and tasks, we plan to extend their functionality in the future as the need arises.

## Examples

As a quickstart, we provide non-trivial examples (scripts in `examples/` and notebooks in `examples/notebooks`) to illustrate the usage of EvoJAX.
We provide example commands to start the training process at the top of each script.
These scripts and notebooks are run with TPUs and/or NVIDIA V100 GPU(s):

### Supervised Learning Tasks

*While one would obviously use gradient-descent for such tasks in practice, the point is to show that neuroevolution can also solve them to some degree of accuracy within a short amount of time, which will be useful when these models are adapted within a more complicated task where gradient-based approaches may not work.*

<img width="100%" src="img/evojax_supervised.png"></img>

* [MNIST Classification](https://github.com/google/evojax/blob/main/examples/train_mnist.py) -
We show that EvoJAX trains a ConvNet policy to achieve >98% test accuracy within 5 min on a single GPU.
* [Seq2Seq Learning](https://github.com/google/evojax/blob/main/examples/train_seq2seq.py) -
We demonstrate that EvoJAX is capable of learning a large network with hundreds of thousands parameters to accomplish a seq2seq task.

### Classic Control Tasks

*The purpose of including control tasks are two-fold: 1) Unlike supervised learning tasks, control tasks in EvoJAX have undetermined number of steps, we thus use these examples to demonstrate the efficiency of our task roll-out loops. 2) We wish to show the speed-up benefit of implementing tasks in JAX and illustrate how to implement one from scratch.*

<img width="100%" src="img/evojax_control.png"></img>

* [Locomotion](https://github.com/google/evojax/blob/main/examples/notebooks/BraxTasks.ipynb) -
[Brax](https://github.com/google/brax) is a differentiable physics engine implemented in JAX.
We wrap it as a task and train with EvoJAX on GPUs/TPUs. It takes EvoJAX tens of minutes to solve a locomotion task in Brax.
* [Cart-Pole Swing Up](https://github.com/google/evojax/blob/main/examples/train_cartpole.py) -
We illustrate how the classic control task can be implemented in JAX and be integrated into EvoJAX's pipeline for significant speed up training.

### Novel Tasks

*In this last category, we go beyond simple illustrations and show examples of novel tasks that are more practical and attractive to researchers in the genetic and evolutionary computation area, with the goal of helping them try out ideas in EvoJAX.*

<table width="100%">
  <tr>
    <td width="30%">
      <img width="100%" src="https://media.giphy.com/media/TG05TWWrDAxPoqKG1s/giphy.gif"></img>
    </td>
    <td width="30%">
      <img width="100%" src="https://media.giphy.com/media/zxSBpuaXdaxIIFbDI4/giphy.gif"></img>
    </td>
    <td width="40%">
      <img width="100%" src="https://media.giphy.com/media/aPiwuFjx9fKeHyIFBH/giphy.gif"></img>
    </td>
  </tr>
  <tr>
    <td>
      Multi-agent WaterWorld
    </td>
    <td>
      ES-CLIP: <i>“A drawing of a cat”</i>
    </td>
    <td>
      Slime Volleyball
    </td>
  </tr>
</table>

* [WaterWorld](https://github.com/google/evojax/blob/main/examples/train_waterworld.py) -
In this [task](https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html), an agent tries to get as much food as possible while avoiding poisons.
EvoJAX is able to train the agent in tens of minutes on a single GPU.
Moreover, we demonstrate that [multi-agents training](https://github.com/google/evojax/blob/main/examples/train_waterworld_ma.py) in EvoJAX is possible, which is beneficial for learning policies that can deal with environmental complexity and uncertainties.
* [Abstract Paintings](https://es-clip.github.io/) ([notebook 1](https://github.com/google/evojax/blob/main/examples/notebooks/AbstractPainting01.ipynb) and [notebook 2](https://github.com/google/evojax/blob/main/examples/notebooks/AbstractPainting02.ipynb)) -
We reproduce the results from this [computational creativity work](https://es-clip.github.io/) and show how the original work, whose implementation requires multiple CPUs and GPUs, could be accelerated on a single GPU efficiently using EvoJAX, which was not possible before.
Moreover, with multiple GPUs/TPUs, EvoJAX can further speed up the mentioned work almost linearly.
We also show that the modular design of EvoJAX allows its components to be used independently -- in this case it is possible to use only the ES algorithms from EvoJAX while leveraging one's own training loops and environment implantation.
* [Neural Slime Volleyball](https://github.com/google/evojax/blob/main/examples/train_slimevolley.py) -
In this [task](https://otoro.net/slimevolley/), the agent's goal is to get the ball to land on the ground of its opponent's side, causing its opponent to lose a life. The episode ends when either agent loses all five lives, or after the time limit. An agent receives a reward of +1 when its opponent loses or -1 when it loses a life.
EvoJAX is able to train the agent in under 5 minutes on a single GPU, compared hours on multiple CPUs.
This implementation is based on the [Slime Volleyball Gym Environment](https://github.com/hardmaru/slimevolleygym), which is a Python port of the original JavaScript version of the [game](https://otoro.net/slimevolley/) that you can play in the web browser. In all of these versions, the built-in AI opponent and the less-than-ideal physics are identical.

## Call for Contributions

The goal of EvoJAX is to get evolutionary computation to able to work on a vast array of tasks using accelerators.

One issue before was that many evolution algorithms were only optimized for one particular task for some paper. This is the reason we focused only on one single algorithm (PGPE) in the first release of EvoJAX, while creating 6+ different tasks in diverse domains, ensuring that one single algorithm works for all of the tasks without any issues. See [Table](https://github.com/google/evojax/blob/main/evojax/algo/README.md) of contributed algorithms.

### Evolutionary Algorithms

We welcome new evolution algorithms to be added to this toolkit. It would be great if you can show that your implementation can perform on cart-pole swing-up (hardmode), BRAX, waterworld, and MNIST, before submitting a pull request.

Ideas for evolutionary algorithm candidates:

- Your favorite Genetic Algorithm.
- [CMA-ES](https://en.wikipedia.org/wiki/CMA-ES) (bare version, and improved versions such as [BIPOP-CMA-ES](https://hal.inria.fr/hal-00818596v1/document))
- Augmented Random Search ([paper](https://arxiv.org/abs/1803.07055))
- AMaLGaM-IDEA ([paper](https://homepages.cwi.nl/~bosman/publications/2013_benchmarkingparameterfree.pdf))

We suggest the below performance guidelines for new algorithms:

1. MNIST: 90%+
2. Cartpole: 900+ (easy), 600+ (hard)
3. Waterworld: 6+ (single-agent), 2+ (multiiagent)
4. Brax ant: 3000+

Note that these are not hard requirements, but just rough guidelines.

Please use the [benchmark script](https://github.com/google/evojax/tree/main/scripts/benchmarks) to evaluate your algorithm before sending us a PR, let us know if you are unable to test on some tasks due to hardware limitations.
See this [example](https://github.com/google/evojax/pull/5#issuecomment-1043879609) pull request thread of a Genetic Algorithm that has been merged into EvoJAX to see how it should be done. 

Feel free to reach out to evojax-dev@google.com or evojax-dev@googlegroups.com if you wish to discuss further.

### New Tasks

We also welcome new tasks and examples (see [here](https://github.com/google/evojax/tree/main/evojax/task) for all tasks in EvoJAX). Some suggestions:

- Train a [Neural Turing Machine](https://en.wikipedia.org/wiki/Neural_Turing_machine) using evolution to come up with a sorting algorithm.
- Soccer via self-play ([Example](https://mobile.aau.at/~welmenre/papers/fehervari-2010-Evolving_Neural_Network_Controllers_for_a_Team_of_Self-organizing_Robots.pdf))
- Evolving Hebbian Learning-capable plastic networks that can remember the map of a maze from the agent’s recent experience.
- [Adaptive Computation Time for RNNs](https://arxiv.org/abs/1603.08983) performing a task that requires an unknown number of steps.
- Tasks that make use of hard attention.

## Sister Projects

There is a growing number of researchers working with evolutionary computation who are using JAX. Here is a list of related efforts:

- QDax: Accelerated Quality-Diversity. A tool that uses JAX to help accelerate Quality-Diveristy (QD) algorithms through hardware accelerators and massive parallelism. ([GitHub](https://github.com/adaptive-intelligent-robotics/QDax) | [paper](https://arxiv.org/abs/2202.01258))

- evosax: A JAX-based library of evolution strategies focusing on JAX-composable ask-tell functionality and strategy diversity. More than 10 ES algorithms implemented. ([GitHub](https://github.com/RobertTLange/evosax))

## Disclaimer
This is not an official Google product.

=== ./setup.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from setuptools import find_packages, setup

_dct = {}
with open("evojax/version.py") as f:
    exec(f.read(), _dct)
__version__ = _dct["__version__"]

JAX_URL = "https://storage.googleapis.com/jax-releases/jax_releases.html"

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

setup(
    name="evojax",
    version=__version__,
    author="Google",
    author_email="evojax-dev@google.com",
    description="EvoJAX: Hardware-accelerated Neuroevolution.",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/google/evojax",
    license="Apache 2.0",
    packages=[
        package for package in find_packages() if package.startswith("evojax")
    ],
    zip_safe=False,
    install_requires=[
        "flax",
        # Upgrade flax dependency after migrating RNN (a change introduced in 0.7.0):
        # https://flax.readthedocs.io/en/latest/guides/rnncell_upgrade_guide.html
        "jax>=0.2.17",
        "jaxlib>=0.1.65",
        "Pillow",
        "cma",
        "matplotlib",
        "pyyaml",
    ],
    extras_require={
        "extra": ['evosax', 'torchvision', 'pandas', 'procgen', 'brax'],
    },
    dependency_links=[JAX_URL],
    python_requires=">=3.8",
    classifiers=[
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: Apache Software License",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
    ],
)

=== ./examples/train_ant_map_elites.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train an ant locomotion controller with MAP-Elites.

To define a different BD extractor, see task/brax_task.py for example.

Example command:
python train_ant_map_elites.py --max-iter=3000
python train_ant_map_elites.py --max-iter=3000 --save-gif  # May cost some time.
"""

import argparse
import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from functools import partial

from evojax import Trainer
from evojax.task.brax_task import BraxTask
from evojax.task.brax_task import AntBDExtractor
from evojax.policy import MLPPolicy
from evojax.algo import MAPElites
from evojax import util


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pop-size', type=int, default=1024, help='NE population size.')
    parser.add_argument(
        '--num-tests', type=int, default=128, help='Number of test rollouts.')
    parser.add_argument(
        '--n-repeats', type=int, default=8, help='Training repetitions.')
    parser.add_argument(
        '--max-iter', type=int, default=300, help='Max training iterations.')
    parser.add_argument(
        '--test-interval', type=int, default=50, help='Test interval.')
    parser.add_argument(
        '--log-interval', type=int, default=10, help='Logging interval.')
    parser.add_argument(
        '--seed', type=int, default=42, help='Random seed for training.')
    parser.add_argument(
        '--iso-sigma', type=float, default=0.05, help='Iso sigma.')
    parser.add_argument(
        '--line-sigma', type=float, default=0.3, help='Line sigma.')
    parser.add_argument(
        '--gpu-id', type=str, help='GPU(s) to use.')
    parser.add_argument(
        '--debug', action='store_true', help='Debug mode.')
    parser.add_argument(
        '--save-gif', action='store_true', help='Save some GIFs.')
    config, _ = parser.parse_known_args()
    return config


def plot_figure(lattice, log_dir, title):
    grid = lattice.reshape((10, 10, 10, 10))
    fig, axes = plt.subplots(10, 10, figsize=(8, 8))
    for i in range(10):
        for j in range(10):
            ax = axes[i][j]
            ax.imshow(grid[i, j])
            ax.set_axis_off()
    fig.suptitle(title, fontsize=20, fontweight='bold')
    plt.savefig(os.path.join(log_dir, '{}.png'.format(title)))


def main(config):
    log_dir = './log/ant_map_elites'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name='AntMapElites', log_dir=log_dir, debug=config.debug)

    logger.info('EvoJAX AntMapElites Demo')
    logger.info('=' * 30)

    bd_extractor = AntBDExtractor(logger=logger)
    train_task = BraxTask(
        env_name='ant', max_steps=500, bd_extractor=bd_extractor, test=False)
    test_task = BraxTask(
        env_name='ant', bd_extractor=bd_extractor, test=True)
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[0],
        hidden_dims=[32, 32, 32, 32],
        output_dim=train_task.act_shape[0],
    )
    solver = MAPElites(
        pop_size=config.pop_size,
        param_size=policy.num_params,
        bd_extractor=bd_extractor,
        iso_sigma=config.iso_sigma,
        line_sigma=config.line_sigma,
        seed=config.seed,
        logger=logger,
    )

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=config.n_repeats,
        n_evaluations=config.num_tests,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )
    trainer.run(demo_mode=False)

    # Visualize the results.
    qd_file = os.path.join(log_dir, 'qd_lattices.npz')
    with open(qd_file, 'rb') as f:
        data = np.load(f)
        params_lattice = data['params_lattice']
        fitness_lattice = data['fitness_lattice']
        occupancy_lattice = data['occupancy_lattice']
    plot_figure(occupancy_lattice, log_dir, 'occupancy')
    plot_figure(fitness_lattice, log_dir, 'score')

    # Visualize the top policies.
    if config.save_gif:
        import jax
        import jax.numpy as jnp
        from brax import envs
        from brax.io import image

        @partial(jax.jit, static_argnums=(1,))
        def get_qp(state, ix):
            return jax.tree_map(lambda x: x[ix], state.qp)

        num_viz = 3
        idx = fitness_lattice.argsort()[-num_viz:]
        bins = [np.unravel_index(ix, (10, 10, 10, 10)) for ix in idx]
        logger.info(
            'Best {} policies: indices={}, bins={}'.format(num_viz, idx, bins))

        policy_params = jnp.array(params_lattice[idx])
        task_reset_fn = jax.jit(test_task.reset)
        policy_reset_fn = jax.jit(policy.reset)
        step_fn = jax.jit(test_task.step)
        act_fn = jax.jit(policy.get_actions)

        total_reward = jnp.zeros(num_viz)
        valid_masks = jnp.ones(num_viz)
        rollouts = {i: [] for i in range(num_viz)}
        keys = jnp.repeat(
            jax.random.PRNGKey(seed=42)[None, :], repeats=num_viz, axis=0)
        task_state = task_reset_fn(key=keys)
        policy_state = policy_reset_fn(task_state)

        for step in range(test_task.max_steps):
            for i in range(num_viz):
                rollouts[i].append(get_qp(task_state.state, i))
            act, policy_state = act_fn(task_state, policy_params, policy_state)
            task_state, reward, done = step_fn(task_state, act)
            total_reward = total_reward + reward * valid_masks
            valid_masks = valid_masks * (1 - done)
        logger.info('test_rewards={}'.format(total_reward))

        logger.info('Start saving GIFs, this can take some time ...')
        env_fn = envs.create_fn(env_name='ant', legacy_spring=True)
        env = env_fn()
        for i in range(num_viz):
            qps = jax.tree_map(lambda x: np.array(x), rollouts[i])
            frames = [
                Image.fromarray(
                    image.render_array(env.sys, qp, 320, 240, None, None, 2))
                for qp in qps]
            frames[0].save(
                os.path.join(log_dir, 'bin_{}.gif'.format(bins[i])),
                format='png',
                append_images=frames[1:],
                save_all=True,
                duration=env.sys.config.dt * 1000,
                loop=0)


if __name__ == '__main__':
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id
    main(configs)

=== ./examples/train_cartpole.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train an agent to solve the classic CartPole swing up task.

Example command to run this script:
# Train in a harder setup.
python train_cartpole.py --gpu-id=0
# Train in an easy setup.
python train_cartpole.py --gpu-id=0 --easy
# Train a permutation invariant agent in a harder setup.
python train_cartpole.py --gpu-id=0 --pi --max-iter=20000 --pop-size=256 \
--center-lr=0.037 \
--std-lr=0.078 \
--init-std=0.082
# Train a permutation invariant agent in a harder setup with CMA-ES.
python train_cartpole.py --gpu-id=0 --pi --max-iter=20000 --pop-size=256 --cma
"""

import argparse
import os
import shutil
import jax

from evojax import Trainer
from evojax.task.cartpole import CartPoleSwingUp
from evojax.policy import MLPPolicy
from evojax.policy import PermutationInvariantPolicy
from evojax.algo import PGPE
from evojax.algo import CMA
from evojax import util
from evojax.util import get_tensorboard_log_fn


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pop-size', type=int, default=64, help='NE population size.')
    parser.add_argument(
        '--hidden-size', type=int, default=64, help='Policy hidden size.')
    parser.add_argument(
        '--num-tests', type=int, default=100, help='Number of test rollouts.')
    parser.add_argument(
        '--n-repeats', type=int, default=16, help='Training repetitions.')
    parser.add_argument(
        '--max-iter', type=int, default=1000, help='Max training iterations.')
    parser.add_argument(
        '--test-interval', type=int, default=100, help='Test interval.')
    parser.add_argument(
        '--log-interval', type=int, default=20, help='Logging interval.')
    parser.add_argument(
        '--seed', type=int, default=42, help='Random seed for training.')
    parser.add_argument(
        '--center-lr', type=float, default=0.05, help='Center learning rate.')
    parser.add_argument(
        '--std-lr', type=float, default=0.1, help='Std learning rate.')
    parser.add_argument(
        '--init-std', type=float, default=0.1, help='Initial std.')
    parser.add_argument(
        '--gpu-id', type=str, help='GPU(s) to use.')
    parser.add_argument(
        '--easy', action='store_true', help='Easy mode.')
    parser.add_argument(
        '--pi', action='store_true', help='Permutation invariant policy.')
    parser.add_argument(
        '--cma', action='store_true', help='Training with CMA-ES.')
    parser.add_argument(
        '--debug', action='store_true', help='Debug mode.')
    config, _ = parser.parse_known_args()
    return config


def main(config):
    hard = not config.easy
    log_dir = './log/cartpole_{}'.format('hard' if hard else 'easy')
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name='CartPole', log_dir=log_dir, debug=config.debug)

    logger.info('EvoJAX CartPole ({}) Demo'.format('hard' if hard else 'easy'))
    logger.info('=' * 30)

    train_task = CartPoleSwingUp(test=False, harder=hard)
    test_task = CartPoleSwingUp(test=True, harder=hard)
    if config.pi:
        policy = PermutationInvariantPolicy(
            act_dim=test_task.act_shape[0],
            hidden_dim=config.hidden_size,
        )
    else:
        policy = MLPPolicy(
            input_dim=train_task.obs_shape[0],
            hidden_dims=[config.hidden_size] * 2,
            output_dim=train_task.act_shape[0],
        )
    if config.cma:
        solver = CMA(
            pop_size=config.pop_size,
            param_size=policy.num_params,
            init_stdev=config.init_std,
            seed=config.seed,
            logger=logger,
        )
    else:
        solver = PGPE(
            pop_size=config.pop_size,
            param_size=policy.num_params,
            optimizer='adam',
            center_learning_rate=config.center_lr,
            stdev_learning_rate=config.std_lr,
            init_stdev=config.init_std,
            logger=logger,
            seed=config.seed,
        )

    try:
        log_scores_fn = get_tensorboard_log_fn(log_dir=os.path.join(log_dir, "tb_logs"))
    except ImportError as e:
        logger.warning(e)

        def log_scores_fn(i, scores, stage):  # noqa
            pass

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=config.n_repeats,
        n_evaluations=config.num_tests,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
        log_scores_fn=log_scores_fn
    )
    trainer.run(demo_mode=False)

    # Test the final model.
    src_file = os.path.join(log_dir, 'best.npz')
    tar_file = os.path.join(log_dir, 'model.npz')
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    trainer.run(demo_mode=True)

    # Generate a GIF to visualize the policy.
    best_params = trainer.solver.best_params[None, :]
    task_reset_fn = jax.jit(test_task.reset)
    policy_reset_fn = jax.jit(policy.reset)
    step_fn = jax.jit(test_task.step)
    act_fn = jax.jit(policy.get_actions)
    rollout_key = jax.random.PRNGKey(seed=0)[None, :]

    images = []
    task_s = task_reset_fn(rollout_key)
    policy_s = policy_reset_fn(task_s)
    images.append(CartPoleSwingUp.render(task_s, 0))
    done = False
    step = 0
    while not done:
        act, policy_s = act_fn(task_s, best_params, policy_s)
        task_s, r, d = step_fn(task_s, act)
        step += 1
        done = bool(d[0])
        if step % 5 == 0:
            images.append(CartPoleSwingUp.render(task_s, 0))

    gif_file = os.path.join(
        log_dir, 'cartpole_{}.gif'.format('hard' if hard else 'easy'))
    images[0].save(
        gif_file, save_all=True, append_images=images[1:], duration=40, loop=0)
    logger.info('GIF saved to {}'.format(gif_file))


if __name__ == '__main__':
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id
    main(configs)

=== ./examples/train_waterworld.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train an agent to solve the WaterWorld task.

In this task, an agent (yellow) tries to catch as much food (green) as possible
while avoiding poisons (red).
This task is based on:
https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html

Example command to run this script: `python train_waterworld.py --gpu-id=0`
"""

import argparse
import os
import shutil
import jax

from evojax.task.waterworld import WaterWorld
from evojax.policy.mlp import MLPPolicy
from evojax.algo import PGPE
from evojax import Trainer
from evojax import util


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pop-size', type=int, default=256, help='ES population size.')
    parser.add_argument(
        '--hidden-size', type=int, default=100, help='Policy hidden size.')
    parser.add_argument(
        '--num-tests', type=int, default=100, help='Number of test rollouts.')
    parser.add_argument(
        '--n-repeats', type=int, default=32, help='Training repetitions.')
    parser.add_argument(
        '--max-iter', type=int, default=500, help='Max training iterations.')
    parser.add_argument(
        '--test-interval', type=int, default=50, help='Test interval.')
    parser.add_argument(
        '--log-interval', type=int, default=10, help='Logging interval.')
    parser.add_argument(
        '--seed', type=int, default=42, help='Random seed for training.')
    parser.add_argument(
        '--center-lr', type=float, default=0.014, help='Center learning rate.')
    parser.add_argument(
        '--std-lr', type=float, default=0.088, help='Std learning rate.')
    parser.add_argument(
        '--init-std', type=float, default=0.069, help='Initial std.')
    parser.add_argument(
        '--gpu-id', type=str, help='GPU(s) to use.')
    parser.add_argument(
        '--debug', action='store_true', help='Debug mode.')
    config, _ = parser.parse_known_args()
    return config


def main(config):
    log_dir = './log/water_world'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name='WaterWorld', log_dir=log_dir, debug=config.debug)
    logger.info('EvoJAX WaterWorld')
    logger.info('=' * 30)

    max_steps = 500
    train_task = WaterWorld(test=False, max_steps=max_steps)
    test_task = WaterWorld(test=True, max_steps=max_steps)
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[0],
        hidden_dims=[config.hidden_size, ],
        output_dim=train_task.act_shape[0],
        output_act_fn='softmax',
    )
    solver = PGPE(
        pop_size=config.pop_size,
        param_size=policy.num_params,
        optimizer='adam',
        center_learning_rate=config.center_lr,
        stdev_learning_rate=config.std_lr,
        init_stdev=config.init_std,
        logger=logger,
        seed=config.seed,
    )

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=config.n_repeats,
        n_evaluations=config.num_tests,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )
    trainer.run(demo_mode=False)

    # Test the final model.
    src_file = os.path.join(log_dir, 'best.npz')
    tar_file = os.path.join(log_dir, 'model.npz')
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    trainer.run(demo_mode=True)

    # Visualize the policy.
    task_reset_fn = jax.jit(test_task.reset)
    policy_reset_fn = jax.jit(policy.reset)
    step_fn = jax.jit(test_task.step)
    action_fn = jax.jit(policy.get_actions)
    best_params = trainer.solver.best_params[None, :]
    key = jax.random.PRNGKey(0)[None, :]

    task_state = task_reset_fn(key)
    policy_state = policy_reset_fn(task_state)
    screens = []
    for _ in range(max_steps):
        action, policy_state = action_fn(task_state, best_params, policy_state)
        task_state, reward, done = step_fn(task_state, action)
        screens.append(WaterWorld.render(task_state))

    gif_file = os.path.join(log_dir, 'water_world.gif')
    screens[0].save(
        gif_file, save_all=True, append_images=screens[1:], duration=40, loop=0)
    logger.info('GIF saved to {}.'.format(gif_file))


if __name__ == '__main__':
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id
    main(configs)

=== ./examples/train_slimevolleyneat.py ===
import argparse
import os
import shutil
import jax
import jax.numpy as jnp

from evojax.task.slimevolley import SlimeVolley
from evojax.policy.tensorneat import NEATPolicy
from evojax.algo.neat_wrapper import NEATWrapper
from evojax import Trainer
from evojax import util


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--pop-size", type=int, default=128, help="NEAT population size."
    )
    parser.add_argument(
        "--hidden-size", type=int, default=20, help="Initial hidden layer size."
    )
    parser.add_argument(
        "--max-nodes", type=int, default=50, help="Maximum nodes in NEAT network."
    )
    parser.add_argument(
        "--max-conns",
        type=int,
        default=200,
        help="Maximum connections in NEAT network.",
    )
    parser.add_argument(
        "--species-size", type=int, default=10, help="Number of species in NEAT."
    )
    parser.add_argument(
        "--num-tests", type=int, default=100, help="Number of test rollouts."
    )
    parser.add_argument(
        "--n-repeats", type=int, default=16, help="Training repetitions."
    )
    parser.add_argument(
        "--max-iter", type=int, default=500, help="Max training iterations."
    )
    parser.add_argument("--test-interval", type=int, default=50, help="Test interval.")
    parser.add_argument(
        "--log-interval", type=int, default=10, help="Logging interval."
    )
    parser.add_argument(
        "--seed", type=int, default=123, help="Random seed for training."
    )
    parser.add_argument("--gpu-id", type=str, help="GPU(s) to use.")
    parser.add_argument("--debug", action="store_true", help="Debug mode.")
    config, _ = parser.parse_known_args()
    return config


def main(config):
    log_dir = "./log/slimevolley"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(name="SlimeVolley", log_dir=log_dir, debug=config.debug)
    logger.info("EvoJAX SlimeVolley with NEAT")
    logger.info("=" * 30)

    max_steps = 3000
    train_task = SlimeVolley(test=False, max_steps=max_steps)
    test_task = SlimeVolley(test=True, max_steps=max_steps)

    # Create NEAT policy and solver
    policy = NEATPolicy(
        input_dim=train_task.obs_shape[0],
        output_dim=train_task.act_shape[0],
        hidden_size=config.hidden_size,
        max_nodes=config.max_nodes,
        max_conns=config.max_conns,
        logger=logger,
    )

    solver = NEATWrapper(
        input_dim=train_task.obs_shape[0],
        output_dim=train_task.act_shape[0],
        pop_size=config.pop_size,
        hidden_size=config.hidden_size,
        species_size=config.species_size,
        max_nodes=config.max_nodes,
        max_conns=config.max_conns,
        seed=config.seed,
    )

    # Create trainer
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=config.n_repeats,
        n_evaluations=config.num_tests,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )

    # Train
    trainer.run(demo_mode=False)

    # Test the final model
    src_file = os.path.join(log_dir, "best.npz")
    tar_file = os.path.join(log_dir, "model.npz")
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    trainer.run(demo_mode=True)

    # Visualize the final policy
    task_reset_fn = jax.jit(test_task.reset)
    policy_reset_fn = jax.jit(policy.reset)
    step_fn = jax.jit(test_task.step)
    action_fn = jax.jit(policy.get_actions)

    # Get best parameters and setup states
    best_params = solver.best_params
    policy.set_params(best_params)  # Make sure to set the params in policy

    key = jax.random.PRNGKey(0)
    key = jnp.expand_dims(key, 0)
    task_state = task_reset_fn(key)
    policy_state = policy_reset_fn(task_state)

    # Run visualization
    screens = []
    for _ in range(max_steps):
        action, policy_state = action_fn(
            task_state, jnp.array([0]), policy_state
        )  # Pass dummy index
        task_state, reward, done = step_fn(task_state, action)
        screens.append(SlimeVolley.render(task_state))

    # Save GIF
    gif_file = os.path.join(log_dir, "slimevolley.gif")
    screens[0].save(
        gif_file, save_all=True, append_images=screens[1:], duration=40, loop=0
    )
    logger.info("GIF saved to {}.".format(gif_file))


if __name__ == "__main__":
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = configs.gpu_id
    main(configs)

=== ./examples/train_mnist.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train an agent for MNIST classification.

Example command to run this script: `python train_mnist.py --gpu-id=0`
"""

import argparse
import os
import shutil

from evojax import Trainer
from evojax.task.mnist import MNIST
from evojax.policy.convnet import ConvNetPolicy
from evojax.algo import PGPE
from evojax import util


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pop-size', type=int, default=64, help='NE population size.')
    parser.add_argument(
        '--batch-size', type=int, default=1024, help='Batch size for training.')
    parser.add_argument(
        '--max-iter', type=int, default=5000, help='Max training iterations.')
    parser.add_argument(
        '--test-interval', type=int, default=1000, help='Test interval.')
    parser.add_argument(
        '--log-interval', type=int, default=100, help='Logging interval.')
    parser.add_argument(
        '--seed', type=int, default=42, help='Random seed for training.')
    parser.add_argument(
        '--center-lr', type=float, default=0.006, help='Center learning rate.')
    parser.add_argument(
        '--std-lr', type=float, default=0.089, help='Std learning rate.')
    parser.add_argument(
        '--init-std', type=float, default=0.039, help='Initial std.')
    parser.add_argument(
        '--gpu-id', type=str, help='GPU(s) to use.')
    parser.add_argument(
        '--debug', action='store_true', help='Debug mode.')
    config, _ = parser.parse_known_args()
    return config


def main(config):
    log_dir = './log/mnist'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name='MNIST', log_dir=log_dir, debug=config.debug)
    logger.info('EvoJAX MNIST Demo')
    logger.info('=' * 30)

    policy = ConvNetPolicy(logger=logger)
    train_task = MNIST(batch_size=config.batch_size, test=False)
    test_task = MNIST(batch_size=config.batch_size, test=True)
    solver = PGPE(
        pop_size=config.pop_size,
        param_size=policy.num_params,
        optimizer='adam',
        center_learning_rate=config.center_lr,
        stdev_learning_rate=config.std_lr,
        init_stdev=config.init_std,
        logger=logger,
        seed=config.seed,
    )

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=1,
        n_evaluations=1,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )
    trainer.run(demo_mode=False)

    # Test the final model.
    src_file = os.path.join(log_dir, 'best.npz')
    tar_file = os.path.join(log_dir, 'model.npz')
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    trainer.run(demo_mode=True)


if __name__ == '__main__':
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id
    main(configs)

=== ./examples/README.md ===
# Running examples

EvoJAX comes with an extensive set of examples and tutorials in the form of Python scripts and notebooks.  
The examples and tutorials can be run readily on a machine with accelerators (GPUs/TPUs).  
In addition, we also provide instructions to run them on Google Cloud Platform (GCP) at the end of this README.

## Tutorials

We provide three tutorials for users who are interested in extending EvoJAX:
1. [Tutorial on Neuroevolution Algorithms](https://github.com/google/evojax/blob/main/examples/notebooks/TutorialAlgorithmImplementation.ipynb) introduces the `NEAlgorithm` interface and gives examples of wrapping an existing implementation or writing a new algorithm from scratch.
2. [Tutorial on Policies](https://github.com/google/evojax/blob/main/examples/notebooks/TutorialPolicyImplementation.ipynb) explains the `PolicyNetwork` and the `PolicyState` interfaces. It also describes how a user can use them to implement a new neural network policy.
3. [Tutorial on Tasks](https://github.com/google/evojax/blob/main/examples/notebooks/TutorialTaskImplementation.ipynb) describes the `VectorizedTask` and the `TaskState` interfaces. We show how various tasks such as MNIST classification and cartpole control can be easily implemented.

## Examples

Examples are in the form of Python scripts and notebooks.  
For scripts, detailed description and example commands to run the script can be found at the top of each file.  
For notebooks, users can simply open them in Google Colab (with GPU/TPU runtime) and run the commands in each cell.  

## Running EvoJAX on GCP

We recommend running the examples on a machine with modern accelerators (e.g., a VM on GCP) for the sake of performance.  
For this purpose, we give the instructions of running EvoJAX examples on a GCP VM.  

### Setting up a GCP Project
As a pre-requisite, you need a GCP project to run the example code. Please follow the setup [instructions](https://cloud.google.com/resource-manager/docs/creating-managing-projects?ref_topic=6158848&visit_id=637860227748301614-1502365940&rd=1) if you don’t have one already.

### Create a VM
Once you have the GCP project set up, use the following commands to create a virtual machine (VM) with one NVIDIA V100 GPU
(Feel free to change the number and type of GPUs).
When the VM is created, you will be able to ssh onto the machine. Upon first login, you may be asked if you would like to install Nvidia drivers.
Type “y” and wait until the installation completes.
```shell
# Configurations.
PROJECT=“your-project-name”  # Fill in your GCP project name.
ZONE=“desired-zone-name”     # E.g., us-central1-a
VM=“desired-vm-name”         # E.g., evojax-gcp

# Create a virtual machine.
gcloud compute instances create ${VM} \
--project=${PROJECT} \
--zone=${ZONE} \
--machine-type=n1-standard-8 \
--accelerator=count=1,type=nvidia-tesla-v100 \
--create-disk=auto-delete=yes,boot=yes,device-name=${VM},image=projects/ml-images/global/images/c0-deeplearning-common-cu113-v20220316-debian-10,mode=rw,size=50,type=projects/${PROJECT}/zones/${ZONE}/diskTypes/pd-balanced \
--maintenance-policy=TERMINATE

# Login to the VM.
gcloud compute ssh ${VM} --project=${PROJECT} --zone=${ZONE}
```

### Install Python Packages and Run Examples
When the drivers are installed, run the following commands to create a working directory and install JAX (with a GPU backend) for you.
```shell
# Create a working directory and install tools.
mkdir evojax_gcp
cd evojax_gcp
python3 -m venv venv
source venv/bin/activate
pip install -U pip

# Install JAX.
pip install --upgrade "jax[cuda]" -f \ https://storage.googleapis.com/jax-releases/jax_releases.html

# Test installation, confirm the output is a GPU device.
# Sample output: "[GpuDevice(id=0, process_index=0)]"
python -c "import jax; print(jax.devices())"
```

Finally, we are ready to install EvoJAX and run example codes.
```shell
# Download and install EvoJAX.
git clone https://github.com/google/evojax.git
cd evojax
pip install -e .

# CartPole: in about 2 min, you should see the average test score reaching 600+.
python examples/train_cartpole.py --gpu-id=0

# Multi-Dimensional Knapsack Problem: you should see approximated solution in less then 1 min.
python train_mdkp.py --gpu-id=0 --item-csv={items}.csv --cap-csv={caps}.csv  # With user CSV files.
python train_mdkp.py --gpu-id=0 --use-synthesized-data                       # Or with synthesized data.

# For notebook examples, install and start a Jupyter server.
pip install jupyter
jupyter notebook
```

=== ./examples/train_mdkp.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Use GA to solve an MDKP problem.

This script shows that we can use EvoJAX to solve MDKP problems.
Please be noted that it is meant to be a demo, and may not always find an
approximated solution.

Example commands:
# Train on synthesized data.
python train_mdkp.py --gpu-id=0 --use-synthesized-data
# Train on user specified data.
python train_mdkp.py --gpu-id=0 --item-csv=my_data.csv --cap-csv=my_cap.csv
"""

import argparse
import os
import sys
import numpy as np
from typing import Tuple

import jax
import jax.numpy as jnp

from evojax import Trainer
from evojax.task.mdkp import MDKP
from evojax.task.mdkp import TaskState
from evojax.policy import PolicyNetwork
from evojax.policy.base import PolicyState
from evojax.algo import SimpleGA
from evojax import util


class MDKPPolicy(PolicyNetwork):
    """This policy ignores inputs and is specific to a certain MDKP config."""

    def __init__(self, num_items, num_bins):
        self.num_params = num_items
        thresholds = jnp.ones([num_bins + 1, num_items]) / (num_bins + 1)
        thresholds = jnp.cumsum(thresholds, axis=0)
        thresholds = thresholds - thresholds.min(axis=0)

        def forward_fn(params, obs):
            if num_bins == 1:
                diff = jax.nn.sigmoid(params)[None, :] - 0.5
            else:
                diff = jnp.clip(params - 0.04, a_min=0, a_max=1)[None, :] - thresholds
            selection = jnp.where(diff >= 0, 1, 0).sum(axis=0) - 1
            act = jnp.zeros(
                [num_bins + 1, num_items]).at[
                selection, jnp.arange(num_items)].set(1)
            return act[1:]
        self._forward_fn = jax.vmap(forward_fn)

    def get_actions(self,
                    t_states: TaskState,
                    params: jnp.ndarray,
                    p_states: PolicyState) -> Tuple[jnp.ndarray, PolicyState]:
        return self._forward_fn(params, t_states.obs), p_states


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pop-size', type=int, default=256, help='NE population size.')
    parser.add_argument(
        '--max-iter', type=int, default=2000, help='Max training iterations.')
    parser.add_argument(
        '--test-interval', type=int, default=100, help='Test interval.')
    parser.add_argument(
        '--log-interval', type=int, default=100, help='Logging interval.')
    parser.add_argument(
        '--seed', type=int, default=42, help='Random seed for training.')
    parser.add_argument(
        '--gpu-id', type=str, help='GPU(s) to use.')
    parser.add_argument(
        '--item-csv', type=str, default=None, help='Items csv file.')
    parser.add_argument(
        '--cap-csv', type=str, default=None, help='Capacities csv file.')
    parser.add_argument(
        '--use-synthesized-data', action='store_true',
        help='Use synthesized data instead of CSV files.')
    parser.add_argument(
        '--debug', action='store_true', help='Debug mode.')
    config, _ = parser.parse_known_args()
    return config


def main(config):

    try:
        import pandas as pd
    except ModuleNotFoundError:
        print('This task requires pandas,'
              'run "pip install -U pandas" to install.')
        sys.exit(1)

    log_dir = './log/mdkp'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name='MDKP', log_dir=log_dir, debug=config.debug)

    logger.info('MDKP Demo')
    logger.info('=' * 30)

    train_task = MDKP(
        items_csv=config.item_csv,
        capacity_csv=config.cap_csv,
        use_synthesized_data=config.use_synthesized_data,
        test=False)
    test_task = MDKP(
        items_csv=config.item_csv,
        capacity_csv=config.cap_csv,
        use_synthesized_data=config.use_synthesized_data,
        test=True)
    policy = MDKPPolicy(
        num_bins=train_task.act_shape[0],
        num_items=train_task.act_shape[1],
    )
    solver = SimpleGA(
        pop_size=config.pop_size,
        param_size=policy.num_params,
        logger=logger,
        seed=config.seed,
    )

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=1,
        n_evaluations=1,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )
    trainer.run(demo_mode=False)

    # Summarize the final result.
    logger.info('')
    logger.info('Summary of results')
    logger.info('=' * 30)
    logger.info('Number of attributes: {}'.format(test_task.num_attrs))
    logger.info('Number of items: {}'.format(test_task.num_items))
    logger.info('Number of bins: {}'.format(test_task.num_bins))
    logger.info('Caps: {}'.format(test_task.caps))
    best_params = trainer.solver.best_params[None, :]
    task_reset_fn = jax.jit(test_task.reset)
    policy_reset_fn = jax.jit(policy.reset)
    step_fn = jax.jit(test_task.step)
    act_fn = jax.jit(policy.get_actions)
    rollout_key = jax.random.PRNGKey(seed=0)[None, :]
    task_s = task_reset_fn(rollout_key)
    policy_s = policy_reset_fn(task_s)
    act, policy_s = act_fn(task_s, best_params, policy_s)
    task_s, reward, _ = step_fn(task_s, act)
    selections = np.array(act)[0]
    bin_assignment = (
            selections *
            (np.arange(test_task.num_bins)[:, None] + 1)).sum(axis=0)
    results = np.hstack([test_task.items, bin_assignment[:, None]])
    num_selected_items = int(selections.sum())
    if reward == 0:
        logger.info('No valid solution is found.')
    else:
        result_csv = os.path.join(log_dir, 'results.csv')
        pd.DataFrame(results).to_csv(result_csv, header=None)
        logger.info('Number of selected items: {}'.format(num_selected_items))
        logger.info('Total value: {}'.format(float(reward)))
        logger.info('Results saved to {}'.format(result_csv))


if __name__ == '__main__':
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id
    main(configs)

=== ./examples/train_seq2seq.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train an agent to solve the simple addition problem in a seq2seq setting.

In each rollout, the agent receives a token representing an addition problem,
and is required to output a token that represents the answer.
E.g., the agent may see '012+345', and then upon receiving the prompt '=', it
outputs '357'.
This task is based on:
https://github.com/google/flax/tree/main/examples/seq2seq

Example command to run this script: `python train_seq2seq.py --gpu-id=0`
"""

import argparse
import os
import shutil

from evojax import Trainer
from evojax.task.seq2seq import Seq2seqTask
from evojax.policy.seq2seq import Seq2seqPolicy
from evojax.algo import PGPE
from evojax import util


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--pop-size', type=int, default=1024, help='ES population size.')
    parser.add_argument(
        '--batch-size', type=int, default=128, help='Batch size for training.')
    parser.add_argument(
        '--hidden-size', type=int, default=128, help='Policy hidden size.')
    parser.add_argument(
        '--max-iter', type=int, default=50000, help='Max training iterations.')
    parser.add_argument(
        '--test-interval', type=int, default=1000, help='Test interval.')
    parser.add_argument(
        '--log-interval', type=int, default=100, help='Logging interval.')
    parser.add_argument(
        '--seed', type=int, default=42, help='Random seed for training.')
    parser.add_argument(
        '--center-lr', type=float, default=0.01, help='Center learning rate.')
    parser.add_argument(
        '--std-lr', type=float, default=0.03, help='Std learning rate.')
    parser.add_argument(
        '--init-std', type=float, default=0.05, help='Initial std.')
    parser.add_argument(
        '--gpu-id', type=str, help='GPU(s) to use.')
    parser.add_argument(
        '--debug', action='store_true', help='Debug mode.')
    config, _ = parser.parse_known_args()
    return config


def main(config):
    log_dir = './log/seq2seq'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name='Seq2seq', log_dir=log_dir, debug=config.debug)
    logger.info('EvoJAX Seq2seq Demo')
    logger.info('=' * 30)

    policy = Seq2seqPolicy(
        hidden_size=config.hidden_size,
        logger=logger,
    )
    train_task = Seq2seqTask(batch_size=config.batch_size, test=False)
    test_task = Seq2seqTask(batch_size=config.batch_size, test=True)
    solver = PGPE(
        pop_size=config.pop_size,
        param_size=policy.num_params,
        optimizer='adam',
        center_learning_rate=config.center_lr,
        stdev_learning_rate=config.std_lr,
        init_stdev=config.init_std,
        logger=logger,
        seed=config.seed,
    )

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=1,
        n_evaluations=100,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )
    trainer.run(demo_mode=False)

    # Test the final model.
    src_file = os.path.join(log_dir, 'best.npz')
    tar_file = os.path.join(log_dir, 'model.npz')
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    trainer.run(demo_mode=True)


if __name__ == '__main__':
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id
    main(configs)

=== ./examples/train_slimevolley.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train an agent to solve the SlimeVolley task.

Slime Volleyball is a game created in the early 2000s by unknown author.

The game is very simple: the agent's goal is to get the ball to land on
the ground of its opponent's side, causing its opponent to lose a life.

Each agent starts off with five lives. The episode ends when either agent
loses all five lives, or after 3000 timesteps has passed. An agent receives
a reward of +1 when its opponent loses or -1 when it loses a life.

An agent loses when it loses 5 times in the Test environment, or if it
loses based on score count after 3000 time steps.

During Training, the game is simply played for 3000 time steps, not
terminating even when one player loses 5 times.

This task is based on:
https://otoro.net/slimevolley/
https://github.com/hardmaru/slimevolleygym

Example command to run this script: `python train_slimevolley.py --gpu-id=0`
"""

import argparse
import os
import shutil
import jax

from evojax.task.slimevolley import SlimeVolley
from evojax.policy.mlp import MLPPolicy
from evojax.algo import CMA
from evojax import Trainer
from evojax import util


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pop-size", type=int, default=128, help="ES population size.")
    parser.add_argument(
        "--hidden-size", type=int, default=20, help="Policy hidden size."
    )
    parser.add_argument(
        "--num-tests", type=int, default=100, help="Number of test rollouts."
    )
    parser.add_argument(
        "--n-repeats", type=int, default=16, help="Training repetitions."
    )
    parser.add_argument(
        "--max-iter", type=int, default=500, help="Max training iterations."
    )
    parser.add_argument("--test-interval", type=int, default=50, help="Test interval.")
    parser.add_argument(
        "--log-interval", type=int, default=10, help="Logging interval."
    )
    parser.add_argument(
        "--seed", type=int, default=123, help="Random seed for training."
    )
    parser.add_argument("--init-std", type=float, default=0.5, help="Initial std.")
    parser.add_argument("--gpu-id", type=str, help="GPU(s) to use.")
    parser.add_argument("--debug", action="store_true", help="Debug mode.")
    config, _ = parser.parse_known_args()
    return config


def main(config):
    log_dir = "./log/slimevolley"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(name="SlimeVolley", log_dir=log_dir, debug=config.debug)
    logger.info("EvoJAX SlimeVolley")
    logger.info("=" * 30)

    max_steps = 3000
    train_task = SlimeVolley(test=False, max_steps=max_steps)
    test_task = SlimeVolley(test=True, max_steps=max_steps)
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[0],
        hidden_dims=[
            config.hidden_size,
        ],
        output_dim=train_task.act_shape[0],
        output_act_fn="tanh",
    )
    solver = CMA(
        pop_size=config.pop_size,
        param_size=policy.num_params,
        init_stdev=config.init_std,
        seed=config.seed,
        logger=logger,
    )
    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_repeats=config.n_repeats,
        n_evaluations=config.num_tests,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )
    trainer.run(demo_mode=False)

    # Test the final model.
    src_file = os.path.join(log_dir, "best.npz")
    tar_file = os.path.join(log_dir, "model.npz")
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    trainer.run(demo_mode=True)

    # Visualize the policy.
    task_reset_fn = jax.jit(test_task.reset)
    policy_reset_fn = jax.jit(policy.reset)
    step_fn = jax.jit(test_task.step)
    action_fn = jax.jit(policy.get_actions)
    best_params = trainer.solver.best_params[None, :]
    key = jax.random.PRNGKey(0)[None, :]

    task_state = task_reset_fn(key)
    policy_state = policy_reset_fn(task_state)
    screens = []
    for _ in range(max_steps):
        action, policy_state = action_fn(task_state, best_params, policy_state)
        task_state, reward, done = step_fn(task_state, action)
        screens.append(SlimeVolley.render(task_state))

    gif_file = os.path.join(log_dir, "slimevolley.gif")
    screens[0].save(
        gif_file, save_all=True, append_images=screens[1:], duration=40, loop=0
    )
    logger.info("GIF saved to {}.".format(gif_file))


if __name__ == "__main__":
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = configs.gpu_id
    main(configs)

=== ./examples/train_waterworld_ma.py ===
# Copyright 2022 The EvoJAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train a population of agents to solve the WaterWorld task.

In this task, agents (yellow) tries to catch as much food (green) as possible
while avoiding poisons (red). We wish to feature that it is possible to train
multiple agents in a single task in EvoJAX.
This task is based on:
https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html

Example command to run this script:
`python train_waterworld_ma.py --gpu-id=0 --max-iter=3000`
"""

import argparse
import os
import shutil
import jax
import jax.numpy as jnp

from evojax.task.ma_waterworld import MultiAgentWaterWorld
from evojax.policy.mlp import MLPPolicy
from evojax.algo import PGPE
from evojax import Trainer
from evojax import util


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--hidden-size', type=int, default=100, help='Policy hidden size.')
    parser.add_argument(
        '--num-tests', type=int, default=100, help='Number of test rollouts.')
    parser.add_argument(
        '--n-repeats', type=int, default=64, help='Training repetitions.')
    parser.add_argument(
        '--max-iter', type=int, default=1000, help='Max training iterations.')
    parser.add_argument(
        '--test-interval', type=int, default=100, help='Test interval.')
    parser.add_argument(
        '--log-interval', type=int, default=10, help='Logging interval.')
    parser.add_argument(
        '--seed', type=int, default=42, help='Random seed for training.')
    parser.add_argument(
        '--center-lr', type=float, default=0.011, help='Center learning rate.')
    parser.add_argument(
        '--std-lr', type=float, default=0.054, help='Std learning rate.')
    parser.add_argument(
        '--init-std', type=float, default=0.095, help='Initial std.')
    parser.add_argument(
        '--gpu-id', type=str, help='GPU(s) to use.')
    parser.add_argument(
        '--debug', action='store_true', help='Debug mode.')
    config, _ = parser.parse_known_args()
    return config


def main(config):
    log_dir = './log/water_world_ma'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name='MultiAgentWaterWorld', log_dir=log_dir, debug=config.debug)

    logger.info('EvoJAX MultiAgentWaterWorld')
    logger.info('=' * 30)

    num_agents = 16
    max_steps = 500
    train_task = MultiAgentWaterWorld(
        num_agents=num_agents, test=False, max_steps=max_steps)
    test_task = MultiAgentWaterWorld(
        num_agents=num_agents, test=True, max_steps=max_steps)
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[-1],
        hidden_dims=[config.hidden_size, ],
        output_dim=train_task.act_shape[-1],
        output_act_fn='softmax',
    )
    solver = PGPE(
        pop_size=num_agents,
        param_size=policy.num_params,
        optimizer='adam',
        center_learning_rate=config.center_lr,
        stdev_learning_rate=config.std_lr,
        init_stdev=config.init_std,
        logger=logger,
        seed=config.seed,
    )

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config.max_iter,
        log_interval=config.log_interval,
        test_interval=config.test_interval,
        n_evaluations=num_agents,
        n_repeats=config.n_repeats,
        test_n_repeats=config.num_tests,
        seed=config.seed,
        log_dir=log_dir,
        logger=logger,
    )
    trainer.run(demo_mode=False)

    # Test the final model.
    src_file = os.path.join(log_dir, 'best.npz')
    tar_file = os.path.join(log_dir, 'model.npz')
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    trainer.run(demo_mode=True)

    # Visualize the policy.
    task_reset_fn = jax.jit(test_task.reset)
    policy_reset_fn = jax.jit(policy.reset)
    step_fn = jax.jit(test_task.step)
    action_fn = jax.jit(policy.get_actions)
    best_params = jnp.repeat(
        trainer.solver.best_params[None, :], num_agents, axis=0)
    key = jax.random.PRNGKey(0)[None, :]

    task_state = task_reset_fn(key)
    policy_state = policy_reset_fn(task_state)
    screens = []
    for _ in range(max_steps):
        num_tasks, num_agents = task_state.obs.shape[:2]
        task_state = task_state.replace(
            obs=task_state.obs.reshape((-1, *task_state.obs.shape[2:])))
        action, policy_state = action_fn(task_state, best_params, policy_state)
        action = action.reshape(num_tasks, num_agents, *action.shape[1:])
        task_state = task_state.replace(
            obs=task_state.obs.reshape(
                num_tasks, num_agents, *task_state.obs.shape[1:]))
        task_state, reward, done = step_fn(task_state, action)
        screens.append(MultiAgentWaterWorld.render(task_state))

    gif_file = os.path.join(log_dir, 'water_world_ma.gif')
    screens[0].save(
        gif_file, save_all=True, append_images=screens[1:], duration=40, loop=0)
    logger.info('GIF saved to {}.'.format(gif_file))


if __name__ == '__main__':
    configs = parse_args()
    if configs.gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id
    main(configs)

=== ./scripts/benchmarks/Readme.md ===
# Utilities for Benchmarking EvoJAX Algorithms 

This repository contains benchmark results, helper scripts, ES configurations and logs for testing the performance of evolutionary strategies in [`evojax`](https://github.com/google/evojax/). These can be useful, when aiming to merge a new JAX-based ES into the project.

## Installation

```
pip install evojax pyyaml
```

## Running the Benchmarks for an Evolution Strategy

1. Fork `evojax`. 
2. Add your strategy to `algo` and the `Strategies` wrapper in the `__init__.py` file.
3. Add the base task configurations for you ES to `configs/<es>/`.
4. Execute the individual training runs for the base/default configurations via:

```
python train.py -config configs/<es>/cartpole_easy.yaml
python train.py -config configs/<es>/cartpole_hard.yaml
python train.py -config configs/<es>/waterworld.yaml
python train.py -config configs/<es>/waterworld_ma.yaml
python train.py -config configs/<es>/brax_ant.yaml
python train.py -config configs/<es>/mnist.yaml
```

5. **[Optional]**: Tune hyperparameters using [`mle-hyperopt`](https://github.com/mle-infrastructure/mle-hyperopt).

```
pip install git+https://github.com/mle-infrastructure/mle-hyperopt.git@main
```

You can then specify hyperparameter ranges and the search strategy in a yaml file as follows:

```yaml
num_iters: 25
search_type: "Grid"
maximize_objective: true
verbose: true
search_params:
  real:
    es_config/optimizer_config/lrate_init:
      begin: 0.001
      end: 0.1
      bins: 5
    es_config/init_stdev:
      begin: 0.01
      end: 0.1
      bins: 5
```

Afterwards, you can easily execute the search using the `mle-search` CLI. Here is an example for running a grid search for ARS over different learning rates and perturbation standard deviations via:

```
mle-search train.py -base configs/ARS/mnist.yaml -search configs/ARS/search.yaml -iters 25 -log log/ARS/mnist/
```

This will sequentially execute 25 ARS-MNIST evolution runs for a grid of different learning rates and standard deviations. After the search has completed, you can access the search log at `log/ARS/mnist/search_log.yaml`. Finally, we provide some [utilities](viz_grid.ipynb) to visualize the search results.

## Benchmark Results

### CMA-ES

|   | Benchmarks | Parameters | Results (Avg) |
|---|---|---|---|
CartPole (easy) | 	900 (max_iter=1000)|[Link](configs/iAMaLGaM/cartpole_easy.yaml)| 919.2294 |
CartPole (hard)	| 600 (max_iter=1000)|[Link](configs/iAMaLGaM/cartpole_hard.yaml)| 620.8969 |
Waterworld	| 6 (max_iter=1000)	 | [Link](configs/iAMaLGaM/waterworld.yaml)| 9.53 |
Waterworld (MA)	| 2 (max_iter=2000)	| [Link](configs/iAMaLGaM/waterworld_ma.yaml) | - |
MNIST	| 0.90 (max_iter=2000)	| [Link](configs/iAMaLGaM/mnist.yaml)| - |
Brax Ant |	3000 (max_iter=1200) |[Link](configs/iAMaLGaM/brax_ant.yaml)| - |

### CMA-ES

|   | Benchmarks | Parameters | Results (Avg) |
|---|---|---|---|
CartPole (easy) | 	900 (max_iter=1000)|[Link](configs/CMA_ES/cartpole_easy.yaml)| 927.3208 |
CartPole (hard)	| 600 (max_iter=1000)|[Link](configs/CMA_ES/cartpole_hard.yaml)| 625.9829 |
MNIST	| 0.90 (max_iter=2000)	| [Link](configs/CMA_ES/mnist.yaml)| 0.9581 |
Brax Ant |	3000 (max_iter=1200) |[Link](configs/CMA_ES/brax_ant.yaml)| 3174.0608 |
Waterworld	| 6 (max_iter=1000)	 | [Link](configs/CMA_ES/waterworld.yaml)| 9.44 |
Waterworld (MA)	| 2 (max_iter=2000)	| [Link](configs/CMA_ES/waterworld_ma.yaml) | 0.5625 |


### Sep-CMA-ES

|   | Benchmarks | Parameters | Results (Avg) |
|---|---|---|---|
CartPole (easy) | 	900 (max_iter=1000)|[Link](configs/Sep_CMA_ES/cartpole_easy.yaml)| 924.3028 |
CartPole (hard)	| 600 (max_iter=1000)|[Link](configs/Sep_CMA_ES/cartpole_hard.yaml)| 626.9728 |
MNIST	| 0.90 (max_iter=2000)	| [Link](configs/Sep_CMA_ES/mnist.yaml)| 0.9545 |
Brax Ant |	3000 (max_iter=1200) |[Link](configs/Sep_CMA_ES/brax_ant.yaml)| 3980.9194 |
Waterworld	| 6 (max_iter=1000)	 | [Link](configs/Sep_CMA_ES/waterworld.yaml)| 9.9000 |
Waterworld (MA)	| 2 (max_iter=2000)	| [Link](configs/Sep_CMA_ES/waterworld_ma.yaml) | 1.1875 |


### PGPE

|   | Benchmarks | Parameters | Results (Avg) |
|---|---|---|---|
CartPole (easy) | 	900 (max_iter=1000)|[Link](configs/PGPE/cartpole_easy.yaml)| 935.4268 |
CartPole (hard)	| 600 (max_iter=1000)|[Link](configs/PGPE/cartpole_hard.yaml)| 631.1020 |
MNIST	| 0.90 (max_iter=2000)	| [Link](configs/PGPE/mnist.yaml)| 0.9743 |
Brax Ant |	3000 (max_iter=1200) |[Link](configs/PGPE/brax_ant.yaml)| 6054.3887 |
Waterworld	| 6 (max_iter=1000)	 | [Link](configs/PGPE/waterworld.yaml)| 11.6400 |
Waterworld (MA)	| 2 (max_iter=2000)	| [Link](configs/PGPE/waterworld_ma.yaml) | 2.0625 |

### CMA-ES-JAX

|   | Benchmarks | Parameters | Results (Avg) |
|---|---|---|---|
CartPole (easy) | 	900 (max_iter=1000)|[Link](configs/CMA_ES_JAX/cartpole_easy.yaml) | 913.3524 |
CartPole (hard)	| 600 (max_iter=1000)|[Link](configs/CMA_ES_JAX/cartpole_hard.yaml)|  626.3537 |
MNIST	| 0.90 (max_iter=2000)	| [Link](configs/CMA_ES_JAX/mnist.yaml) | 0.9519 |
Waterworld	| 6 (max_iter=1000)	 | [Link](configs/CMA_ES_JAX/waterworld.yaml)  | 10.5100 |
Brax Ant |	3000 (max_iter=1200) | - | - |
Waterworld (MA)	| 2 (max_iter=2000)	| - | - |


### OpenES

|   | Benchmarks | Parameters | Results (Avg) |
|---|---|---|---|
CartPole (easy) | 	900 (max_iter=1000)|[Link](configs/OpenES/cartpole_easy.yaml)| 929.4153 |
CartPole (hard)	| 600 (max_iter=1000)|[Link](configs/OpenES/cartpole_hard.yaml)| 604.6940 |
MNIST	| 0.90 (max_iter=2000)	| [Link](configs/OpenES/mnist.yaml)| 0.9669 |
Brax Ant |	3000 (max_iter=1200) |[Link](configs/OpenES/brax_ant.yaml)| 6726.2100 |
Waterworld	| 6 (max_iter=1000)	 | - | - |
Waterworld (MA)	| 2 (max_iter=2000)	| - | - |


*Note*: For the brax environment I reduced the population size from 1024 to 256 and increased the search iterations by the same factor (300 to 1200) in the main run. For the grid search I used a population size of 256 but with 500 iterations.

| Cartpole-Easy  | Cartpole-Hard | MNIST | Brax|
|---|---|---|---|
<img src="figures/OpenES/cartpole_easy.png?raw=true" alt="drawing" width="200" />|<img src="figures/OpenES/cartpole_hard.png?raw=true" alt="drawing" width="200" />| <img src="figures/OpenES/mnist.png?raw=true" alt="drawing" width="200" /> | <img src="figures/OpenES/brax.png?raw=true" alt="drawing" width="200" /> |
### Augmented Random Search


|   | Benchmarks | Parameters | Results |
|---|---|---|---|
CartPole (easy) | 	900 (max_iter=1000)|[Link](configs/ARS/cartpole_easy.yaml)| 902.107 |
CartPole (hard)	| 600 (max_iter=1000)|[Link](configs/ARS/cartpole_hard.yaml)| 666.6442 |
Waterworld	| 6 (max_iter=1000)	 |[Link](configs/ARS/waterworld.yaml)| 6.1300 |
Waterworld (MA)	| 2 (max_iter=2000)	| [Link](configs/ARS/waterworld_ma.yaml)| 1.4831 |
Brax Ant |	3000 (max_iter=300) |[Link](configs/ARS/brax_ant.yaml)| 3298.9746 |
MNIST	| 0.90 (max_iter=2000)	| [Link](configs/ARS/mnist.yaml)| 0.9610 |


| Cartpole-Easy  | Cartpole-Hard | MNIST | 
|---|---|---|
<img src="figures/ARS/cartpole_easy.png?raw=true" alt="drawing" width="200" />|<img src="figures/ARS/cartpole_hard.png?raw=true" alt="drawing" width="200" />| <img src="figures/ARS/mnist.png?raw=true" alt="drawing" width="200" /> |



### Augmented Random Search (jax native)

|                          | Benchmark                   | Params   | Results (avg.) |
| ----------------|-----------------------|----------|------------- |
| CartPole (easy) | 900 (max_iter=1000)  | [Link](configs/ARS_native/cartpole_easy.yaml)| 910        |
| CartPole (hard) | 600 (max_iter=2000)  | [Link](configs/ARS_native/cartpole_hard.yaml) | 558.02       |
| MNIST | 0.90 (max_iter=4000)  | [Link](configs/ARS_native/minst.yaml) | 0.92       |
| Brax Ant | 3000 (max_iter=700)  | [Link](configs/ARS_native/brax_ant.yaml) | 4129.83        |
| Waterworld |  6 (max_iter=2000) |[Link](configs/ARS_native/waterworld.yaml) | 7.29 | 
| Waterworld (MA) |  2 (max_iter=2000) | [Link](configs/ARS_native/waterworld_ma.yaml) | 1.68 | 

=== ./scripts/benchmarks/problems.py ===
import ast
import re
import yaml
import json
import numpy as np
from evojax.policy import MLPPolicy
from evojax.policy.convnet import ConvNetPolicy


def setup_problem(config, logger):
    if config["problem_type"] == "cartpole_easy":
        return setup_cartpole(config, False)
    elif config["problem_type"] == "cartpole_hard":
        return setup_cartpole(config, True)
    elif config["problem_type"] == "brax":
        return setup_brax(config)
    elif config["problem_type"] == "mnist":
        return setup_mnist(config, logger)
    elif config["problem_type"] == "waterworld":
        return setup_waterworld(config)
    elif config["problem_type"] == "waterworld_ma":
        return setup_waterworld_ma(config)


def setup_cartpole(config, hard=False):
    from evojax.task.cartpole import CartPoleSwingUp

    train_task = CartPoleSwingUp(test=False, harder=hard)
    test_task = CartPoleSwingUp(test=True, harder=hard)
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[0],
        hidden_dims=[config["hidden_size"]] * 2,
        output_dim=train_task.act_shape[0],
    )
    return train_task, test_task, policy


def setup_brax(config):
    from evojax.task.brax_task import BraxTask

    train_task = BraxTask(env_name=config["env_name"], test=False)
    test_task = BraxTask(env_name=config["env_name"], test=True)
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[0],
        output_dim=train_task.act_shape[0],
        hidden_dims=[32, 32, 32, 32],
    )
    return train_task, test_task, policy


def setup_mnist(config, logger):
    from evojax.task.mnist import MNIST

    policy = ConvNetPolicy(logger=logger)
    train_task = MNIST(batch_size=config["batch_size"], test=False)
    test_task = MNIST(batch_size=config["batch_size"], test=True)
    return train_task, test_task, policy


def setup_waterworld(config, max_steps=500):
    from evojax.task.waterworld import WaterWorld

    train_task = WaterWorld(test=False, max_steps=max_steps)
    test_task = WaterWorld(test=True, max_steps=max_steps)
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[0],
        hidden_dims=[
            config["hidden_size"],
        ],
        output_dim=train_task.act_shape[0],
        output_act_fn="softmax",
    )
    return train_task, test_task, policy


def setup_waterworld_ma(config, num_agents=16, max_steps=500):
    from evojax.task.ma_waterworld import MultiAgentWaterWorld

    train_task = MultiAgentWaterWorld(
        num_agents=num_agents, test=False, max_steps=max_steps
    )
    test_task = MultiAgentWaterWorld(
        num_agents=num_agents, test=True, max_steps=max_steps
    )
    policy = MLPPolicy(
        input_dim=train_task.obs_shape[-1],
        hidden_dims=[
            config["hidden_size"],
        ],
        output_dim=train_task.act_shape[-1],
        output_act_fn="softmax",
    )
    return train_task, test_task, policy


def convert(obj):
    """Conversion helper instead of JSON encoder for handling booleans."""
    if isinstance(obj, bool):
        return int(obj)
    if isinstance(obj, (list, tuple)):
        return [convert(item) for item in obj]
    if isinstance(obj, dict):
        return {convert(key): convert(value) for key, value in obj.items()}
    if isinstance(obj, np.integer):
        return int(obj)
    if isinstance(obj, np.floating):
        return float(obj)
    if isinstance(obj, np.ndarray):
        return convert(obj.tolist())
    if isinstance(obj, np.bool_):
        return int(obj)
    return obj


def save_yaml(obj: dict, filename: str) -> None:
    """Save object as yaml file."""
    data = json.dumps(convert(obj), indent=1)
    data_dump = ast.literal_eval(data)
    with open(filename, "w") as f:
        yaml.safe_dump(data_dump, f, default_flow_style=False)


def load_yaml(config_fname: str) -> dict:
    """Load in YAML config file."""
    loader = yaml.SafeLoader
    loader.add_implicit_resolver(
        "tag:yaml.org,2002:float",
        re.compile(
            """^(?:
        [-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+]?[0-9]+)?
        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)
        |\\.[0-9_]+(?:[eE][-+][0-9]+)?
        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*
        |[-+]?\\.(?:inf|Inf|INF)
        |\\.(?:nan|NaN|NAN))$""",
            re.X,
        ),
        list("-+0123456789."),
    )
    with open(config_fname) as file:
        yaml_config = yaml.load(file, Loader=loader)
    return yaml_config

=== ./scripts/benchmarks/train.py ===
import os
import shutil
from evojax import Trainer
from evojax.algo import Strategies
from evojax import util
from problems import setup_problem, save_yaml, load_yaml


def main(config):
    # Set cuda device visibility
    if config["gpu_id"] is not None:
        if type(config["gpu_id"]) is list:
            os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(
                [str(i) for i in config["gpu_id"]]
            )
        else:
            os.environ["CUDA_VISIBLE_DEVICES"] = str(config["gpu_id"])

    # Setup logging. - add id if in config (mle-search) else log to default
    log_dir = f"./log/{config['es_name']}/{config['problem_type']}/"
    if "search_eval_id" in config.keys():
        log_dir += config["search_eval_id"]
    else:
        log_dir += "default"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    logger = util.create_logger(
        name=f"{config['problem_type']}", log_dir=log_dir, debug=config["debug"]
    )

    logger.info(f"EvoJAX {config['problem_type']}")
    logger.info("=" * 30)

    # Store evaluation configuration in log dir.
    save_yaml(config, os.path.join(log_dir, "config.yaml"))

    # Setup task.
    train_task, test_task, policy = setup_problem(config, logger)

    # Setup ES.
    solver = Strategies[config["es_name"]](
        **config["es_config"],
        param_size=policy.num_params,
        seed=config["seed"],
    )

    # Train.
    trainer = Trainer(
        policy=policy,
        solver=solver,
        train_task=train_task,
        test_task=test_task,
        max_iter=config["max_iter"],
        log_interval=config["log_interval"],
        test_interval=config["test_interval"],
        n_repeats=config["n_repeats"],
        n_evaluations=config["num_tests"],
        seed=config["seed"],
        log_dir=log_dir,
        logger=logger,
        normalize_obs=config["normalize"],
    )

    trainer.run(demo_mode=False)

    # Test the final model.
    src_file = os.path.join(log_dir, "best.npz")
    tar_file = os.path.join(log_dir, "model.npz")
    shutil.copy(src_file, tar_file)
    trainer.model_dir = log_dir
    score = trainer.run(demo_mode=True)
    return score


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-config",
        "--config_fname",
        type=str,
        default="configs/ARS/brax_ant.yaml",
        help="Path to configuration yaml.",
    )
    args, _ = parser.parse_known_args()
    config = load_yaml(args.config_fname)
    main(config)
